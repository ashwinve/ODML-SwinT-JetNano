{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/andrew.cmu.edu/usr8/bmarimut/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\n",
      "Tutel has not been installed. To use Swin-MoE, please install Tutel; otherwise, just ignore this.\n",
      "To use FusedLAMB or FusedAdam, please install apex.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import argparse\n",
    "from logger import create_logger\n",
    "import os\n",
    "\n",
    "\n",
    "from utils import load_checkpoint, load_pretrained, NativeScalerWithGradNormCount\n",
    "from config import get_config\n",
    "from data import build_loader\n",
    "from models import build_model\n",
    "from lr_scheduler import build_scheduler\n",
    "from optimizer import build_optimizer\n",
    "\n",
    "from main import train_one_epoch, validate, throughput\n",
    "\n",
    "from config import get_only_config\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "importlib.reload(sys.modules['main'])\n",
    "from main import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml\n"
     ]
    }
   ],
   "source": [
    "config_path = 'configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml'\n",
    "config = get_only_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.defrost()\n",
    "config.OUTPUT = \"/afs/ece.cmu.edu/usr/bmarimut/Private/output\"\n",
    "# config.MODEL.PRETRAINED = \"/afs/ece.cmu.edu/usr/ashwinve/Public/ckpt_epoch_29_6.pth\"\n",
    "config.MODEL.PRETRAINED = \"/afs/ece.cmu.edu/usr/ashwinve/Public/golden_resisc45.pth\"\n",
    "config.MODEL.RESUME = \"/afs/ece.cmu.edu/usr/ashwinve/Public/golden_resisc45.pth\"\n",
    "config.DATA.CACHE_MODE = 'no'\n",
    "config.DATA.DATA_PATH = './data/RESISC45/'\n",
    "config.DATA.ZIP_MODE = True\n",
    "config.PRINT_FREQ = 120\n",
    "config.DATA.BATCH_SIZE = 4\n",
    "config.freeze()\n",
    "os.makedirs(config.OUTPUT, exist_ok=True)\n",
    "logger = create_logger(output_dir=config.OUTPUT, name=f\"{config.MODEL.NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/andrew.cmu.edu/usr8/bmarimut/.local/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.lora_k.weight', 'layers.0.blocks.0.attn.lora_v.weight', 'layers.0.blocks.0.attn.lora_rpb.weight', 'layers.1.blocks.0.attn.lora_k.weight', 'layers.1.blocks.0.attn.lora_v.weight', 'layers.1.blocks.0.attn.lora_rpb.weight', 'layers.2.blocks.0.attn.lora_k.weight', 'layers.2.blocks.0.attn.lora_v.weight', 'layers.2.blocks.0.attn.lora_rpb.weight', 'layers.2.blocks.2.attn.lora_k.weight', 'layers.2.blocks.2.attn.lora_v.weight', 'layers.2.blocks.2.attn.lora_rpb.weight', 'layers.2.blocks.4.attn.lora_k.weight', 'layers.2.blocks.4.attn.lora_v.weight', 'layers.2.blocks.4.attn.lora_rpb.weight', 'layers.3.blocks.0.attn.lora_k.weight', 'layers.3.blocks.0.attn.lora_v.weight', 'layers.3.blocks.0.attn.lora_rpb.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_pretrained(config, model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze specific layers for downstream task training\n",
    "# model_named_params = list(model.named_parameters())\n",
    "# num_params = len(model_named_params)\n",
    "# for param_iter in range(num_params):\n",
    "#     param_name, param = model_named_params[param_iter]\n",
    "#     if \"lora\" not in param_name:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-12-04 23:01:52 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-8-ae1c34bdd2f3> 2)\u001b[0m: INFO number of params: 52624074\n",
      "\u001b[32m[2022-12-04 23:01:52 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-8-ae1c34bdd2f3> 5)\u001b[0m: INFO number of GFLOPs: 4.423600896\n"
     ]
    }
   ],
   "source": [
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"number of params: {n_parameters}\")\n",
    "if hasattr(model, 'flops'):\n",
    "    flops = model.flops()\n",
    "    logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n",
    "\n",
    "model.cuda()\n",
    "model_without_ddp = model\n",
    "super_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_SELECTOR = 0\n",
    "LORA_RANK_DICT = {\n",
    "    'layers.0.blocks.0.attn': [9,   12, 13, 49],\n",
    "    'layers.0.blocks.1.attn': [28,  35, 38, 49],\n",
    "    'layers.1.blocks.0.attn': [24,  32, 39, 49],\n",
    "    'layers.1.blocks.1.attn': [19,  22, 23, 49],\n",
    "    'layers.2.blocks.0.attn': [16,  18, 19, 49],\n",
    "    'layers.2.blocks.1.attn': [20,  22, 22, 49],\n",
    "    'layers.2.blocks.2.attn': [22,  26, 30, 49],\n",
    "    'layers.2.blocks.3.attn': [22,  25, 26, 49],\n",
    "    'layers.2.blocks.4.attn': [24,  24, 25, 49],\n",
    "    'layers.2.blocks.5.attn': [23,  24, 24, 49],\n",
    "    'layers.3.blocks.0.attn': [20,  21, 22, 49],\n",
    "    'layers.3.blocks.1.attn': [16,  16, 17, 49]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_md = get_attn(2,5)\n",
    "# print(get_attn(2,5).input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(list(model.children())[2][0].children())[0][1])\n",
    "# print(get_attn(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# # import models\n",
    "# import sys\n",
    "# importlib.reload(sys.modules['models'])\n",
    "# from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LORA_WindowAttention2(torch.nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, lora_rank, window_size, num_heads, param_name, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = torch.nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = torch.nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = torch.nn.Dropout(attn_drop)\n",
    "        self.proj = torch.nn.Linear(dim, dim)\n",
    "        self.proj_drop = torch.nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Defining LORA parameters\n",
    "        # self.lora_k = torch.nn.Parameter(torch.FloatTensor(self.window_size[0] * self.window_size[1], lora_rank), requires_grad=True)\n",
    "        # self.lora_v = torch.nn.Parameter(torch.FloatTensor(self.window_size[0] * self.window_size[1], lora_rank), requires_grad=True)\n",
    "        # self.lora_rpb = torch.nn.Parameter(torch.FloatTensor(self.window_size[0] * self.window_size[1], lora_rank), requires_grad=True)\n",
    "        \n",
    "        self.lora_k = torch.nn.Linear(head_dim * self.window_size[0] * self.window_size[1], head_dim * lora_rank, bias=False)\n",
    "        self.lora_v = torch.nn.Linear(head_dim * self.window_size[0] * self.window_size[1], head_dim * lora_rank, bias=False)\n",
    "        self.lora_rpb = torch.nn.Linear(self.window_size[0] * self.window_size[1] * self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1] * lora_rank, bias=False)\n",
    "\n",
    "        # self.lora_k.weight.data.fill_(0.01)\n",
    "        # self.lora_v.weight.data.fill_(0.01)\n",
    "        # self.lora_rpb.weight.data.fill_(0.01)\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        # torch.nn.init.ones_(self.lora_k)\n",
    "        # torch.nn.init.ones_(self.lora_v)\n",
    "        # torch.nn.init.ones_(self.lora_rpb)\n",
    "        # torch.nn.init.xavier_normal_(self.lora_k)\n",
    "        # torch.nn.init.xavier_normal_(self.lora_v)\n",
    "        # torch.nn.init.xavier_normal_(self.lora_rpb)\n",
    "\n",
    "        # optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.1)\n",
    "        # self.optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)\n",
    "        lora_params = [p[1] for p in self.named_parameters() if \"lora\" in p[0]]\n",
    "        # print(lora_params)\n",
    "        self.optimizer = torch.optim.Adam(lora_params, lr=learning_rate)\n",
    "        # self.optimizer = torch.optim.AdamW(filter(lambda p[1]: \"lora\" in p[0], self.named_parameters()), lr=learning_rate)\n",
    "\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=1700)\n",
    "        # self.lr_scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=1500, gamma=0.1)\n",
    "        # create a loss function\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        # self.criterion = torch.nn.L1Loss()\n",
    "\n",
    "        self.param_name = param_name\n",
    "        self.avg_loss = 0\n",
    "\n",
    "        \n",
    "    \n",
    "    def do_backward(self, target, print_log):\n",
    "        self.optimizer.zero_grad()\n",
    "        # self.loss = self.criterion(self.output, target)\n",
    "        self.loss = torch.sqrt(self.criterion(self.output*100, target*100))\n",
    "        self.loss.backward()\n",
    "        self.avg_loss += self.loss.item()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def do_lr_step(self):\n",
    "        self.lr_scheduler.step(self.loss)\n",
    "\n",
    "    \n",
    "    def load_pretrained_weights(self, super_model):\n",
    "        new_sd = copy.deepcopy(self.state_dict())\n",
    "        # for name in params_names_list:\n",
    "        # print(new_sd)\n",
    "        new_sd['qkv.weight'] = super_model.state_dict()[self.param_name+'.qkv.weight']\n",
    "        new_sd['qkv.bias'] = super_model.state_dict()[self.param_name+'.qkv.bias']\n",
    "        \n",
    "        new_sd['relative_position_bias_table'] = super_model.state_dict()[self.param_name+'.relative_position_bias_table']\n",
    "        \n",
    "        self.load_state_dict(new_sd)\n",
    "\n",
    "        # model_named_params = list(self.named_parameters())\n",
    "        # num_params = len(model_named_params)\n",
    "        # for param_iter in range(num_params):\n",
    "        #     param_name, param = model_named_params[param_iter]\n",
    "        #     if \"lora\" not in param_name:\n",
    "        #         param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        # k_lora = k.transpose(-2, -1) @ self.lora_k\n",
    "\n",
    "        flattened_k = k.transpose(-2, -1).reshape(B_, self.num_heads, -1)\n",
    "        k_lora = self.lora_k(flattened_k).reshape(B_, self.num_heads, C // self.num_heads, self.lora_rank)\n",
    "\n",
    "        attn = (q @ k_lora)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        \n",
    "        # attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        \n",
    "        rpb = relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        # rpb_lora = rpb @ self.lora_rpb\n",
    "        flattened_rpb = rpb.reshape(self.num_heads, -1)\n",
    "        \n",
    "        # rpb_lora: num_heads * Window_size * window_size * lora_rank\n",
    "        rpb_lora = self.lora_rpb(flattened_rpb).view(self.num_heads, self.window_size[0] * self.window_size[1], self.lora_rank)\n",
    "\n",
    "        attn = attn + rpb_lora\n",
    "\n",
    "        if mask is not None:\n",
    "            # print(\"lora_k: \", self.lora_k.shape)\n",
    "            # print(\"lora_v: \", self.lora_v.shape)\n",
    "            # print(\"lora_rpb: \", self.lora_rpb.shape)\n",
    "            # print(\"attn: \", attn.shape)\n",
    "            # print(\"mask: \", mask.shape)\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        # v_lora = (v.transpose(-2, -1) @ self.lora_v).transpose(-2, -1)\n",
    "\n",
    "        flattened_v_lora = v.transpose(-2, -1).reshape(B_, self.num_heads, -1)\n",
    "        v_lora = self.lora_v(flattened_v_lora).reshape(B_, self.num_heads, C // self.num_heads, self.lora_rank).transpose(-2, -1)\n",
    "\n",
    "        x = (attn @ v_lora).transpose(1, 2).reshape(B_, N, C)\n",
    "        \n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        self.output = x\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        \n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        # flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        # flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "\n",
    "        # TODO: Figure out FLOPS compute\n",
    "        # # transform : k @ lora_k\n",
    "        # flops += k.shape[0] * k.shape[1] * 2 * k.shape[2] * k.shape[3] * k_lora.shape[3]\n",
    "        # # attn: q @ k_lora\n",
    "        # flops += q.shape[0] * q.shape[1] * 2 * q.shape[2] * q.shape[3] * k_lora.shape[3]\n",
    "        # # transform : v @ v_lora\n",
    "        # flops += v.shape[0] * v.shape[1] * 2 * v.shape[2] * v.shape[3] * v_lora.shape[3]\n",
    "        # # op : attn @ v_lora\n",
    "        # flops += attn.shape[0] * attn.shape[1] * 2 * attn.shape[2] * attn.shape[3] * v_lora.shape[3]\n",
    "\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block(my_model, layer_id, block_id):\n",
    "    # print(list(list(model.children())[2][layer_id].children())[0][block_id])\n",
    "    return list(list(my_model.children())[2][layer_id].children())[0][block_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn(my_model, layer_id, block_id):\n",
    "    block = get_block(my_model, layer_id, block_id)\n",
    "    return list(block.children())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_attn(super_model, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_block(super_model, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_DEPTHS = [2, 2, 6, 2]\n",
    "NUM_LAYERS = len(LAYER_DEPTHS)\n",
    "NUM_HEADS = [ 3, 6, 12, 24 ]\n",
    "H = 224\n",
    "W = 224\n",
    "B = 1\n",
    "L = 224 * 224\n",
    "window_size = 7\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# C = 96 * 2**i\n",
    "# num_heads = NUM_HEADS[i]\n",
    "# dim = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lora_attns = []\n",
    "for layer_id in range(NUM_LAYERS):\n",
    "    layer_attns = []\n",
    "    C = 96 * 2**layer_id\n",
    "    num_heads = NUM_HEADS[layer_id]\n",
    "    dim = C\n",
    "    for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        if block_id%2 == 0:\n",
    "            param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "            lora_attn = LORA_WindowAttention2(dim, LORA_RANK_DICT[param_name][LORA_SELECTOR],\n",
    "            to_2tuple(window_size), num_heads, param_name)\n",
    "            lora_attn.cuda()\n",
    "            lora_attn.load_pretrained_weights(super_model)\n",
    "            layer_attns.append(lora_attn)\n",
    "        else:\n",
    "            layer_attns.append(None)\n",
    "    all_lora_attns.append(layer_attns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save LoRA Students to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_id in range(NUM_LAYERS):\n",
    "    for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        if block_id%2 == 0:\n",
    "            param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "            torch.save({\n",
    "                'state_dict': all_lora_attns[layer_id][block_id].state_dict()},\n",
    "                param_name+\".pth\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lora_attns = []\n",
    "for layer_id in range(NUM_LAYERS):\n",
    "    layer_attns = []\n",
    "    C = 96 * 2**layer_id\n",
    "    num_heads = NUM_HEADS[layer_id]\n",
    "    dim = C\n",
    "    for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        if block_id%2 == 0:\n",
    "            param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "            lora_attn = LORA_WindowAttention2(dim, LORA_RANK_DICT[param_name][LORA_SELECTOR],\n",
    "            to_2tuple(window_size), num_heads, param_name)\n",
    "            lora_attn.load_state_dict(torch.load(param_name+\".pth\")['state_dict'])\n",
    "            lora_attn.cuda()\n",
    "            layer_attns.append(lora_attn)\n",
    "        else:\n",
    "            layer_attns.append(None)\n",
    "    all_lora_attns.append(layer_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora_weights(super_model):\n",
    "    new_sm_sd = copy.deepcopy(super_model.state_dict())\n",
    "    for layer_id in range(NUM_LAYERS):\n",
    "        for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "            if block_id%2 == 0:\n",
    "                param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "                # for name in params_names_list:\n",
    "                # print(new_sd)\n",
    "                new_sm_sd[param_name+'.lora_k.weight'] = all_lora_attns[layer_id][block_id].state_dict()['lora_k.weight']\n",
    "                new_sm_sd[param_name+'.lora_v.weight'] = all_lora_attns[layer_id][block_id].state_dict()['lora_v.weight']\n",
    "                new_sm_sd[param_name+'.lora_rpb.weight'] = all_lora_attns[layer_id][block_id].state_dict()['lora_rpb.weight']\n",
    "                \n",
    "                super_model.load_state_dict(new_sm_sd)\n",
    "    return super_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_name+'.lora_k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model = load_lora_weights(super_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super_model.state_dict()['layers.3.blocks.0.attn.lora_k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_lora_attns[0][0].state_dict()\n",
    "# [p[1] for p in all_lora_attns[0][0].named_parameters() if \"lora\" in p[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "# # create a loss function\n",
    "# criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student teacher training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "def teacher_validate(config, data_loader, model):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    attn_loss_meters = []\n",
    "    for layer_id in range(NUM_LAYERS):\n",
    "        block_loss = []\n",
    "        for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "            if block_id%2 == 0:\n",
    "                atn_loss_meter = AverageMeter()\n",
    "                block_loss.append(atn_loss_meter)\n",
    "            else:\n",
    "                block_loss.append(None)\n",
    "        attn_loss_meters.append(block_loss)\n",
    "\n",
    "    acc1_meter = AverageMeter()\n",
    "    acc5_meter = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (images, target) in enumerate(data_loader):\n",
    "        images = images.cuda(non_blocking=False)\n",
    "        target = target.cuda(non_blocking=False)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n",
    "                output = model(images)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        loss = criterion(output, target)\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        \n",
    "        # Train student\n",
    "        for layer_id in range(NUM_LAYERS):\n",
    "            for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "                if block_id%2 == 0:\n",
    "                    lora_md = all_lora_attns[layer_id][block_id]\n",
    "                    teacher_attn = get_attn(super_model, layer_id, block_id)\n",
    "                    lora_md.forward(teacher_attn.input)\n",
    "                    lora_md.do_backward(teacher_attn.output, idx % config.PRINT_FREQ == 0)\n",
    "                    # lora_md.avg_loss = acc1\n",
    "                    lora_md.do_lr_step()\n",
    "                    attn_loss_meters[layer_id][block_id].update(lora_md.loss.item(), 1)\n",
    "                    \n",
    "\n",
    "        # Train student\n",
    "        # for layer_id in range(NUM_LAYERS):\n",
    "        #     for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        #         if block_id%2 == 0:\n",
    "        #             lora_md = all_lora_attns[layer_id][block_id]\n",
    "        #             teacher_attn = get_attn(super_model, layer_id, block_id)\n",
    "        #             param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "\n",
    "        #             # with open(param_name+'.ndarray',mode='ba+') as f:\n",
    "        #             #     teacher_attn.input.cpu().numpy().tofile(f)\n",
    "        #             #     teacher_attn.output.cpu().numpy().tofile(f)\n",
    "        #             lora_md.forward(teacher_attn.input)\n",
    "        #             lora_md.do_backward(teacher_attn.output, idx % config.PRINT_FREQ == 0)\n",
    "        #             lora_md.do_lr_step()\n",
    "\n",
    "\n",
    "\n",
    "        loss_meter.update(loss.item(), target.size(0))\n",
    "        acc1_meter.update(acc1.item(), target.size(0))\n",
    "        acc5_meter.update(acc5.item(), target.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if idx % config.PRINT_FREQ == 0:\n",
    "            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n",
    "            logger.info(\n",
    "                f'Test: [{idx}/{len(data_loader)}]\\t'\n",
    "                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n",
    "                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n",
    "                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n",
    "                f'Mem {memory_used:.0f}MB')\n",
    "            for layer_id in range(NUM_LAYERS):\n",
    "                for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "                    if block_id%2 == 0:\n",
    "                        param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "                        print(param_name, \": Loss : \", attn_loss_meters[layer_id][block_id].val, \" : \", attn_loss_meters[layer_id][block_id].avg, \n",
    "                            \" LR : \", all_lora_attns[layer_id][block_id].optimizer.param_groups[0]['lr'])\n",
    "    # lr_scheduler.get_last_lr()\n",
    "    logger.info(f' * Acc@1 {acc1_meter.avg:.3f} Acc@5 {acc5_meter.avg:.3f}')\n",
    "    return acc1_meter.avg, acc5_meter.avg, loss_meter.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-12-04 21:56:31 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-198-0dbba6bb9711> 1)\u001b[0m: INFO Start teacher-student training\n",
      " Epoch :  0\n",
      "\u001b[32m[2022-12-04 21:56:32 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [0/3150]\tTime 0.841 (0.841)\tLoss 0.0740 (0.0740)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\tMem 1573MB\n",
      "layers.0.blocks.0.attn : Loss :  41.49988555908203  :  41.49988555908203  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  30.26732635498047  :  30.26732635498047  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  32.3546257019043  :  32.3546257019043  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.35563659667969  :  33.35563659667969  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  50.34062194824219  :  50.34062194824219  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  39.98451614379883  :  39.98451614379883  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:56:42 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [120/3150]\tTime 0.090 (0.097)\tLoss 0.9372 (0.5819)\tAcc@1 87.500 (88.326)\tAcc@5 100.000 (97.417)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  42.897151947021484  :  34.56544423694453  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  31.057720184326172  :  33.63060667495097  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  31.006547927856445  :  34.14370068636808  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  37.7449951171875  :  40.95862661314405  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  55.13256072998047  :  58.37092662842806  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  53.570743560791016  :  59.04802309777126  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:56:53 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [240/3150]\tTime 0.090 (0.094)\tLoss 0.0613 (0.7551)\tAcc@1 100.000 (85.270)\tAcc@5 100.000 (96.214)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  25.697643280029297  :  32.09196373239098  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  30.37642478942871  :  31.445097911407345  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  31.05246925354004  :  31.990152018693472  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  38.34580993652344  :  39.236954906669396  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  54.08430099487305  :  56.136793571883715  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  51.24934387207031  :  55.3716057425218  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:57:04 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [360/3150]\tTime 0.092 (0.093)\tLoss 0.2541 (0.7144)\tAcc@1 87.500 (85.145)\tAcc@5 100.000 (96.676)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  24.67336082458496  :  31.277737171035724  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  26.896644592285156  :  30.902788045994132  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  30.038055419921875  :  31.466076420284704  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  37.8400764465332  :  38.94875594585556  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  53.40848922729492  :  55.31916168720108  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  49.8457145690918  :  53.04503151848706  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:57:15 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [480/3150]\tTime 0.092 (0.093)\tLoss 0.2636 (0.6741)\tAcc@1 75.000 (86.149)\tAcc@5 100.000 (96.881)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  27.564424514770508  :  30.777167829803023  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  29.355701446533203  :  30.277102167046245  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  30.129724502563477  :  30.9633806034334  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  39.379005432128906  :  38.64458039694169  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  53.752174377441406  :  54.667580245686175  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  36.94393539428711  :  50.58048577417703  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:57:26 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [600/3150]\tTime 0.092 (0.093)\tLoss 0.0045 (0.6765)\tAcc@1 100.000 (85.004)\tAcc@5 100.000 (97.005)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  21.296226501464844  :  29.92476222757095  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  27.49466323852539  :  29.774909449496406  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  29.004959106445312  :  30.61581867744839  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.56321334838867  :  38.16812891650716  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  48.328224182128906  :  54.139557957450876  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  46.0345344543457  :  49.665401763408234  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:57:38 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [720/3150]\tTime 0.093 (0.093)\tLoss 0.3610 (0.6335)\tAcc@1 87.500 (86.026)\tAcc@5 100.000 (97.122)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  25.353790283203125  :  29.49816497982623  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  26.66108512878418  :  29.452299125978257  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  29.288272857666016  :  30.286922827838364  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  35.31809616088867  :  37.63549786169552  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  50.319252014160156  :  53.38347660122896  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  44.47152328491211  :  48.79164538337189  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:57:49 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [840/3150]\tTime 0.094 (0.093)\tLoss 0.4812 (0.6649)\tAcc@1 87.500 (85.107)\tAcc@5 100.000 (97.072)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  29.265493392944336  :  29.18945828686146  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  29.387100219726562  :  29.06012069030834  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  30.546789169311523  :  30.064419758872667  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  36.1102180480957  :  37.25163067648725  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  49.76117706298828  :  52.85678717469205  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  45.3031005859375  :  48.15668639501692  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:58:00 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [960/3150]\tTime 0.094 (0.093)\tLoss 0.0065 (0.6368)\tAcc@1 100.000 (85.835)\tAcc@5 100.000 (97.334)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  33.205345153808594  :  30.490988083362083  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  52.52751159667969  :  30.568871532841104  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  34.61750030517578  :  30.447491704363234  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  37.70452880859375  :  37.29411397689835  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  48.85226821899414  :  52.53191796103328  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  36.42989730834961  :  47.133651423776804  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:58:11 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1080/3150]\tTime 0.095 (0.093)\tLoss 0.0256 (0.6452)\tAcc@1 100.000 (85.673)\tAcc@5 100.000 (97.306)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  30.839725494384766  :  30.487247530560488  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  25.627300262451172  :  31.011310780302008  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  27.77787971496582  :  30.445648697986304  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.15380859375  :  37.08840462457902  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  48.909202575683594  :  52.23840577639881  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  47.000762939453125  :  46.885530600605136  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:58:23 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1200/3150]\tTime 0.095 (0.093)\tLoss 2.0079 (0.6467)\tAcc@1 62.500 (85.606)\tAcc@5 100.000 (97.252)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  21.0639705657959  :  30.039260205182305  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  25.60955238342285  :  30.58703109207598  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.729164123535156  :  30.208624075096314  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.263450622558594  :  36.74076983176302  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  50.29265213012695  :  51.87975568874591  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  47.470455169677734  :  46.968380625500075  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:58:34 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1320/3150]\tTime 0.093 (0.093)\tLoss 0.6554 (0.6431)\tAcc@1 87.500 (85.740)\tAcc@5 87.500 (97.256)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  23.051372528076172  :  29.526168043553874  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  30.29376983642578  :  30.238248441004554  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  29.875656127929688  :  30.054786790418948  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  35.97600555419922  :  36.572444082660084  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  48.31870651245117  :  51.69849676150972  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  48.82204818725586  :  46.973607360008174  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:58:45 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1440/3150]\tTime 0.093 (0.093)\tLoss 1.5740 (0.6674)\tAcc@1 87.500 (85.245)\tAcc@5 100.000 (97.155)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  36.769622802734375  :  29.84152490508631  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  30.585886001586914  :  30.10551237150003  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  30.47365951538086  :  30.010515213012695  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.176719665527344  :  36.43074477868805  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  49.60913848876953  :  51.50284466819578  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  47.11927795410156  :  46.96739814647116  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:58:56 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1560/3150]\tTime 0.093 (0.093)\tLoss 0.0053 (0.6787)\tAcc@1 100.000 (85.114)\tAcc@5 100.000 (97.213)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  32.7872428894043  :  30.354444325390265  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  26.479188919067383  :  30.184392253379652  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.790842056274414  :  29.939506604074285  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.13903045654297  :  36.25632053670938  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  48.66263961791992  :  51.27376475508285  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  35.86980056762695  :  46.82158319252107  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:59:08 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1680/3150]\tTime 0.093 (0.093)\tLoss 0.0003 (0.6897)\tAcc@1 100.000 (84.511)\tAcc@5 100.000 (97.226)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  29.97968101501465  :  30.27474209469461  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  27.545228958129883  :  30.296862095612134  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.1142635345459  :  29.947439916498386  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.77482986450195  :  36.206458656224804  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  50.49174880981445  :  51.14597436744921  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  48.70445251464844  :  46.461712845729124  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:59:19 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1800/3150]\tTime 0.094 (0.093)\tLoss 0.3811 (0.6806)\tAcc@1 87.500 (84.738)\tAcc@5 100.000 (97.279)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  22.246416091918945  :  29.95559324363018  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  25.52505874633789  :  30.03732796393124  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.02891731262207  :  29.816237810252442  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  32.105491638183594  :  36.00071800291241  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  44.59233093261719  :  50.90976183656717  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  40.25492858886719  :  46.35284844482693  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:59:30 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1920/3150]\tTime 0.092 (0.093)\tLoss 0.7103 (0.6948)\tAcc@1 75.000 (84.364)\tAcc@5 100.000 (97.254)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  28.6151180267334  :  29.72009284194716  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  27.303266525268555  :  29.866250911396907  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.484664916992188  :  29.779987752220396  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.279151916503906  :  35.87447624762564  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  49.809688568115234  :  50.8199585254834  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  46.522308349609375  :  46.3423475516705  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:59:41 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2040/3150]\tTime 0.095 (0.093)\tLoss 0.0068 (0.7035)\tAcc@1 100.000 (84.187)\tAcc@5 100.000 (97.189)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  27.534744262695312  :  29.50417041918742  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  26.72677993774414  :  29.653158947628306  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  31.8748779296875  :  29.758418906032894  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.90789794921875  :  35.766805817016724  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  48.411006927490234  :  50.79730483470507  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  45.5151481628418  :  46.37958028327011  LR :  0.01\n",
      "\u001b[32m[2022-12-04 21:59:52 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2160/3150]\tTime 0.093 (0.093)\tLoss 0.4402 (0.7131)\tAcc@1 87.500 (84.053)\tAcc@5 100.000 (97.143)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  26.836111068725586  :  29.331307883838544  LR :  0.01\n",
      "layers.1.blocks.0.attn : Loss :  27.330427169799805  :  29.525076236398284  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  30.123836517333984  :  29.79727022099528  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.34406280517578  :  35.68179003456465  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  48.33284378051758  :  50.69606890980704  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  44.96470642089844  :  46.35984653965404  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:00:04 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2280/3150]\tTime 0.092 (0.093)\tLoss 0.9982 (0.7147)\tAcc@1 87.500 (83.971)\tAcc@5 100.000 (97.172)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  21.26405906677246  :  29.366677385001577  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  27.67401885986328  :  29.562794000106337  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  27.36632537841797  :  29.82396375396073  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  32.53740692138672  :  35.57353923802624  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  47.743919372558594  :  50.61597861056263  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  44.4573860168457  :  46.36601289768378  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:00:15 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2400/3150]\tTime 0.091 (0.093)\tLoss 2.3630 (0.7220)\tAcc@1 62.500 (84.043)\tAcc@5 75.000 (97.116)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  13.927068710327148  :  28.943137860407386  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  29.333961486816406  :  29.504286300138055  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.831823348999023  :  29.77161615841193  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.39472961425781  :  35.496605040976426  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  47.82380676269531  :  50.57609477394673  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  46.14271545410156  :  46.42681790867829  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:00:26 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2520/3150]\tTime 0.092 (0.093)\tLoss 1.5870 (0.7024)\tAcc@1 75.000 (84.495)\tAcc@5 100.000 (97.179)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  17.53416633605957  :  28.462529738145136  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  27.195505142211914  :  29.402522160107537  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  31.825349807739258  :  29.74363164814726  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  35.95492935180664  :  35.44026559877377  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  52.77668762207031  :  50.48222155875509  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  47.604270935058594  :  46.36976926320131  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:00:37 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2640/3150]\tTime 0.095 (0.093)\tLoss 0.1942 (0.6870)\tAcc@1 87.500 (84.864)\tAcc@5 100.000 (97.217)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  22.716115951538086  :  28.079472795664834  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  27.577905654907227  :  29.312290946356683  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.037155151367188  :  29.734867713434234  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  32.37602615356445  :  35.35438215926668  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  46.016788482666016  :  50.433244838808484  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  44.58428192138672  :  46.38076684295296  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:00:48 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2760/3150]\tTime 0.093 (0.093)\tLoss 0.5441 (0.6867)\tAcc@1 87.500 (84.942)\tAcc@5 87.500 (97.193)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  23.13996124267578  :  27.727976167258124  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  29.03030014038086  :  29.253264343250667  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  29.290048599243164  :  29.659369493903824  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  35.69839096069336  :  35.28772131544401  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  51.40700149536133  :  50.37057677216825  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  51.757286071777344  :  46.443856743270615  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:00:59 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2880/3150]\tTime 0.094 (0.093)\tLoss 2.2234 (0.6808)\tAcc@1 62.500 (85.140)\tAcc@5 75.000 (97.197)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  20.08274269104004  :  27.46794473011847  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  24.58966064453125  :  29.153132529690712  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.21024513244629  :  29.62022876573991  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.67161178588867  :  35.272117596208865  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  50.176666259765625  :  50.4490918176367  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  51.91310119628906  :  46.642724837912574  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:01:10 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3000/3150]\tTime 0.095 (0.093)\tLoss 0.0326 (0.6871)\tAcc@1 100.000 (85.067)\tAcc@5 100.000 (97.197)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  15.507606506347656  :  27.125164445421373  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  26.638561248779297  :  29.058137779909543  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  29.614118576049805  :  29.594076900234306  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.12297821044922  :  35.235575774160075  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  47.57136535644531  :  50.40621141448334  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  47.07773971557617  :  46.776713051266846  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:01:21 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3120/3150]\tTime 0.090 (0.093)\tLoss 1.0366 (0.6844)\tAcc@1 75.000 (85.073)\tAcc@5 87.500 (97.240)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  20.758943557739258  :  26.81593324740091  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  30.864187240600586  :  28.978641071215687  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  30.387081146240234  :  29.57653628798182  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  35.07130432128906  :  35.19769685568622  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  48.81243133544922  :  50.381734420839926  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  46.408287048339844  :  46.86743761629143  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:01:24 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 91)\u001b[0m: INFO  * Acc@1 85.067 Acc@5 97.254\n",
      " Epoch :  1\n",
      "\u001b[32m[2022-12-04 22:01:25 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [0/3150]\tTime 0.769 (0.769)\tLoss 0.0007 (0.0007)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  22.681438446044922  :  22.681438446044922  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  31.76057243347168  :  31.76057243347168  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  31.863195419311523  :  31.863195419311523  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  37.61505889892578  :  37.61505889892578  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  54.547122955322266  :  54.547122955322266  LR :  0.01\n",
      "layers.3.blocks.0.attn : Loss :  52.26481628417969  :  52.26481628417969  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:01:36 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [120/3150]\tTime 0.092 (0.098)\tLoss 0.8178 (0.6524)\tAcc@1 75.000 (87.293)\tAcc@5 100.000 (96.591)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  14.954813957214355  :  19.212174502286043  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  26.838224411010742  :  29.81452896181217  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.279247283935547  :  29.426881128106235  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  35.38496017456055  :  34.76984052421633  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  43.51806640625  :  49.813266596518275  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  49.70394515991211  :  49.55467397516424  LR :  0.01\n",
      "\u001b[32m[2022-12-04 22:01:47 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [240/3150]\tTime 0.093 (0.095)\tLoss 0.0066 (0.7472)\tAcc@1 100.000 (85.529)\tAcc@5 100.000 (95.902)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  20.421844482421875  :  19.126237900919936  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  31.01201057434082  :  29.613795830501065  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  29.516136169433594  :  29.547198291636107  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.453208923339844  :  34.71050533120563  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  41.79155349731445  :  45.94481600369655  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  41.96773147583008  :  47.240894697513816  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:01:58 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [360/3150]\tTime 0.092 (0.094)\tLoss 0.2387 (0.7860)\tAcc@1 75.000 (85.215)\tAcc@5 100.000 (95.810)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  19.182809829711914  :  18.937525836384527  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  29.34682273864746  :  28.895773029063218  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.811037063598633  :  29.450813309307573  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.555259704589844  :  34.70484280784375  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  39.270599365234375  :  43.80265124418729  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  36.82474136352539  :  43.79142652390076  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:02:09 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [480/3150]\tTime 0.091 (0.094)\tLoss 0.5097 (0.6917)\tAcc@1 75.000 (86.902)\tAcc@5 100.000 (96.414)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  20.593162536621094  :  19.18174657008752  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  32.794307708740234  :  29.1365072642939  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  28.583370208740234  :  29.28025138204658  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.318321228027344  :  34.69372860755841  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  32.498504638671875  :  42.24606344655249  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  27.469789505004883  :  41.04065701569936  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:02:20 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [600/3150]\tTime 0.090 (0.093)\tLoss 0.9052 (0.7046)\tAcc@1 87.500 (85.316)\tAcc@5 87.500 (96.589)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  22.99005889892578  :  19.505692921541694  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  25.7587833404541  :  29.310649960687673  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  27.841432571411133  :  29.334773563505607  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.06095504760742  :  34.64173762849881  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  38.44936752319336  :  41.75674286777287  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  33.10318374633789  :  39.90419539834021  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:02:31 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [720/3150]\tTime 0.091 (0.093)\tLoss 0.4680 (0.6514)\tAcc@1 87.500 (86.442)\tAcc@5 100.000 (96.931)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  19.408910751342773  :  19.321722235659784  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  30.475765228271484  :  28.91145713319395  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  29.17872428894043  :  29.10871881916189  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  33.8034782409668  :  34.40211312747696  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  37.91298294067383  :  41.04362795719988  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  32.65837478637695  :  38.79660236818946  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:02:42 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [840/3150]\tTime 0.090 (0.093)\tLoss 1.3945 (0.6837)\tAcc@1 87.500 (85.285)\tAcc@5 100.000 (97.042)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  22.645721435546875  :  19.736195487159613  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  28.349512100219727  :  28.676333412687505  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  30.77891731262207  :  29.088579168784634  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  34.93789291381836  :  34.271195573840785  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  40.286014556884766  :  40.58990019028309  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  35.60676193237305  :  37.83059618901129  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:02:53 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [960/3150]\tTime 0.092 (0.093)\tLoss 0.7529 (0.6540)\tAcc@1 87.500 (85.900)\tAcc@5 87.500 (97.229)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  18.564899444580078  :  19.597157229245887  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  41.41046142578125  :  29.401828118839322  LR :  0.01\n",
      "layers.2.blocks.0.attn : Loss :  38.442344665527344  :  29.293501013399535  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  37.85638427734375  :  34.406949264574  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  32.05693435668945  :  39.71663801330185  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  28.478368759155273  :  36.75434236992906  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:03:04 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1080/3150]\tTime 0.091 (0.093)\tLoss 0.1890 (0.6640)\tAcc@1 87.500 (85.765)\tAcc@5 100.000 (97.190)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  17.442840576171875  :  19.476096689645942  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  23.152973175048828  :  29.69644632260078  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  27.784423828125  :  29.493555547130207  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  32.10036087036133  :  34.40489008336239  LR :  0.01\n",
      "layers.2.blocks.4.attn : Loss :  39.25149154663086  :  39.55353798808928  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  36.2821044921875  :  36.44233987686482  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:03:15 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1200/3150]\tTime 0.092 (0.093)\tLoss 0.3561 (0.6681)\tAcc@1 87.500 (85.627)\tAcc@5 100.000 (97.138)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  19.142642974853516  :  19.452850662599893  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  21.843748092651367  :  29.00286530138154  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  28.4782772064209  :  29.394666973498342  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  29.27467155456543  :  34.16559157423136  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  40.32840347290039  :  39.55449715899389  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  35.96466827392578  :  36.36961856908743  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:03:26 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1320/3150]\tTime 0.092 (0.093)\tLoss 0.6939 (0.6780)\tAcc@1 87.500 (85.636)\tAcc@5 100.000 (97.038)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  20.64203643798828  :  19.679509835023037  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  23.191883087158203  :  28.509713114435975  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  28.635072708129883  :  29.298144406211097  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  26.6546630859375  :  33.57432991622705  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  37.782474517822266  :  39.60096282439192  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  32.875953674316406  :  36.17194676020216  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:03:37 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1440/3150]\tTime 0.091 (0.093)\tLoss 1.0590 (0.7098)\tAcc@1 75.000 (85.167)\tAcc@5 100.000 (96.921)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  18.23613929748535  :  19.708273540843617  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  22.272865295410156  :  28.03021442617169  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  30.166854858398438  :  29.27105619045366  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  26.5079402923584  :  32.994400614752365  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  38.80057907104492  :  39.592020967948116  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  34.4834098815918  :  36.00340057230095  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:03:48 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1560/3150]\tTime 0.093 (0.092)\tLoss 0.0750 (0.7219)\tAcc@1 100.000 (85.026)\tAcc@5 100.000 (96.941)\tMem 1673MB\n",
      "layers.0.blocks.0.attn : Loss :  16.84233283996582  :  19.517644979341302  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  19.900632858276367  :  27.497375456512472  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  29.751962661743164  :  29.29190361125587  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  22.640180587768555  :  32.42056844052713  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  31.159982681274414  :  39.42177233796789  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  28.213787078857422  :  35.839863590824535  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:03:59 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1680/3150]\tTime 0.091 (0.092)\tLoss 0.4356 (0.7289)\tAcc@1 87.500 (84.407)\tAcc@5 100.000 (96.981)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  25.39866065979004  :  19.459000488748725  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  23.40313148498535  :  27.085950588201356  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  29.746841430664062  :  29.382242017810647  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  26.38568115234375  :  31.85198615832672  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  40.64054489135742  :  39.1808054672686  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  35.58027267456055  :  35.47192340376546  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:04:10 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1800/3150]\tTime 0.090 (0.092)\tLoss 0.4524 (0.7145)\tAcc@1 75.000 (84.738)\tAcc@5 100.000 (97.036)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.08433723449707  :  19.577584007195405  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  21.149784088134766  :  26.812907664263534  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  27.75672149658203  :  29.29603642533052  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  23.992055892944336  :  31.40872875999438  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  34.386775970458984  :  39.06567398910057  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  30.16362190246582  :  35.23313983660946  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:04:21 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1920/3150]\tTime 0.090 (0.092)\tLoss 1.8551 (0.7277)\tAcc@1 50.000 (84.422)\tAcc@5 100.000 (97.007)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.075992584228516  :  19.65423794031019  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  23.17392349243164  :  26.572857591151447  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  27.72801399230957  :  29.2583134684446  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  26.214780807495117  :  31.088390221265627  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  38.987403869628906  :  39.07862243771491  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  34.100833892822266  :  35.12661454629178  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:04:32 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2040/3150]\tTime 0.092 (0.092)\tLoss 1.3908 (0.7404)\tAcc@1 87.500 (84.119)\tAcc@5 100.000 (96.919)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.24017906188965  :  19.783499413994466  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  22.27122688293457  :  26.38115803065807  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  30.63126564025879  :  29.23526094895493  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  26.066112518310547  :  30.81504187707747  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  38.15665817260742  :  39.133795441511154  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  32.500762939453125  :  35.05117146213051  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:04:43 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2160/3150]\tTime 0.093 (0.092)\tLoss 0.3228 (0.7479)\tAcc@1 87.500 (84.000)\tAcc@5 100.000 (96.871)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.883197784423828  :  19.826291642548696  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  24.55963706970215  :  26.197962442739648  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  30.328170776367188  :  29.297399089271302  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  25.883525848388672  :  30.534808848644506  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  38.89476013183594  :  39.077641070082144  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  32.279327392578125  :  34.92848484563585  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:04:55 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2280/3150]\tTime 0.093 (0.092)\tLoss 0.8641 (0.7465)\tAcc@1 87.500 (84.004)\tAcc@5 87.500 (96.898)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.674192428588867  :  19.722716564360756  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  23.131454467773438  :  25.980830695534003  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  27.97136878967285  :  29.327644596492863  LR :  0.01\n",
      "layers.2.blocks.2.attn : Loss :  25.246084213256836  :  30.28191672663373  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  37.46986770629883  :  39.0284183624281  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  34.46002197265625  :  34.88044025332506  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:05:06 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2400/3150]\tTime 0.092 (0.092)\tLoss 4.1809 (0.7480)\tAcc@1 62.500 (84.038)\tAcc@5 75.000 (96.835)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.36786651611328  :  19.7013822819282  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  21.473098754882812  :  25.804780813119454  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  24.562358856201172  :  29.2542790750919  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.634044647216797  :  30.10420910073836  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  37.26068878173828  :  39.0937038979298  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  32.63042068481445  :  34.88429006999952  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:05:17 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2520/3150]\tTime 0.091 (0.092)\tLoss 1.6762 (0.7284)\tAcc@1 62.500 (84.470)\tAcc@5 100.000 (96.931)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.15323829650879  :  19.64632172454023  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  21.50996971130371  :  25.615446173165914  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  23.847381591796875  :  28.952197834122327  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  27.44606590270996  :  29.89014951561425  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  41.20048141479492  :  39.01729538799136  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  39.67432403564453  :  34.766360813641725  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:05:28 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2640/3150]\tTime 0.091 (0.092)\tLoss 0.0000 (0.7157)\tAcc@1 100.000 (84.793)\tAcc@5 100.000 (96.961)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  23.890127182006836  :  19.665319697688087  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  22.094860076904297  :  25.46470581496678  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  20.04581642150879  :  28.641552983247525  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  23.800172805786133  :  29.69480457472016  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  35.5892219543457  :  38.983288854146174  LR :  0.001\n",
      "layers.3.blocks.0.attn : Loss :  30.18450164794922  :  34.742667124154934  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:05:39 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2760/3150]\tTime 0.093 (0.092)\tLoss 0.4215 (0.7108)\tAcc@1 87.500 (84.865)\tAcc@5 100.000 (96.971)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.514629364013672  :  19.697495018041295  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  22.4959774017334  :  25.337631964070432  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  22.16726303100586  :  28.333281620996924  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  26.985313415527344  :  29.529697150175487  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  41.08901596069336  :  39.007390197400206  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  36.25823211669922  :  34.757140338874216  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:05:50 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2880/3150]\tTime 0.090 (0.092)\tLoss 1.4938 (0.7049)\tAcc@1 62.500 (85.031)\tAcc@5 100.000 (96.976)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.33913230895996  :  19.77585217953556  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  23.14647102355957  :  25.244997685256  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  21.870004653930664  :  28.068042085138472  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  27.2694149017334  :  29.42534980522349  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  42.08889389038086  :  39.14184045973688  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  38.0116081237793  :  34.826569766726855  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:06:01 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3000/3150]\tTime 0.092 (0.092)\tLoss 0.1614 (0.7114)\tAcc@1 100.000 (84.884)\tAcc@5 100.000 (96.934)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.410844802856445  :  19.758551699286578  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  22.04883575439453  :  25.141380448613077  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  21.181856155395508  :  27.81185671251164  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  24.553224563598633  :  29.286955671682232  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  36.054683685302734  :  39.10964265548162  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  32.69842529296875  :  34.82581311676829  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:06:12 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3120/3150]\tTime 0.091 (0.092)\tLoss 0.4914 (0.7112)\tAcc@1 87.500 (84.829)\tAcc@5 100.000 (96.964)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.56523895263672  :  19.731135840753605  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  22.94294548034668  :  25.0438957024904  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  20.61860466003418  :  27.561600609644604  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  24.900074005126953  :  29.151189788798803  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  35.7220573425293  :  39.069218965265776  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  29.58989906311035  :  34.790791508785546  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:06:15 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 91)\u001b[0m: INFO  * Acc@1 84.742 Acc@5 96.980\n",
      " Epoch :  2\n",
      "\u001b[32m[2022-12-04 22:06:15 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [0/3150]\tTime 0.808 (0.808)\tLoss 0.9377 (0.9377)\tAcc@1 87.500 (87.500)\tAcc@5 87.500 (87.500)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  23.76951789855957  :  23.76951789855957  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  24.520484924316406  :  24.520484924316406  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  24.252845764160156  :  24.252845764160156  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  27.812393188476562  :  27.812393188476562  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  41.58014678955078  :  41.58014678955078  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  39.59865951538086  :  39.59865951538086  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:06:26 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [120/3150]\tTime 0.092 (0.098)\tLoss 0.0329 (0.6422)\tAcc@1 100.000 (87.913)\tAcc@5 100.000 (97.314)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.751813888549805  :  19.38089370727539  LR :  0.001\n",
      "layers.1.blocks.0.attn : Loss :  22.22249412536621  :  22.568826628125404  LR :  0.001\n",
      "layers.2.blocks.0.attn : Loss :  21.33514404296875  :  21.788994544793752  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.432172775268555  :  26.114409265439374  LR :  0.001\n",
      "layers.2.blocks.4.attn : Loss :  38.5041389465332  :  39.1482359515734  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  34.67987060546875  :  34.72791927116962  LR :  0.001\n",
      "\u001b[32m[2022-12-04 22:06:37 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [240/3150]\tTime 0.091 (0.095)\tLoss 1.1817 (0.7811)\tAcc@1 75.000 (85.996)\tAcc@5 100.000 (96.629)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.843902587890625  :  19.21404572245491  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  21.224578857421875  :  22.43116613839177  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  22.242061614990234  :  21.68623085338545  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  26.946151733398438  :  26.21827186489501  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  40.856441497802734  :  39.32574382164666  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  37.03236389160156  :  35.74229212717397  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:06:48 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [360/3150]\tTime 0.092 (0.094)\tLoss 2.1530 (0.7340)\tAcc@1 62.500 (86.219)\tAcc@5 75.000 (96.849)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.052459716796875  :  19.09408240965529  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  20.777402877807617  :  22.329041837655275  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  21.69680404663086  :  21.713981850325567  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  27.03326988220215  :  26.128637313842773  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  38.76285171508789  :  39.160178810605714  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  36.21757888793945  :  35.53257396412688  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:06:59 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [480/3150]\tTime 0.091 (0.093)\tLoss 0.0184 (0.6688)\tAcc@1 100.000 (87.266)\tAcc@5 100.000 (97.011)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.802736282348633  :  19.336342240569497  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  23.65444564819336  :  22.35689129898801  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  20.739391326904297  :  21.650488201149287  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  23.43994903564453  :  25.987094776050466  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  32.04994201660156  :  38.68084074405028  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  27.774490356445312  :  34.62534331085776  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:07:10 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [600/3150]\tTime 0.091 (0.093)\tLoss 0.0089 (0.6921)\tAcc@1 100.000 (85.524)\tAcc@5 100.000 (97.130)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  21.143875122070312  :  19.590481415366174  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  21.61075782775879  :  22.43789295190186  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  20.628849029541016  :  21.658960088517226  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.62841033935547  :  25.99824467435256  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  38.64811325073242  :  38.86560567643202  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  33.11512756347656  :  34.64322606815078  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:07:21 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [720/3150]\tTime 0.091 (0.093)\tLoss 0.0287 (0.6524)\tAcc@1 100.000 (86.304)\tAcc@5 100.000 (97.295)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.684194564819336  :  19.41295619017539  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  22.837087631225586  :  22.379803344049336  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  21.699012756347656  :  21.54183876630172  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.925168991088867  :  25.90155052841123  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  38.2255859375  :  38.59608330905189  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  32.443477630615234  :  34.37654934379165  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:07:32 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [840/3150]\tTime 0.091 (0.092)\tLoss 2.9594 (0.6766)\tAcc@1 50.000 (85.330)\tAcc@5 100.000 (97.310)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.115215301513672  :  19.858450688873546  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  22.502674102783203  :  22.512942945772913  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  25.57945442199707  :  21.506450895747165  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  26.97797203063965  :  25.781776562031894  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  37.274288177490234  :  38.45203279456684  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  33.955360412597656  :  33.98453572303872  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:07:43 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [960/3150]\tTime 0.093 (0.092)\tLoss 0.1044 (0.6588)\tAcc@1 87.500 (85.874)\tAcc@5 100.000 (97.451)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.188610076904297  :  19.727309448289823  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  21.064069747924805  :  22.448817142958944  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  18.929927825927734  :  21.44440417334391  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  23.214258193969727  :  25.602185238412467  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  30.38848876953125  :  37.8236787162884  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  28.099775314331055  :  33.398029017770945  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:07:54 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1080/3150]\tTime 0.091 (0.092)\tLoss 0.0002 (0.6557)\tAcc@1 100.000 (85.858)\tAcc@5 100.000 (97.479)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.646015167236328  :  19.577922724883496  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  21.907386779785156  :  22.392822136821625  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  20.555179595947266  :  21.467035653522785  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.344379425048828  :  25.60343928165065  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  38.17360305786133  :  37.83951361283012  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  33.016136169433594  :  33.42526351335862  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:08:05 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1200/3150]\tTime 0.091 (0.092)\tLoss 1.0047 (0.6748)\tAcc@1 75.000 (85.564)\tAcc@5 100.000 (97.179)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.2194881439209  :  19.482298352339185  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  24.332061767578125  :  22.348643332297954  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  23.03317642211914  :  21.451409799669506  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  27.301332473754883  :  25.615224269704953  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  41.277523040771484  :  37.96551382333214  LR :  0.0001\n",
      "layers.3.blocks.0.attn : Loss :  35.96603775024414  :  33.60887058013484  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:08:16 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1320/3150]\tTime 0.092 (0.092)\tLoss 0.0275 (0.6746)\tAcc@1 100.000 (85.702)\tAcc@5 100.000 (97.133)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.04766082763672  :  19.679377755100848  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  23.727476119995117  :  22.427707982550597  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  21.47353744506836  :  21.506960638538622  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.31170654296875  :  25.667370578901593  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  38.84407424926758  :  38.164973027231476  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  33.3817024230957  :  33.66688842253645  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:08:27 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1440/3150]\tTime 0.091 (0.092)\tLoss 0.7338 (0.7030)\tAcc@1 87.500 (85.141)\tAcc@5 100.000 (97.059)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  15.19372844696045  :  19.691885751622323  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  19.760719299316406  :  22.437934017777028  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  20.761085510253906  :  21.52892266802288  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.987573623657227  :  25.69584084218943  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  38.01850509643555  :  38.281063623845284  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  34.43385314941406  :  33.68303138658787  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:08:38 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1560/3150]\tTime 0.093 (0.092)\tLoss 1.0422 (0.7246)\tAcc@1 87.500 (84.825)\tAcc@5 100.000 (97.045)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.989540100097656  :  19.528978355720515  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  20.431886672973633  :  22.339371338477125  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  19.68215560913086  :  21.480086901491838  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  23.23540687561035  :  25.657440246640682  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  31.567304611206055  :  38.25504714995145  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  27.119155883789062  :  33.6729109420141  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:08:49 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1680/3150]\tTime 0.092 (0.092)\tLoss 0.0070 (0.7237)\tAcc@1 100.000 (84.555)\tAcc@5 100.000 (97.122)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  24.541664123535156  :  19.44354137928978  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  23.673587799072266  :  22.28633382843762  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  21.740036010742188  :  21.412397284794253  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.952505111694336  :  25.551829865119203  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  40.61805725097656  :  38.11400845134492  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  35.6305046081543  :  33.44500100307703  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:09:00 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1800/3150]\tTime 0.092 (0.092)\tLoss 0.1711 (0.7102)\tAcc@1 100.000 (84.856)\tAcc@5 100.000 (97.189)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  15.646615982055664  :  19.568426534111534  LR :  0.0001\n",
      "layers.1.blocks.0.attn : Loss :  21.174217224121094  :  22.33774767022607  LR :  0.0001\n",
      "layers.2.blocks.0.attn : Loss :  20.17694091796875  :  21.392681106999476  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  23.326229095458984  :  25.51664210716133  LR :  0.0001\n",
      "layers.2.blocks.4.attn : Loss :  33.689571380615234  :  38.08251912563394  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  29.414884567260742  :  33.33247511682082  LR :  0.0001\n",
      "\u001b[32m[2022-12-04 22:09:11 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1920/3150]\tTime 0.091 (0.092)\tLoss 2.0865 (0.7201)\tAcc@1 37.500 (84.513)\tAcc@5 100.000 (97.195)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  21.53278923034668  :  19.628417643080894  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  22.130029678344727  :  22.37186292681081  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  21.1342716217041  :  21.428590989001155  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.177461624145508  :  25.55350134857988  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  38.21901321411133  :  38.14938624062804  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  34.14546203613281  :  33.37354515667448  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:09:22 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2040/3150]\tTime 0.091 (0.092)\tLoss 0.0137 (0.7286)\tAcc@1 100.000 (84.199)\tAcc@5 100.000 (97.134)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.77375602722168  :  19.758116893590753  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  22.99756622314453  :  22.43507330771581  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  23.103424072265625  :  21.459745721102113  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.565509796142578  :  25.609558083040348  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  37.586952209472656  :  38.26344801806049  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  31.716999053955078  :  33.46847882754425  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:09:33 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2160/3150]\tTime 0.092 (0.092)\tLoss 0.7131 (0.7287)\tAcc@1 75.000 (84.070)\tAcc@5 100.000 (97.143)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.73493003845215  :  19.77827708073536  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  23.28081703186035  :  22.463976425354485  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  21.98328399658203  :  21.503294169378744  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.451053619384766  :  25.611164402818304  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  38.01572799682617  :  38.24976266881262  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  33.21409606933594  :  33.467770579566235  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:09:44 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2280/3150]\tTime 0.091 (0.092)\tLoss 1.0292 (0.7319)\tAcc@1 75.000 (83.982)\tAcc@5 100.000 (97.107)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.63227653503418  :  19.664093367523503  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  21.67770767211914  :  22.439013416335438  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  20.723501205444336  :  21.531620784894983  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  24.861499786376953  :  25.604496079962903  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  38.37649154663086  :  38.23774558606246  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  34.64400863647461  :  33.511205643951136  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:09:55 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2400/3150]\tTime 0.093 (0.092)\tLoss 3.6617 (0.7312)\tAcc@1 75.000 (84.064)\tAcc@5 75.000 (97.090)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  15.148843765258789  :  19.649957617140473  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  21.415313720703125  :  22.439770536093054  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  22.425662994384766  :  21.536278918106227  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  26.501615524291992  :  25.64981911709685  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  37.64044189453125  :  38.33797433941328  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  32.40540313720703  :  33.60095483509018  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:10:06 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2520/3150]\tTime 0.092 (0.092)\tLoss 0.8876 (0.7125)\tAcc@1 75.000 (84.495)\tAcc@5 100.000 (97.149)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  21.23097801208496  :  19.59419073069299  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  22.579530715942383  :  22.421301543263393  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  23.941511154174805  :  21.52275183351207  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.586442947387695  :  25.65313616824122  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  40.30781936645508  :  38.314930230745574  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  36.64459991455078  :  33.59512009293444  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:10:17 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2640/3150]\tTime 0.090 (0.092)\tLoss 0.0077 (0.7051)\tAcc@1 100.000 (84.679)\tAcc@5 100.000 (97.184)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.862510681152344  :  19.61050812541784  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  22.79948616027832  :  22.415909865009926  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  20.482927322387695  :  21.523012301363035  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  24.51730728149414  :  25.64721928848908  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  36.171714782714844  :  38.30793352354571  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  32.29646301269531  :  33.63184969770958  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:10:28 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2760/3150]\tTime 0.090 (0.092)\tLoss 0.0246 (0.7020)\tAcc@1 100.000 (84.793)\tAcc@5 100.000 (97.189)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.085939407348633  :  19.632125450709037  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  22.324459075927734  :  22.41726651825537  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  21.63983917236328  :  21.50957430570465  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  27.444705963134766  :  25.651444640637997  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  41.32133102416992  :  38.33556941763641  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  37.02872848510742  :  33.71250764790845  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:10:39 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2880/3150]\tTime 0.090 (0.092)\tLoss 0.0748 (0.6964)\tAcc@1 100.000 (84.979)\tAcc@5 100.000 (97.184)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.30413818359375  :  19.69988242523077  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  22.083003997802734  :  22.440498509617242  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  21.58092498779297  :  21.519653969777618  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  26.096345901489258  :  25.70266601220885  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  40.80541229248047  :  38.49643309717996  LR :  1e-05\n",
      "layers.3.blocks.0.attn : Loss :  40.78560256958008  :  33.864193309557514  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:10:50 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3000/3150]\tTime 0.092 (0.092)\tLoss 0.2835 (0.7063)\tAcc@1 87.500 (84.797)\tAcc@5 100.000 (97.147)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.042673110961914  :  19.677460472808605  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  21.409746170043945  :  22.441891376275453  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  21.939172744750977  :  21.52222377862266  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.358591079711914  :  25.70468343539621  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  36.80475997924805  :  38.492573058354935  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  33.13096237182617  :  33.91413930875784  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:11:01 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3120/3150]\tTime 0.094 (0.092)\tLoss 0.8175 (0.7079)\tAcc@1 75.000 (84.712)\tAcc@5 100.000 (97.148)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.882095336914062  :  19.65882220355336  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  21.02743911743164  :  22.44328159905519  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  21.41000747680664  :  21.50930826712098  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  24.64022445678711  :  25.69392911916511  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  35.88606643676758  :  38.48141345470542  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  30.894521713256836  :  33.9209978200808  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:11:04 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 91)\u001b[0m: INFO  * Acc@1 84.687 Acc@5 97.171\n",
      " Epoch :  3\n",
      "\u001b[32m[2022-12-04 22:11:05 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [0/3150]\tTime 0.698 (0.698)\tLoss 1.2523 (1.2523)\tAcc@1 75.000 (75.000)\tAcc@5 100.000 (100.000)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.887784957885742  :  17.887784957885742  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  21.248462677001953  :  21.248462677001953  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  23.677167892456055  :  23.677167892456055  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  26.661771774291992  :  26.661771774291992  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  40.400413513183594  :  40.400413513183594  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  36.91523742675781  :  36.91523742675781  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:11:16 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [120/3150]\tTime 0.091 (0.097)\tLoss 1.1111 (0.6660)\tAcc@1 50.000 (87.500)\tAcc@5 100.000 (96.074)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.35404396057129  :  18.938601099755154  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  21.72694969177246  :  22.34912769853576  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  21.498781204223633  :  21.86387952694223  LR :  0.001\n",
      "layers.2.blocks.2.attn : Loss :  25.794727325439453  :  25.776515235585613  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  39.20582580566406  :  39.41543843923522  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  34.436073303222656  :  34.93684603163033  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:11:27 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [240/3150]\tTime 0.090 (0.094)\tLoss 0.6695 (0.7779)\tAcc@1 87.500 (84.855)\tAcc@5 100.000 (95.902)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.252500534057617  :  18.820856383232655  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  21.710718154907227  :  22.206327422525874  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  21.752788543701172  :  21.64491525230566  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  26.854907989501953  :  25.87930330300232  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  42.32684326171875  :  39.55786147058257  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  37.91730499267578  :  35.858054133371695  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:11:38 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [360/3150]\tTime 0.090 (0.093)\tLoss 0.2621 (0.7502)\tAcc@1 87.500 (85.353)\tAcc@5 100.000 (96.053)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.41263771057129  :  18.682697409738132  LR :  1e-05\n",
      "layers.1.blocks.0.attn : Loss :  21.21748924255371  :  22.12342061494526  LR :  1e-05\n",
      "layers.2.blocks.0.attn : Loss :  22.03460121154785  :  21.639052182353435  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  26.7731990814209  :  25.89802352081045  LR :  1e-05\n",
      "layers.2.blocks.4.attn : Loss :  40.775047302246094  :  39.41281754554474  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  35.61491394042969  :  35.65713909920563  LR :  1e-05\n",
      "\u001b[32m[2022-12-04 22:11:49 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [480/3150]\tTime 0.091 (0.093)\tLoss 0.3577 (0.6836)\tAcc@1 87.500 (86.746)\tAcc@5 100.000 (96.544)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.196149826049805  :  18.95844108200866  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  23.422576904296875  :  22.19291224648204  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  20.132659912109375  :  21.565898114083463  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  25.10798454284668  :  25.916009018673964  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  36.6609001159668  :  39.155267655973375  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  31.003170013427734  :  35.11786063148673  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:12:00 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [600/3150]\tTime 0.091 (0.093)\tLoss 0.0000 (0.7034)\tAcc@1 100.000 (85.254)\tAcc@5 100.000 (96.651)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.475202560424805  :  19.266145717284445  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  21.628189086914062  :  22.29398943858218  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  20.524442672729492  :  21.539604079108468  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  26.238290786743164  :  25.920491864399583  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  40.46940612792969  :  39.25518909905001  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  36.8060302734375  :  35.13030692305224  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:12:11 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [720/3150]\tTime 0.094 (0.092)\tLoss 0.0473 (0.6508)\tAcc@1 100.000 (86.425)\tAcc@5 100.000 (96.845)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.141986846923828  :  19.069018386438717  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  22.301517486572266  :  22.250333420942628  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  20.33078956604004  :  21.40275875043935  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  25.74933433532715  :  25.86456787172865  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  38.55923080444336  :  39.02999249906712  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  32.81827163696289  :  34.957209723336355  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:12:22 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [840/3150]\tTime 0.090 (0.092)\tLoss 1.4061 (0.6944)\tAcc@1 75.000 (85.181)\tAcc@5 100.000 (96.730)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.901132583618164  :  19.563661121726746  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  22.37692642211914  :  22.42047046792918  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  23.936403274536133  :  21.339590006861194  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  25.13239288330078  :  25.80064175273519  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  34.77119445800781  :  38.9119930607527  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  30.34980583190918  :  34.62349038685403  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:12:33 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [960/3150]\tTime 0.091 (0.092)\tLoss 0.6014 (0.6764)\tAcc@1 87.500 (85.653)\tAcc@5 87.500 (96.917)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.03972625732422  :  19.52504629796055  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  21.45616912841797  :  22.382020430311826  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  19.326717376708984  :  21.29265131240829  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  25.474002838134766  :  25.69084718646666  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  33.220767974853516  :  38.34716414214421  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  30.301530838012695  :  34.07775654232093  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:12:44 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1080/3150]\tTime 0.092 (0.092)\tLoss 0.0081 (0.6860)\tAcc@1 100.000 (85.569)\tAcc@5 100.000 (96.843)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.042325973510742  :  19.38472983354115  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  19.40882110595703  :  22.327039245760737  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  20.03586769104004  :  21.29904226678924  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  24.312185287475586  :  25.688541258848122  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  38.893192291259766  :  38.347709114081766  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  36.08799362182617  :  34.13120362584399  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:12:55 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1200/3150]\tTime 0.092 (0.092)\tLoss 0.0335 (0.6969)\tAcc@1 100.000 (85.429)\tAcc@5 100.000 (96.701)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  21.562137603759766  :  19.332470023562568  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  21.90593910217285  :  22.30540924326367  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  21.338205337524414  :  21.262908386052597  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  26.208032608032227  :  25.697665429730698  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  40.34928894042969  :  38.44491487736507  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  36.852516174316406  :  34.33177543341568  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:13:06 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1320/3150]\tTime 0.091 (0.092)\tLoss 0.0016 (0.6986)\tAcc@1 100.000 (85.466)\tAcc@5 100.000 (96.688)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  21.36015510559082  :  19.556252865065776  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  23.979341506958008  :  22.406624178196967  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  21.03596305847168  :  21.30544281294633  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  26.58686637878418  :  25.76070442892763  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  39.04081726074219  :  38.60766893656844  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  35.8646354675293  :  34.422405164951094  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:13:17 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1440/3150]\tTime 0.092 (0.092)\tLoss 1.0931 (0.7272)\tAcc@1 62.500 (84.915)\tAcc@5 100.000 (96.591)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  14.704245567321777  :  19.58948752981023  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  19.408676147460938  :  22.423668418635437  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  20.634254455566406  :  21.309801339937696  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  26.05406379699707  :  25.790381343558295  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  38.1180305480957  :  38.6683961191912  LR :  1.0000000000000002e-06\n",
      "layers.3.blocks.0.attn : Loss :  35.693641662597656  :  34.409125705959895  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:13:28 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1560/3150]\tTime 0.092 (0.092)\tLoss 0.0033 (0.7413)\tAcc@1 100.000 (84.801)\tAcc@5 100.000 (96.613)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  15.524628639221191  :  19.422467393038136  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  21.132051467895508  :  22.340428699979714  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  20.06924819946289  :  21.261140310176284  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  24.944562911987305  :  25.768536059998457  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  32.207130432128906  :  38.612590861885614  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  29.345781326293945  :  34.42133239169368  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:13:39 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1680/3150]\tTime 0.091 (0.092)\tLoss 0.0750 (0.7440)\tAcc@1 100.000 (84.399)\tAcc@5 100.000 (96.721)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  23.76053237915039  :  19.356547314237087  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  22.56187629699707  :  22.298387282381167  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  20.822525024414062  :  21.19574923177761  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  26.78680992126465  :  25.693143598385188  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  40.259132385253906  :  38.463902306088656  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  36.07508087158203  :  34.20261391226129  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:13:50 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1800/3150]\tTime 0.092 (0.092)\tLoss 1.2857 (0.7324)\tAcc@1 62.500 (84.578)\tAcc@5 100.000 (96.738)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.04116439819336  :  19.463174867603527  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  22.88112449645996  :  22.33270991027256  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  20.518051147460938  :  21.171329696333324  LR :  0.0001\n",
      "layers.2.blocks.2.attn : Loss :  24.196701049804688  :  25.66030290827097  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  34.24188995361328  :  38.40780198130589  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  29.965295791625977  :  34.108298023696214  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:14:01 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1920/3150]\tTime 0.093 (0.092)\tLoss 2.9773 (0.7414)\tAcc@1 37.500 (84.292)\tAcc@5 87.500 (96.694)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  21.360055923461914  :  19.51601321494933  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  23.58513069152832  :  22.362871147207393  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  21.057825088500977  :  21.206444998944196  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  26.051786422729492  :  25.690686100290073  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  39.698909759521484  :  38.45206240227544  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  33.74708938598633  :  34.09769630531418  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:14:12 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2040/3150]\tTime 0.091 (0.092)\tLoss 0.0357 (0.7437)\tAcc@1 100.000 (84.034)\tAcc@5 100.000 (96.736)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.916091918945312  :  19.664637650653816  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  21.759241104125977  :  22.43122454918465  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.0.attn : Loss :  22.835702896118164  :  21.24371103981089  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  25.143217086791992  :  25.736895860730872  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.4.attn : Loss :  37.84319305419922  :  38.5497157037521  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  32.59022903442383  :  34.11912846880646  LR :  1.0000000000000002e-06\n",
      "\u001b[32m[2022-12-04 22:14:23 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2160/3150]\tTime 0.090 (0.092)\tLoss 0.0645 (0.7491)\tAcc@1 100.000 (83.908)\tAcc@5 100.000 (96.732)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.3212890625  :  19.691906269255306  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  24.538005828857422  :  22.458798062078714  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  22.467178344726562  :  21.288856660806708  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  25.908769607543945  :  25.735750617610254  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  39.212440490722656  :  38.53192212370908  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  33.36894226074219  :  34.091591470057296  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:14:34 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2280/3150]\tTime 0.092 (0.092)\tLoss 0.8461 (0.7445)\tAcc@1 87.500 (83.889)\tAcc@5 100.000 (96.811)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  11.749175071716309  :  19.597688862233245  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  19.930112838745117  :  22.431584314315373  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  20.64335823059082  :  21.319217212782362  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  25.472414016723633  :  25.72373591384988  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  36.81574630737305  :  38.51077798561588  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  37.86845397949219  :  34.09907999084687  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:14:45 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2400/3150]\tTime 0.092 (0.092)\tLoss 1.5365 (0.7495)\tAcc@1 87.500 (83.908)\tAcc@5 87.500 (96.772)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  10.906990051269531  :  19.586550148563155  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  19.924442291259766  :  22.42952872216329  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.748287200927734  :  21.31642122518912  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  25.633840560913086  :  25.759890872108496  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  37.42626953125  :  38.59165325188627  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  33.57841873168945  :  34.16086076309859  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:14:56 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2520/3150]\tTime 0.093 (0.092)\tLoss 4.2144 (0.7290)\tAcc@1 37.500 (84.351)\tAcc@5 87.500 (96.861)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.243772506713867  :  19.527722710515242  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  21.20464515686035  :  22.41238016781472  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.17688751220703  :  21.315945066567785  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  25.644088745117188  :  25.75822346364634  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  39.14493179321289  :  38.55825641409644  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  35.21735763549805  :  34.12450325181682  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:15:07 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2640/3150]\tTime 0.092 (0.092)\tLoss 0.0000 (0.7172)\tAcc@1 100.000 (84.627)\tAcc@5 100.000 (96.919)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.626079559326172  :  19.574910837337402  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  24.079965591430664  :  22.411157051210644  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.073945999145508  :  21.32020889074231  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  24.94312858581543  :  25.744542908009503  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  36.2153205871582  :  38.54023970820966  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  32.18839645385742  :  34.147945447385155  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:15:18 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2760/3150]\tTime 0.091 (0.092)\tLoss 0.0017 (0.7142)\tAcc@1 100.000 (84.670)\tAcc@5 100.000 (96.949)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.17951202392578  :  19.590366747274818  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  22.8218994140625  :  22.409558281351025  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.678333282470703  :  21.30524510138711  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  26.769630432128906  :  25.741430384143026  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  41.93899154663086  :  38.554261793739684  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  37.29827880859375  :  34.18846734491478  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:15:29 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2880/3150]\tTime 0.091 (0.092)\tLoss 1.5793 (0.7061)\tAcc@1 75.000 (84.905)\tAcc@5 87.500 (96.941)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.642532348632812  :  19.676589294176722  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  21.889280319213867  :  22.43719595234165  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.378124237060547  :  21.310925362218548  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  26.24081802368164  :  25.787319498482532  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  40.79461669921875  :  38.69982768942275  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  37.4346809387207  :  34.32215545889984  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:15:40 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3000/3150]\tTime 0.090 (0.092)\tLoss 0.1367 (0.7135)\tAcc@1 87.500 (84.738)\tAcc@5 100.000 (96.918)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.210609436035156  :  19.646864546572754  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  22.533035278320312  :  22.443837360635037  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.42898178100586  :  21.30713516408227  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  25.06691551208496  :  25.78263229197242  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  37.74111557006836  :  38.68705056675749  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  33.15226745605469  :  34.35209844844098  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:15:51 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3120/3150]\tTime 0.092 (0.092)\tLoss 0.1061 (0.7098)\tAcc@1 100.000 (84.773)\tAcc@5 100.000 (96.984)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.265134811401367  :  19.628135169022208  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  21.91037368774414  :  22.445762259652316  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  19.85158920288086  :  21.288802742461524  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  24.082901000976562  :  25.769124897349077  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  36.24647903442383  :  38.66393514888021  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  31.64193344116211  :  34.337258442515406  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:15:54 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 91)\u001b[0m: INFO  * Acc@1 84.726 Acc@5 96.992\n",
      " Epoch :  4\n",
      "\u001b[32m[2022-12-04 22:15:55 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [0/3150]\tTime 0.812 (0.812)\tLoss 0.3084 (0.3084)\tAcc@1 87.500 (87.500)\tAcc@5 100.000 (100.000)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.7254581451416  :  17.7254581451416  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  22.311447143554688  :  22.311447143554688  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.541473388671875  :  21.541473388671875  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  25.93966293334961  :  25.93966293334961  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  40.07722473144531  :  40.07722473144531  LR :  1.0000000000000002e-07\n",
      "layers.3.blocks.0.attn : Loss :  37.199039459228516  :  37.199039459228516  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:16:06 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [120/3150]\tTime 0.091 (0.098)\tLoss 0.1954 (0.6850)\tAcc@1 87.500 (86.157)\tAcc@5 100.000 (96.281)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.526309967041016  :  19.05455126644166  LR :  1.0000000000000002e-06\n",
      "layers.1.blocks.0.attn : Loss :  22.300434112548828  :  22.390014128251508  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  20.777881622314453  :  21.455330620127278  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  25.647552490234375  :  25.767839857369417  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  39.31104278564453  :  39.32707331003236  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  33.45609664916992  :  34.85175785348435  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:16:17 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [240/3150]\tTime 0.091 (0.095)\tLoss 0.1349 (0.7624)\tAcc@1 87.500 (85.218)\tAcc@5 100.000 (95.747)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.900325775146484  :  18.915126503750496  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  20.931819915771484  :  22.1856740975281  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  22.304502487182617  :  21.31274519322819  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  27.458560943603516  :  25.87767019311422  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  41.53456497192383  :  39.47537457794569  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  39.2502326965332  :  35.752409377038724  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:16:28 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [360/3150]\tTime 0.091 (0.094)\tLoss 0.1355 (0.7304)\tAcc@1 87.500 (85.526)\tAcc@5 100.000 (96.087)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  21.086597442626953  :  18.808513049603828  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  22.812549591064453  :  22.13963332136582  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  22.321762084960938  :  21.395633819030593  LR :  1e-05\n",
      "layers.2.blocks.2.attn : Loss :  27.994173049926758  :  25.90930452637395  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  41.16206741333008  :  39.332508139993344  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  37.99115753173828  :  35.601881423485246  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:16:39 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [480/3150]\tTime 0.091 (0.093)\tLoss 0.1808 (0.6466)\tAcc@1 87.500 (87.032)\tAcc@5 100.000 (96.544)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.341951370239258  :  19.133401275920274  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  23.205677032470703  :  22.22370458591009  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.323070526123047  :  21.495478933417623  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  24.64573097229004  :  25.93629314002277  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  34.54537582397461  :  39.0736406845759  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  31.214336395263672  :  35.0083836696252  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:16:50 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [600/3150]\tTime 0.093 (0.093)\tLoss 0.0016 (0.6625)\tAcc@1 100.000 (85.483)\tAcc@5 100.000 (96.797)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.516347885131836  :  19.421429010476924  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  21.72513771057129  :  22.319363598815613  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.0.attn : Loss :  21.139461517333984  :  21.47483356621817  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  26.360918045043945  :  25.9458130504843  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.4.attn : Loss :  40.271148681640625  :  39.209172120308516  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  37.11695098876953  :  35.007236620352394  LR :  1.0000000000000002e-07\n",
      "\u001b[32m[2022-12-04 22:17:01 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [720/3150]\tTime 0.092 (0.093)\tLoss 0.5348 (0.6275)\tAcc@1 75.000 (86.182)\tAcc@5 100.000 (97.001)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  15.743355751037598  :  19.23260927266453  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  21.766515731811523  :  22.31265814327499  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.560077667236328  :  21.41129189225407  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.601892471313477  :  25.90201518340514  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  38.310909271240234  :  39.00102812175777  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  31.326833724975586  :  34.81567079647239  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:17:12 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [840/3150]\tTime 0.094 (0.093)\tLoss 2.4797 (0.6807)\tAcc@1 75.000 (85.181)\tAcc@5 100.000 (96.923)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.563716888427734  :  19.726434130447515  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  21.502117156982422  :  22.467174241999242  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.809946060180664  :  21.382817596090817  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.184711456298828  :  25.819545020104588  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  35.270084381103516  :  38.88568032680879  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  29.909883499145508  :  34.47005813952433  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:17:23 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [960/3150]\tTime 0.090 (0.093)\tLoss 0.1483 (0.6615)\tAcc@1 87.500 (85.718)\tAcc@5 100.000 (97.138)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  25.05858612060547  :  19.687943929936214  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  22.331417083740234  :  22.427568902086144  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.097715377807617  :  21.396012116669368  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  24.576290130615234  :  25.684694413215883  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  33.689735412597656  :  38.31224727729853  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  28.87340545654297  :  33.92435864181598  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:17:34 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1080/3150]\tTime 0.093 (0.092)\tLoss 0.4875 (0.6744)\tAcc@1 87.500 (85.523)\tAcc@5 100.000 (97.144)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.090469360351562  :  19.532673908978232  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  23.309459686279297  :  22.370214352885625  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.91676902770996  :  21.38065623266624  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.12220001220703  :  25.679310407823817  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  38.55693435668945  :  38.305059932317036  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  34.71939468383789  :  33.959396584622844  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:17:45 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1200/3150]\tTime 0.093 (0.092)\tLoss 0.1553 (0.6862)\tAcc@1 87.500 (85.314)\tAcc@5 100.000 (96.940)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  21.602052688598633  :  19.47537384223779  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  23.32183265686035  :  22.343219574444696  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  22.245845794677734  :  21.343843728477612  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  26.2741641998291  :  25.680949502543147  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  41.62080001831055  :  38.4036506189097  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  37.574039459228516  :  34.153377409084555  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:17:56 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1320/3150]\tTime 0.092 (0.092)\tLoss 0.0851 (0.6789)\tAcc@1 100.000 (85.513)\tAcc@5 100.000 (96.991)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.731956481933594  :  19.703819547793252  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  23.81948471069336  :  22.44381567170275  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  22.464492797851562  :  21.397153515061316  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  26.103422164916992  :  25.741483344714087  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  39.1036491394043  :  38.57058403078667  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  36.069923400878906  :  34.25267690821726  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:18:07 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1440/3150]\tTime 0.090 (0.092)\tLoss 0.8221 (0.7009)\tAcc@1 62.500 (84.984)\tAcc@5 100.000 (96.947)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  13.154244422912598  :  19.720182874151437  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  20.508934020996094  :  22.463131506851695  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.4699649810791  :  21.415687231451667  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.174591064453125  :  25.77517166111223  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  37.30741500854492  :  38.63649572389644  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  34.25548553466797  :  34.252756683963774  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:18:18 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1560/3150]\tTime 0.091 (0.092)\tLoss 0.2040 (0.7234)\tAcc@1 87.500 (84.649)\tAcc@5 100.000 (96.957)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  13.16254711151123  :  19.531276818959093  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  19.90708351135254  :  22.378160349609452  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.693742752075195  :  21.385807816910486  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.248424530029297  :  25.756484088494485  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  32.89254379272461  :  38.584658334989875  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  30.649700164794922  :  34.2794519103085  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:18:29 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1680/3150]\tTime 0.091 (0.092)\tLoss 0.0504 (0.7268)\tAcc@1 100.000 (84.228)\tAcc@5 100.000 (97.003)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  27.74736213684082  :  19.493425729514446  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  24.73151206970215  :  22.331621482073018  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.511232376098633  :  21.335999390683238  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.942426681518555  :  25.678946597742083  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  40.50532150268555  :  38.435795728966  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  35.27480697631836  :  34.06977418517159  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:18:40 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1800/3150]\tTime 0.090 (0.092)\tLoss 0.8991 (0.7074)\tAcc@1 75.000 (84.592)\tAcc@5 87.500 (97.064)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  19.15702247619629  :  19.597224314963402  LR :  1.0000000000000002e-07\n",
      "layers.1.blocks.0.attn : Loss :  20.309690475463867  :  22.36302481274814  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.563518524169922  :  21.31298356212423  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.074350357055664  :  25.646386696721766  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  34.512176513671875  :  38.37706929005099  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  32.13594436645508  :  33.98150768099991  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:18:51 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [1920/3150]\tTime 0.092 (0.092)\tLoss 1.0998 (0.7235)\tAcc@1 62.500 (84.246)\tAcc@5 100.000 (96.987)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.892337799072266  :  19.657411590707234  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  22.96718978881836  :  22.39135380079199  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.940710067749023  :  21.338136028585676  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.71698760986328  :  25.67150404799549  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  39.7036018371582  :  38.41650678762727  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  33.25160217285156  :  33.97783686406038  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:19:02 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2040/3150]\tTime 0.092 (0.092)\tLoss 0.0500 (0.7344)\tAcc@1 100.000 (83.991)\tAcc@5 100.000 (96.895)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.474319458007812  :  19.7899160506619  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  22.43041229248047  :  22.453536218197424  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  22.935718536376953  :  21.371697636575806  LR :  1.0000000000000002e-06\n",
      "layers.2.blocks.2.attn : Loss :  25.286264419555664  :  25.714902227383977  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  38.0121955871582  :  38.51421847009355  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  32.819366455078125  :  34.00190366850595  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:19:13 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2160/3150]\tTime 0.092 (0.092)\tLoss 0.0396 (0.7396)\tAcc@1 100.000 (83.948)\tAcc@5 100.000 (96.929)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  24.362041473388672  :  19.828871511629696  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  24.822338104248047  :  22.48681036116403  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  22.858003616333008  :  21.421220805015018  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.912338256835938  :  25.715401416021272  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  38.91366195678711  :  38.50537116965559  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  33.07483673095703  :  33.981958988583344  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:19:24 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2280/3150]\tTime 0.092 (0.092)\tLoss 1.5696 (0.7424)\tAcc@1 75.000 (83.900)\tAcc@5 100.000 (96.937)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  23.759368896484375  :  19.724660676072325  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  23.419261932373047  :  22.453428222442184  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.80988883972168  :  21.441533022207004  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.59911346435547  :  25.703110811533712  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  38.113258361816406  :  38.48342683784588  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  33.09212112426758  :  33.99093649670619  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:19:35 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2400/3150]\tTime 0.093 (0.092)\tLoss 2.6200 (0.7445)\tAcc@1 50.000 (83.892)\tAcc@5 87.500 (96.913)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.43012046813965  :  19.708288412002762  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  21.03533363342285  :  22.446887397210034  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.97681999206543  :  21.432147735856265  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  26.102561950683594  :  25.74256097332034  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  37.94037628173828  :  38.56982049143647  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  32.958885192871094  :  34.05932125932026  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:19:46 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2520/3150]\tTime 0.092 (0.092)\tLoss 1.4602 (0.7233)\tAcc@1 62.500 (84.356)\tAcc@5 100.000 (97.000)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.359203338623047  :  19.673707564829456  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  21.978132247924805  :  22.433644695350075  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.045799255371094  :  21.43726558299445  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.81749725341797  :  25.746099825370695  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  40.066795349121094  :  38.54761374086579  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  34.33842849731445  :  34.03956935312104  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:19:57 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2640/3150]\tTime 0.091 (0.092)\tLoss 0.0000 (0.7112)\tAcc@1 100.000 (84.618)\tAcc@5 100.000 (97.066)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  25.062847137451172  :  19.695234794862248  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  23.831357955932617  :  22.42623039268716  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.65435218811035  :  21.43201683272651  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  24.600814819335938  :  25.730016150830235  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  36.79242706298828  :  38.528105494922784  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  31.310420989990234  :  34.05830475671054  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:20:08 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2760/3150]\tTime 0.091 (0.092)\tLoss 0.0652 (0.7132)\tAcc@1 100.000 (84.661)\tAcc@5 100.000 (97.066)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.4949893951416  :  19.709388941882615  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  22.142803192138672  :  22.42271712861688  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.2820987701416  :  21.41126348971457  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.868019104003906  :  25.726716855003193  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  39.764347076416016  :  38.53599703635741  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  37.22419738769531  :  34.10676306807792  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:20:19 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [2880/3150]\tTime 0.096 (0.092)\tLoss 0.8372 (0.7067)\tAcc@1 87.500 (84.875)\tAcc@5 100.000 (97.067)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  14.467061042785645  :  19.770888519551928  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  20.623756408691406  :  22.442982176781033  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.189998626708984  :  21.4118541080643  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.89887809753418  :  25.772852807275044  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  39.34551239013672  :  38.68154121512797  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  37.66362762451172  :  34.24466660170868  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:20:30 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3000/3150]\tTime 0.090 (0.092)\tLoss 0.1550 (0.7189)\tAcc@1 87.500 (84.638)\tAcc@5 100.000 (97.026)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  16.453126907348633  :  19.731089630749814  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  21.681596755981445  :  22.44123593031347  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.90067481994629  :  21.40455282262785  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.268768310546875  :  25.76706329404176  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  36.739200592041016  :  38.66018687054063  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  32.46084976196289  :  34.27388408588433  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:20:41 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [3120/3150]\tTime 0.099 (0.092)\tLoss 0.3598 (0.7200)\tAcc@1 75.000 (84.616)\tAcc@5 100.000 (97.032)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  14.843911170959473  :  19.715629023338348  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  21.490427017211914  :  22.440522603673912  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.47597312927246  :  21.383308899551583  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  24.806644439697266  :  25.752381622084666  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  35.75062561035156  :  38.63898364735964  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  31.92612648010254  :  34.26125839817642  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:20:44 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 91)\u001b[0m: INFO  * Acc@1 84.556 Acc@5 97.052\n",
      " Epoch :  5\n",
      "\u001b[32m[2022-12-04 22:20:45 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [0/3150]\tTime 0.818 (0.818)\tLoss 0.0009 (0.0009)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  22.532386779785156  :  22.532386779785156  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  24.334991455078125  :  24.334991455078125  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  22.08821678161621  :  22.08821678161621  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  26.402652740478516  :  26.402652740478516  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  40.55975341796875  :  40.55975341796875  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  35.922611236572266  :  35.922611236572266  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:20:56 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [120/3150]\tTime 0.091 (0.098)\tLoss 1.5639 (0.6874)\tAcc@1 62.500 (86.674)\tAcc@5 100.000 (96.488)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  17.89879608154297  :  19.020030399984563  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  22.163177490234375  :  22.416403400011298  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.281986236572266  :  21.389496953034204  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.82204818725586  :  25.80885636510928  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  38.56297302246094  :  39.36157267546851  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  34.036468505859375  :  34.768613862597256  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:21:07 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [240/3150]\tTime 0.092 (0.095)\tLoss 0.0515 (0.7599)\tAcc@1 100.000 (85.425)\tAcc@5 100.000 (96.317)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.941917419433594  :  18.806775746009162  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  22.550195693969727  :  22.224117239481185  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.98000144958496  :  21.272677781670914  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  28.044647216796875  :  25.879648184875236  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  42.11848449707031  :  39.52553536189542  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  38.224117279052734  :  35.701628435696804  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:21:18 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [360/3150]\tTime 0.091 (0.094)\tLoss 0.0411 (0.7693)\tAcc@1 100.000 (85.145)\tAcc@5 100.000 (96.260)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.127904891967773  :  18.70494242057906  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  21.671079635620117  :  22.109225117268654  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.623374938964844  :  21.322236396599344  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.950157165527344  :  25.898843981882873  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  40.04795455932617  :  39.35764250398673  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  37.06037902832031  :  35.48764056578237  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:21:29 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [480/3150]\tTime 0.093 (0.093)\tLoss 0.2648 (0.6916)\tAcc@1 87.500 (86.720)\tAcc@5 100.000 (96.804)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  23.133255004882812  :  19.034916275008552  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  24.396385192871094  :  22.208352991052575  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.11722755432129  :  21.415942122683457  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  25.498981475830078  :  25.912227293557784  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  36.53461837768555  :  39.059091254728  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  30.38242530822754  :  34.88833043580244  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:21:40 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [600/3150]\tTime 0.092 (0.093)\tLoss 0.0000 (0.6977)\tAcc@1 100.000 (85.420)\tAcc@5 100.000 (97.026)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  20.122037887573242  :  19.313723533998512  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  22.7701358795166  :  22.309250712593066  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  21.264881134033203  :  21.40050030548045  LR :  1.0000000000000002e-07\n",
      "layers.2.blocks.2.attn : Loss :  26.259201049804688  :  25.907648223013727  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  41.00480651855469  :  39.1908201322381  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  37.32038497924805  :  34.926757215858494  LR :  1.0000000000000004e-08\n",
      "\u001b[32m[2022-12-04 22:21:51 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-197-2d8bca5164a9> 78)\u001b[0m: INFO Test: [720/3150]\tTime 0.091 (0.093)\tLoss 0.3909 (0.6565)\tAcc@1 87.500 (86.494)\tAcc@5 100.000 (97.243)\tMem 1674MB\n",
      "layers.0.blocks.0.attn : Loss :  18.44773292541504  :  19.128782499845084  LR :  1.0000000000000004e-08\n",
      "layers.1.blocks.0.attn : Loss :  22.29595947265625  :  22.289161893762596  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.0.attn : Loss :  20.743661880493164  :  21.339752821585012  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.2.attn : Loss :  25.79920196533203  :  25.85546576861039  LR :  1.0000000000000004e-08\n",
      "layers.2.blocks.4.attn : Loss :  39.37128829956055  :  38.98609344440095  LR :  1.0000000000000004e-08\n",
      "layers.3.blocks.0.attn : Loss :  32.66484069824219  :  34.76522972911142  LR :  1.0000000000000004e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-0dbba6bb9711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#                 loss_scaler=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Epoch : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0macc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuper_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# for layer_id in range(NUM_LAYERS):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#     for block_id in range(LAYER_DEPTHS[layer_id]):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-197-2d8bca5164a9>\u001b[0m in \u001b[0;36mteacher_validate\u001b[0;34m(config, data_loader, model)\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mlora_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRINT_FREQ\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;31m# lora_md.avg_loss = acc1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                     \u001b[0mlora_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lr_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                     \u001b[0mattn_loss_meters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-190-d227e02e824f>\u001b[0m in \u001b[0;36mdo_lr_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_lr_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, metrics, epoch)\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;31m# convert `metrics` to float, in case it's a zero-dim Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger.info(\"Start teacher-student training\")\n",
    "start_time = time.time()\n",
    "# for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n",
    "for epoch in range(0, 10):\n",
    "    # if not config.TEST.SEQUENTIAL:\n",
    "    # data_loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "    # train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler=lr_scheduler,\n",
    "    #                 loss_scaler=None)\n",
    "    print(\" Epoch : \", epoch)\n",
    "    acc1, acc5, loss = teacher_validate(config, data_loader_train, super_model)\n",
    "    # for layer_id in range(NUM_LAYERS):\n",
    "    #     for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "    #         if block_id%2 == 0:\n",
    "    #             avg_loss = all_lora_attns[layer_id][block_id].avg_loss / len(data_loader_train)\n",
    "    #             # all_lora_attns[layer_id][block_id].do_lr_step()\n",
    "    #             # all_lora_attns[layer_id][block_id].do_lr_step(avg_loss)\n",
    "    #             all_lora_attns[layer_id][block_id].avg_loss = 0\n",
    "\n",
    "    # acc1, acc5, loss = validate(config, data_loader_val, model)\n",
    "    # logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n",
    "    # max_accuracy = max(max_accuracy, acc1)\n",
    "    # logger.info(f'Max accuracy: {max_accuracy:.2f}%')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "logger.info('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Teacher inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_params = [p[1] for p in super_model.named_parameters() if \"lora\" in p[0]]\n",
    "\n",
    "optimizer = torch.optim.SGD(lora_params, momentum=config.TRAIN.OPTIMIZER.MOMENTUM, nesterov=True,\n",
    "                              lr=config.TRAIN.BASE_LR, weight_decay=config.TRAIN.WEIGHT_DECAY)\n",
    "\n",
    "loss_scaler = NativeScalerWithGradNormCount()\n",
    "\n",
    "lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "max_accuracy = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-12-04 23:17:36 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-34-58531364c9af> 3)\u001b[0m: INFO Start training\n",
      "\u001b[32m[2022-12-04 23:17:38 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [1/30][0/393]\teta 0:13:05 lr 0.0000997261\t wd 0.0000500000\ttime 1.9990 (1.9990)\tloss 4.1720 (4.1720)\tgrad_norm 20412088.0000 (20412088.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7896MB\n",
      "\u001b[32m[2022-12-04 23:19:49 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [1/30][120/393]\teta 0:05:01 lr 0.0000997261\t wd 0.0000500000\ttime 1.1269 (1.1034)\tloss 8.2928 (6.7201)\tgrad_norm 25304230.0000 (31591084.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:22:03 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [1/30][240/393]\teta 0:02:49 lr 0.0000997261\t wd 0.0000500000\ttime 1.0970 (1.1077)\tloss 5.7804 (6.7508)\tgrad_norm 22607152.0000 (29714714.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:24:15 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [1/30][360/393]\teta 0:00:36 lr 0.0000997261\t wd 0.0000500000\ttime 1.0938 (1.1052)\tloss 5.6065 (6.6392)\tgrad_norm 25139352.0000 (28053542.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:24:50 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 295)\u001b[0m: INFO EPOCH 1 training takes 0:07:14\n",
      "\u001b[32m[2022-12-04 23:24:51 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 337)\u001b[0m: INFO Test: [0/99]\tTime 0.877 (0.877)\tLoss 3.4436 (3.4436)\tAcc@1 4.688 (4.688)\tAcc@5 46.875 (46.875)\tMem 7899MB\n",
      "\u001b[32m[2022-12-04 23:25:30 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 343)\u001b[0m: INFO  * Acc@1 3.095 Acc@5 14.302\n",
      "\u001b[32m[2022-12-04 23:25:30 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-34-58531364c9af> 14)\u001b[0m: INFO Accuracy of the network on the 6300 test images: 3.1%\n",
      "\u001b[32m[2022-12-04 23:25:30 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-34-58531364c9af> 16)\u001b[0m: INFO Max accuracy: 3.10%\n",
      "\u001b[32m[2022-12-04 23:25:32 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [2/30][0/393]\teta 0:13:45 lr 0.0000989074\t wd 0.0000500000\ttime 2.1006 (2.1006)\tloss 4.2208 (4.2208)\tgrad_norm 22412940.0000 (22412940.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:27:43 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [2/30][120/393]\teta 0:05:01 lr 0.0000989074\t wd 0.0000500000\ttime 1.0964 (1.1036)\tloss 8.5008 (6.8068)\tgrad_norm 36936120.0000 (28367458.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:29:54 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [2/30][240/393]\teta 0:02:48 lr 0.0000989074\t wd 0.0000500000\ttime 1.0791 (1.0985)\tloss 5.8306 (6.8001)\tgrad_norm 16576036.0000 (27656534.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:32:05 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [2/30][360/393]\teta 0:00:36 lr 0.0000989074\t wd 0.0000500000\ttime 1.0807 (1.0962)\tloss 5.9627 (6.6952)\tgrad_norm 22813230.0000 (26597934.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:32:40 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 295)\u001b[0m: INFO EPOCH 2 training takes 0:07:10\n",
      "\u001b[32m[2022-12-04 23:32:41 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 337)\u001b[0m: INFO Test: [0/99]\tTime 0.714 (0.714)\tLoss 3.4436 (3.4436)\tAcc@1 4.688 (4.688)\tAcc@5 46.875 (46.875)\tMem 7899MB\n",
      "\u001b[32m[2022-12-04 23:33:20 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 343)\u001b[0m: INFO  * Acc@1 3.095 Acc@5 14.302\n",
      "\u001b[32m[2022-12-04 23:33:20 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-34-58531364c9af> 14)\u001b[0m: INFO Accuracy of the network on the 6300 test images: 3.1%\n",
      "\u001b[32m[2022-12-04 23:33:20 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-34-58531364c9af> 16)\u001b[0m: INFO Max accuracy: 3.10%\n",
      "\u001b[32m[2022-12-04 23:33:22 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [3/30][0/393]\teta 0:13:24 lr 0.0000975528\t wd 0.0000500000\ttime 2.0461 (2.0461)\tloss 4.5967 (4.5967)\tgrad_norm 20672378.0000 (20672378.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:35:33 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [3/30][120/393]\teta 0:04:59 lr 0.0000975528\t wd 0.0000500000\ttime 1.0788 (1.0983)\tloss 8.3289 (6.7525)\tgrad_norm 37145016.0000 (27547496.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:37:45 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [3/30][240/393]\teta 0:02:48 lr 0.0000975528\t wd 0.0000500000\ttime 1.1440 (1.1007)\tloss 5.6310 (6.7182)\tgrad_norm 59013768.0000 (35191536.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:39:59 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [3/30][360/393]\teta 0:00:36 lr 0.0000975528\t wd 0.0000500000\ttime 1.0754 (1.1043)\tloss 5.6873 (6.6067)\tgrad_norm 36402104.0000 (33927792.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:40:33 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 295)\u001b[0m: INFO EPOCH 3 training takes 0:07:13\n",
      "\u001b[32m[2022-12-04 23:40:34 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 337)\u001b[0m: INFO Test: [0/99]\tTime 0.739 (0.739)\tLoss 3.4436 (3.4436)\tAcc@1 4.688 (4.688)\tAcc@5 46.875 (46.875)\tMem 7899MB\n",
      "\u001b[32m[2022-12-04 23:41:13 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 343)\u001b[0m: INFO  * Acc@1 3.095 Acc@5 14.302\n",
      "\u001b[32m[2022-12-04 23:41:13 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-34-58531364c9af> 14)\u001b[0m: INFO Accuracy of the network on the 6300 test images: 3.1%\n",
      "\u001b[32m[2022-12-04 23:41:13 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-34-58531364c9af> 16)\u001b[0m: INFO Max accuracy: 3.10%\n",
      "\u001b[32m[2022-12-04 23:41:15 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [4/30][0/393]\teta 0:12:06 lr 0.0000956773\t wd 0.0000500000\ttime 1.8496 (1.8496)\tloss 4.2878 (4.2878)\tgrad_norm 15748820.0000 (15748820.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n",
      "\u001b[32m[2022-12-04 23:43:25 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 282)\u001b[0m: INFO Train: [4/30][120/393]\teta 0:04:59 lr 0.0000956773\t wd 0.0000500000\ttime 1.0900 (1.0961)\tloss 7.8151 (6.7951)\tgrad_norm 27940698.0000 (27776036.0000)\tloss_scale 65536.0000 (65536.0000)\tmem 7899MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-58531364c9af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler,\n\u001b[0;32m---> 10\u001b[0;31m                     loss_scaler, logger)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/ece.cmu.edu/usr/bmarimut/Private/ODML_project/ODML-SwinT-JetNano/ODML-Swin-Transformer/main.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(config, model, criterion, data_loader, optimizer, epoch, mixup_fn, lr_scheduler, loss_scaler, logger)\u001b[0m\n\u001b[1;32m    254\u001b[0m         grad_norm = loss_scaler(loss, optimizer, clip_grad=config.TRAIN.CLIP_GRAD,\n\u001b[1;32m    255\u001b[0m                                 \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_second_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                                 update_grad=(idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0)\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACCUMULATION_STEPS\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/afs/ece.cmu.edu/usr/bmarimut/Private/ODML_project/ODML-SwinT-JetNano/ODML-Swin-Transformer/utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, loss, optimizer, clip_grad, parameters, create_graph, update_grad)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclip_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = build_optimizer(config, model)\n",
    "\n",
    "logger.info(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(1, 10):\n",
    "    if not config.TEST.SEQUENTIAL:\n",
    "        data_loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "    train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler,\n",
    "                    loss_scaler, logger)\n",
    "    \n",
    "\n",
    "    acc1, acc5, loss = validate(config, data_loader_val, model, logger)\n",
    "    logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n",
    "    max_accuracy = max(max_accuracy, acc1)\n",
    "    logger.info(f'Max accuracy: {max_accuracy:.2f}%')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "logger.info('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-12-04 23:16:43 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 337)\u001b[0m: INFO Test: [0/99]\tTime 0.925 (0.925)\tLoss 3.4437 (3.4437)\tAcc@1 4.688 (4.688)\tAcc@5 46.875 (46.875)\tMem 7863MB\n",
      "\u001b[32m[2022-12-04 23:17:20 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(main.py 343)\u001b[0m: INFO  * Acc@1 3.079 Acc@5 14.302\n",
      "\u001b[32m[2022-12-04 23:17:20 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-33-d2c43735fa0c> 2)\u001b[0m: INFO Accuracy of the network on the 6300 test images: 3.1%\n",
      "\u001b[32m[2022-12-04 23:17:20 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-33-d2c43735fa0c> 4)\u001b[0m: INFO Max accuracy: 3.08%\n"
     ]
    }
   ],
   "source": [
    "acc1, acc5, loss = validate(config, data_loader_val, model, logger)\n",
    "logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n",
    "max_accuracy = max(max_accuracy, acc1)\n",
    "logger.info(f'Max accuracy: {max_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
