{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/andrew.cmu.edu/usr8/bmarimut/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\n",
      "Tutel has not been installed. To use Swin-MoE, please install Tutel; otherwise, just ignore this.\n",
      "To use FusedLAMB or FusedAdam, please install apex.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import argparse\n",
    "from logger import create_logger\n",
    "import os\n",
    "\n",
    "\n",
    "from utils import load_checkpoint, load_pretrained\n",
    "from config import get_config\n",
    "from data import build_loader\n",
    "from models import build_model\n",
    "\n",
    "from main import train_one_epoch, validate, throughput\n",
    "\n",
    "from config import get_only_config\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "\n",
    "import datetime\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.utils import accuracy, AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml\n"
     ]
    }
   ],
   "source": [
    "config_path = 'configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml'\n",
    "config = get_only_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.defrost()\n",
    "config.OUTPUT = \"/afs/ece.cmu.edu/usr/bmarimut/Private/output\"\n",
    "# config.MODEL.PRETRAINED = \"/afs/ece.cmu.edu/usr/ashwinve/Public/ckpt_epoch_29_6.pth\"\n",
    "config.MODEL.PRETRAINED = \"/afs/ece.cmu.edu/usr/ashwinve/Public/golden_resisc45.pth\"\n",
    "config.MODEL.RESUME = \"/afs/ece.cmu.edu/usr/ashwinve/Public/golden_resisc45.pth\"\n",
    "config.DATA.CACHE_MODE = 'no'\n",
    "config.DATA.DATA_PATH = './data/RESISC45/'\n",
    "config.DATA.ZIP_MODE = True\n",
    "config.PRINT_FREQ = 120\n",
    "config.DATA.BATCH_SIZE = 8\n",
    "config.freeze()\n",
    "os.makedirs(config.OUTPUT, exist_ok=True)\n",
    "logger = create_logger(output_dir=config.OUTPUT, name=f\"{config.MODEL.NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/andrew.cmu.edu/usr8/bmarimut/.local/lib/python3.6/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "lora_selector = 4\n",
    "lora_layer = 0\n",
    "keep_qkv = True\n",
    "model = build_model(config, lora_selector, lora_layer, keep_qkv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.qkv_wUSprime', 'layers.0.blocks.0.attn.qkv_wVprime', 'layers.0.blocks.0.attn.qkv_b', 'layers.0.blocks.1.attn.qkv_wUSprime', 'layers.0.blocks.1.attn.qkv_wVprime', 'layers.0.blocks.1.attn.qkv_b', 'layers.1.blocks.0.attn.qkv_wUSprime', 'layers.1.blocks.0.attn.qkv_wVprime', 'layers.1.blocks.0.attn.qkv_b', 'layers.1.blocks.1.attn.qkv_wUSprime', 'layers.1.blocks.1.attn.qkv_wVprime', 'layers.1.blocks.1.attn.qkv_b', 'layers.2.blocks.0.attn.qkv_wUSprime', 'layers.2.blocks.0.attn.qkv_wVprime', 'layers.2.blocks.0.attn.qkv_b', 'layers.2.blocks.1.attn.qkv_wUSprime', 'layers.2.blocks.1.attn.qkv_wVprime', 'layers.2.blocks.1.attn.qkv_b', 'layers.2.blocks.2.attn.qkv_wUSprime', 'layers.2.blocks.2.attn.qkv_wVprime', 'layers.2.blocks.2.attn.qkv_b', 'layers.2.blocks.3.attn.qkv_wUSprime', 'layers.2.blocks.3.attn.qkv_wVprime', 'layers.2.blocks.3.attn.qkv_b', 'layers.2.blocks.4.attn.qkv_wUSprime', 'layers.2.blocks.4.attn.qkv_wVprime', 'layers.2.blocks.4.attn.qkv_b', 'layers.2.blocks.5.attn.qkv_wUSprime', 'layers.2.blocks.5.attn.qkv_wVprime', 'layers.2.blocks.5.attn.qkv_b', 'layers.3.blocks.0.attn.qkv_wUSprime', 'layers.3.blocks.0.attn.qkv_wVprime', 'layers.3.blocks.0.attn.qkv_b', 'layers.3.blocks.1.attn.qkv_wUSprime', 'layers.3.blocks.1.attn.qkv_wVprime', 'layers.3.blocks.1.attn.qkv_b'], unexpected_keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/ece.cmu.edu/usr/bmarimut/Private/ODML_project/ODML-SwinT-JetNano/ODML-Swin-Transformer/models/swin_transformer.py:243: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  new_sd['qkv_wVprime'] = torch.tensor([q_wVprime, k_wVprime, v_wVprime])\n"
     ]
    }
   ],
   "source": [
    "model.init_qkv_low_rank_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CONFIGS = 5\n",
    "\n",
    "for i in range(NUM_CONFIGS):\n",
    "\n",
    "    config_name = \"config\"+str(i)+\"_chkpt.pth\"\n",
    "\n",
    "    lora_selector = i\n",
    "    lora_layer = 0\n",
    "    keep_qkv = True\n",
    "\n",
    "    my_model = build_model(config, lora_selector, lora_layer, keep_qkv)\n",
    "\n",
    "    checkpoint = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n",
    "    my_model.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "    my_model.init_qkv_low_rank_weights()\n",
    "\n",
    "    new_sd = copy.deepcopy(my_model.state_dict())\n",
    "\n",
    "    del_keys = [key for key in new_sd.keys() if (\"qkv.weight\" in key or \"qkv.bias\" in key)]\n",
    "    for key in del_keys:\n",
    "        del new_sd[key]\n",
    "\n",
    "    torch.save(new_sd, config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_pretrained(config, model, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model_without_ddp = model\n",
    "super_model = model\n",
    "dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze specific layers for downstream task training\n",
    "lora_w_name_pattern = ['q_b', 'k_b', 'v_b', 'prime']\n",
    "\n",
    "n_parameters_orig = 0\n",
    "n_parameters_lora = 0\n",
    "model_named_params = list(model.named_parameters())\n",
    "num_params = len(model_named_params)\n",
    "for param_iter in range(num_params):\n",
    "    param_name, param = model_named_params[param_iter]\n",
    "    # if \"lora\" not in param_name:\n",
    "    if any(substring in param_name for substring in lora_w_name_pattern):\n",
    "        n_parameters_lora += param.numel()\n",
    "    elif \"qkv.weight\" in param_name or \"qkv.bias\" in param_name:\n",
    "        n_parameters_orig += param.numel()\n",
    "    else:\n",
    "        n_parameters_orig += param.numel()\n",
    "        n_parameters_lora += param.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"number of params: {n_parameters_orig}\")\n",
    "logger.info(f\"number of LoRA params: {n_parameters_lora}\")\n",
    "flops = 0\n",
    "flops_lora = 0\n",
    "if hasattr(model, 'flops'):\n",
    "    flops = model.flops()\n",
    "    logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n",
    "    flops_lora = model.flops_lora()\n",
    "    logger.info(f\"Lora number of GFLOPs: {flops_lora / 1e9}\")\n",
    "\n",
    "model.cuda()\n",
    "model_without_ddp = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora rank 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Param savings = \", (n_parameters_orig - n_parameters_lora)/n_parameters_orig*100.0, \" % \")\n",
    "print(\"FLOPs savings = \", (flops - flops_lora)/flops*100.0, \" % \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lora 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Param savings = \", (n_parameters_orig - n_parameters_lora)/n_parameters_orig*100.0, \" % \")\n",
    "print(\"FLOPs savings = \", (flops - flops_lora)/flops*100.0, \" % \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_SELECTOR = 0\n",
    "LORA_RANK_DICT = {\n",
    "    'layers.0.blocks.0.attn': [9,   12, 13, 49],\n",
    "    'layers.0.blocks.1.attn': [28,  35, 38, 49],\n",
    "    'layers.1.blocks.0.attn': [24,  32, 39, 49],\n",
    "    'layers.1.blocks.1.attn': [19,  22, 23, 49],\n",
    "    'layers.2.blocks.0.attn': [16,  18, 19, 49],\n",
    "    'layers.2.blocks.1.attn': [20,  22, 22, 49],\n",
    "    'layers.2.blocks.2.attn': [22,  26, 30, 49],\n",
    "    'layers.2.blocks.3.attn': [22,  25, 26, 49],\n",
    "    'layers.2.blocks.4.attn': [24,  24, 25, 49],\n",
    "    'layers.2.blocks.5.attn': [23,  24, 24, 49],\n",
    "    'layers.3.blocks.0.attn': [20,  21, 22, 49],\n",
    "    'layers.3.blocks.1.attn': [16,  16, 17, 49]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LORA_SELECTOR = 2\n",
    "LORA_RANK_DICT = {\n",
    "    'layers.0.blocks.0.attn': [300, 325, 350, 384],\n",
    "    'layers.0.blocks.1.attn': [300, 325, 350, 384],\n",
    "    'layers.1.blocks.0.attn': [300, 325, 350, 384],\n",
    "    'layers.1.blocks.1.attn': [300, 325, 350, 384],\n",
    "    'layers.2.blocks.0.attn': [300, 325, 350, 384],\n",
    "    'layers.2.blocks.1.attn': [300, 325, 350, 384],\n",
    "    'layers.2.blocks.2.attn': [300, 325, 350, 384],\n",
    "    'layers.2.blocks.3.attn': [300, 325, 350, 384],\n",
    "    'layers.2.blocks.4.attn': [300, 325, 350, 384],\n",
    "    'layers.2.blocks.5.attn': [300, 325, 350, 384],\n",
    "    'layers.3.blocks.0.attn': [300, 325, 350, 384],\n",
    "    'layers.3.blocks.1.attn': [300, 325, 350, 384]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_md = get_attn(2,5)\n",
    "# print(get_attn(2,5).input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(list(model.children())[2][0].children())[0][1])\n",
    "# print(get_attn(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# # import models\n",
    "# import sys\n",
    "# importlib.reload(sys.modules['models'])\n",
    "# from models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block(my_model, layer_id, block_id):\n",
    "    # print(list(list(model.children())[2][layer_id].children())[0][block_id])\n",
    "    return list(list(my_model.children())[2][layer_id].children())[0][block_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn(my_model, layer_id, block_id):\n",
    "    block = get_block(my_model, layer_id, block_id)\n",
    "    return list(block.children())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_attn(super_model, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_block(super_model, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_DEPTHS = [2, 2, 6, 2]\n",
    "NUM_LAYERS = len(LAYER_DEPTHS)\n",
    "NUM_HEADS = [ 3, 6, 12, 24 ]\n",
    "H = 224\n",
    "W = 224\n",
    "B = 1\n",
    "L = 224 * 224\n",
    "window_size = 7\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# C = 96 * 2**i\n",
    "# num_heads = NUM_HEADS[i]\n",
    "# dim = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lora_attns = []\n",
    "for layer_id in range(NUM_LAYERS):\n",
    "    layer_attns = []\n",
    "    C = 96 * 2**layer_id\n",
    "    num_heads = NUM_HEADS[layer_id]\n",
    "    dim = C\n",
    "    for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        if layer_id>=3:\n",
    "            param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "            lora_attn = LORA_WindowAttention3(dim, LORA_RANK_DICT[param_name][LORA_SELECTOR],\n",
    "            to_2tuple(window_size), num_heads, param_name)\n",
    "            lora_attn.load_pretrained_weights(super_model)\n",
    "            lora_attn.init_low_rank_approx_weights()\n",
    "            lora_attn.cuda()\n",
    "            layer_attns.append(lora_attn)\n",
    "        else:\n",
    "            layer_attns.append(None)\n",
    "    all_lora_attns.append(layer_attns)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = LORA_WindowAttention2(dim, LORA_RANK_DICT[param_name][LORA_SELECTOR],\n",
    "            to_2tuple(window_size), num_heads, param_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.load_state_dict(torch.load(\"layers.3.blocks.0.attn.pth\")['state_dict'])\n",
    "tmp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save LoRA Students to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_id in range(NUM_LAYERS):\n",
    "    for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        if block_id%2 == 0:\n",
    "            param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "            torch.save({\n",
    "                'state_dict': all_lora_attns[layer_id][block_id].state_dict()},\n",
    "                param_name+\".pth\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lora_attns = []\n",
    "for layer_id in range(NUM_LAYERS):\n",
    "    layer_attns = []\n",
    "    C = 96 * 2**layer_id\n",
    "    num_heads = NUM_HEADS[layer_id]\n",
    "    dim = C\n",
    "    for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        if block_id%2 == 0 and layer_id>=2:\n",
    "            param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "            lora_attn = LORA_WindowAttention2(dim, LORA_RANK_DICT[param_name][LORA_SELECTOR],\n",
    "            to_2tuple(window_size), num_heads, param_name)\n",
    "            lora_attn.load_state_dict(torch.load(param_name+\".pth\")['state_dict'])\n",
    "            lora_attn.cuda()\n",
    "            layer_attns.append(lora_attn)\n",
    "        else:\n",
    "            layer_attns.append(None)\n",
    "    all_lora_attns.append(layer_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora_weights(super_model):\n",
    "    new_sm_sd = copy.deepcopy(super_model.state_dict())\n",
    "    for layer_id in range(NUM_LAYERS):\n",
    "        for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "            if block_id%2 == 0 and layer_id>=2:\n",
    "                param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "                # for name in params_names_list:\n",
    "                # print(new_sd)\n",
    "                new_sm_sd[param_name+'.lora_k.weight'] = all_lora_attns[layer_id][block_id].state_dict()['lora_k.weight']\n",
    "                new_sm_sd[param_name+'.lora_v.weight'] = all_lora_attns[layer_id][block_id].state_dict()['lora_v.weight']\n",
    "                new_sm_sd[param_name+'.lora_rpb.weight'] = all_lora_attns[layer_id][block_id].state_dict()['lora_rpb.weight']\n",
    "                \n",
    "                super_model.load_state_dict(new_sm_sd)\n",
    "    return super_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_name+'.lora_k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model = load_lora_weights(super_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super_model.state_dict()['layers.3.blocks.0.attn.lora_k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_lora_attns[0][0].state_dict()\n",
    "# [p[1] for p in all_lora_attns[0][0].named_parameters() if \"lora\" in p[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "# # create a loss function\n",
    "# criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student teacher training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "def teacher_validate(config, data_loader, model):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    # attn_loss_meters = []\n",
    "    # for layer_id in range(NUM_LAYERS):\n",
    "    #     block_loss = []\n",
    "    #     for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "    #         if block_id%2 == 0:\n",
    "    #             atn_loss_meter = AverageMeter()\n",
    "    #             block_loss.append(atn_loss_meter)\n",
    "    #         else:\n",
    "    #             block_loss.append(None)\n",
    "    #     attn_loss_meters.append(block_loss)\n",
    "\n",
    "    acc1_meter = AverageMeter()\n",
    "    acc5_meter = AverageMeter()\n",
    "\n",
    "    end = time.time()\n",
    "    for idx, (images, target) in enumerate(data_loader):\n",
    "        images = images.cuda(non_blocking=False)\n",
    "        target = target.cuda(non_blocking=False)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled=config.AMP_ENABLE):\n",
    "                output = model(images)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        loss = criterion(output, target)\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        \n",
    "        # Train student\n",
    "        # for layer_id in range(NUM_LAYERS):\n",
    "        #     for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        #         if block_id%2 == 0:\n",
    "        #             lora_md = all_lora_attns[layer_id][block_id]\n",
    "        #             teacher_attn = get_attn(super_model, layer_id, block_id)\n",
    "        #             lora_md.forward(teacher_attn.input)\n",
    "        #             lora_md.do_backward(teacher_attn.output, idx % config.PRINT_FREQ == 0)\n",
    "        #             lora_md.do_lr_step()\n",
    "        #             attn_loss_meters[layer_id][block_id].update(lora_md.loss.item(), 1)\n",
    "                    \n",
    "\n",
    "        # Train student\n",
    "        # for layer_id in range(NUM_LAYERS):\n",
    "        #     for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "        #         if block_id%2 == 0:\n",
    "        #             lora_md = all_lora_attns[layer_id][block_id]\n",
    "        #             teacher_attn = get_attn(super_model, layer_id, block_id)\n",
    "        #             param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "\n",
    "        #             # with open(param_name+'.ndarray',mode='ba+') as f:\n",
    "        #             #     teacher_attn.input.cpu().numpy().tofile(f)\n",
    "        #             #     teacher_attn.output.cpu().numpy().tofile(f)\n",
    "        #             lora_md.forward(teacher_attn.input)\n",
    "        #             lora_md.do_backward(teacher_attn.output, idx % config.PRINT_FREQ == 0)\n",
    "        #             lora_md.do_lr_step()\n",
    "\n",
    "\n",
    "\n",
    "        loss_meter.update(loss.item(), target.size(0))\n",
    "        acc1_meter.update(acc1.item(), target.size(0))\n",
    "        acc5_meter.update(acc5.item(), target.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if idx % config.PRINT_FREQ == 0:\n",
    "            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n",
    "            logger.info(\n",
    "                f'Test: [{idx}/{len(data_loader)}]\\t'\n",
    "                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n",
    "                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n",
    "                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n",
    "                f'Mem {memory_used:.0f}MB')\n",
    "            # for layer_id in range(NUM_LAYERS):\n",
    "            #     for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "            #         if block_id%2 == 0:\n",
    "            #             param_name = 'layers.' + str(layer_id) + \".blocks.\" + str(block_id) + \".attn\"\n",
    "            #             print(param_name, \": Loss : \", attn_loss_meters[layer_id][block_id].val, \" : \", attn_loss_meters[layer_id][block_id].avg, \n",
    "            #                 \" LR : \", all_lora_attns[layer_id][block_id].lr_scheduler.get_last_lr())\n",
    "        \n",
    "    logger.info(f' * Acc@1 {acc1_meter.avg:.3f} Acc@5 {acc5_meter.avg:.3f}')\n",
    "    return acc1_meter.avg, acc5_meter.avg, loss_meter.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student training\")\n",
    "start_time = time.time()\n",
    "# for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n",
    "for epoch in range(0, 10):\n",
    "    # if not config.TEST.SEQUENTIAL:\n",
    "    # data_loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "    # train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler=lr_scheduler,\n",
    "    #                 loss_scaler=None)\n",
    "    print(\" Epoch : \", epoch)\n",
    "    acc1, acc5, loss = teacher_validate(config, data_loader_train, super_model)\n",
    "    for layer_id in range(NUM_LAYERS):\n",
    "        for block_id in range(LAYER_DEPTHS[layer_id]):\n",
    "            if block_id%2 == 0:\n",
    "                avg_loss = all_lora_attns[layer_id][block_id].avg_loss / len(data_loader_train)\n",
    "                # all_lora_attns[layer_id][block_id].do_lr_step()\n",
    "                # all_lora_attns[layer_id][block_id].do_lr_step(avg_loss)\n",
    "                all_lora_attns[layer_id][block_id].avg_loss = 0\n",
    "\n",
    "    # acc1, acc5, loss = validate(config, data_loader_val, model)\n",
    "    # logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n",
    "    # max_accuracy = max(max_accuracy, acc1)\n",
    "    # logger.info(f'Max accuracy: {max_accuracy:.2f}%')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "logger.info('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Layer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only layer 3 Low rank: 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low rank: 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low rank: 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_model.state_dict()['layers.' + str(2) + \".blocks.\" + str(0) + \".attn.q_wUSprime\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2 and  3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low rank: 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low Rank 325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low rank 325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low rank 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low rank 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, super_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      "\u001b[32m[2022-12-07 23:27:43 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-10-d2eb1b709886> 77)\u001b[0m: INFO Test: [0/788]\tTime 0.307 (0.307)\tLoss 0.0000 (0.0000)\tAcc@1 100.000 (100.000)\tAcc@5 100.000 (100.000)\tMem 314MB\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      "\u001b[32m[2022-12-07 23:27:51 swin_tiny_patch4_window7_224_resisc45]\u001b[0m\u001b[33m(<ipython-input-10-d2eb1b709886> 77)\u001b[0m: INFO Test: [120/788]\tTime 0.060 (0.064)\tLoss 0.0000 (0.2422)\tAcc@1 100.000 (93.492)\tAcc@5 100.000 (99.793)\tMem 352MB\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n",
      " Doing attn fwd44\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-580953f83e5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# logger.info(\"Start teacher-student inference\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-d2eb1b709886>\u001b[0m in \u001b[0;36mteacher_validate\u001b[0;34m(config, data_loader, model)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    737\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHALLENGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message = %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m     \u001b[0mdigest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHMAC_DIGEST_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m     \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/hmac.py\u001b[0m in \u001b[0;36mdigest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mupdating\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mcalling\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/hmac.py\u001b[0m in \u001b[0;36m_current\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[1;32m    122\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# logger.info(\"Start teacher-student inference\")\n",
    "start_time = time.time()\n",
    "acc1, acc5, loss = teacher_validate(config, data_loader_val, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
