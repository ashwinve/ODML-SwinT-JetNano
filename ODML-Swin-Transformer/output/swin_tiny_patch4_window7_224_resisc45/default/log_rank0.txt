[2022-10-17 08:58:37 swin_tiny_patch4_window7_224_resisc45] (main.py 387): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 08:58:37 swin_tiny_patch4_window7_224_resisc45] (main.py 390): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: ./data/imagenet/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: true
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 08:58:37 swin_tiny_patch4_window7_224_resisc45] (main.py 391): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/imagenet/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": true, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 08:58:37 swin_tiny_patch4_window7_224_resisc45] (main.py 92): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-10-17 08:58:37 swin_tiny_patch4_window7_224_resisc45] (main.py 94): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2022-10-17 08:58:37 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO number of params: 28288354
[2022-10-17 08:58:37 swin_tiny_patch4_window7_224_resisc45] (main.py 100): INFO number of GFLOPs: 4.49440512
[2022-10-17 08:59:48 swin_tiny_patch4_window7_224_resisc45] (main.py 387): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 08:59:48 swin_tiny_patch4_window7_224_resisc45] (main.py 390): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: ./data/imagenet/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 08:59:48 swin_tiny_patch4_window7_224_resisc45] (main.py 391): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/imagenet/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:00:16 swin_tiny_patch4_window7_224_resisc45] (main.py 387): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:00:16 swin_tiny_patch4_window7_224_resisc45] (main.py 390): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:00:16 swin_tiny_patch4_window7_224_resisc45] (main.py 391): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:05:25 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:05:25 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:05:25 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:06:40 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:06:40 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:06:40 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:07:37 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:07:37 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:07:37 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:10:37 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:10:37 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:10:37 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:11:00 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:11:00 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:11:00 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:13:19 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:13:19 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:13:19 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:13:52 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:13:52 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:13:52 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:14:20 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:14:20 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: part
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:14:20 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "part", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:18:00 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:18:00 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:18:00 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:20:23 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:20:23 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:20:23 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 09:55:18 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 09:55:18 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 09:55:18 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 10:00:14 swin_tiny_patch4_window7_224_resisc45] (main.py 385): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-10-17 10:00:14 swin_tiny_patch4_window7_224_resisc45] (main.py 388): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-10-17 10:00:14 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-10-17 10:00:14 swin_tiny_patch4_window7_224_resisc45] (main.py 92): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-10-17 10:00:14 swin_tiny_patch4_window7_224_resisc45] (main.py 94): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-10-17 10:00:14 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO number of params: 27553959
[2022-10-17 10:00:14 swin_tiny_patch4_window7_224_resisc45] (main.py 100): INFO number of GFLOPs: 4.49367168
[2022-11-04 15:57:30 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 15:57:30 swin_tiny_patch4_window7_224_resisc45] (main.py 392): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/zipped_archives/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 15:57:30 swin_tiny_patch4_window7_224_resisc45] (main.py 393): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/zipped_archives/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 15:59:00 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 15:59:00 swin_tiny_patch4_window7_224_resisc45] (main.py 392): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 15:59:00 swin_tiny_patch4_window7_224_resisc45] (main.py 393): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 15:59:00 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 15:59:01 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 16:00:52 swin_tiny_patch4_window7_224_resisc45] (main.py 389): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 16:00:52 swin_tiny_patch4_window7_224_resisc45] (main.py 392): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 16:00:52 swin_tiny_patch4_window7_224_resisc45] (main.py 393): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 16:00:53 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 16:00:53 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 17:37:36 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 17:37:36 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 17:37:36 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 17:37:36 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 17:37:37 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 17:37:37 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 17:37:37 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 17:37:37 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 17:41:35 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 17:41:35 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 17:41:35 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 17:41:35 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 17:41:36 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 17:41:36 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 17:41:36 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 17:41:36 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 17:42:06 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 17:42:06 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 17:42:06 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 17:42:06 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 17:42:07 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 17:42:07 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 17:42:07 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 17:42:07 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 17:42:09 swin_tiny_patch4_window7_224_resisc45] (utils.py 15): INFO ==============> Resuming form ./pretrained/swin_tiny_patch4_window7_224.pth....................
[2022-11-04 17:54:24 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 17:54:24 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: true
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 17:54:24 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": true, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 17:54:24 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 17:54:25 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 17:54:25 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 17:54:25 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 17:54:25 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 17:54:27 swin_tiny_patch4_window7_224_resisc45] (utils.py 15): INFO ==============> Resuming form ./pretrained/swin_tiny_patch4_window7_224.pth....................
[2022-11-04 17:54:27 swin_tiny_patch4_window7_224_resisc45] (utils.py 28): INFO _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])
[2022-11-04 17:54:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [0/1575]	Time 2.106 (2.106)	Loss 4.0117 (4.0117)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 157MB
[2022-11-04 17:54:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [10/1575]	Time 0.021 (0.211)	Loss 3.9766 (3.9210)	Acc@1 0.000 (0.000)	Acc@5 0.000 (4.545)	Mem 157MB
[2022-11-04 17:54:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [20/1575]	Time 0.021 (0.121)	Loss 3.6895 (3.8459)	Acc@1 25.000 (1.190)	Acc@5 25.000 (5.952)	Mem 157MB
[2022-11-04 17:54:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [30/1575]	Time 0.021 (0.089)	Loss 3.8223 (3.8621)	Acc@1 0.000 (0.806)	Acc@5 0.000 (6.452)	Mem 157MB
[2022-11-04 17:54:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [40/1575]	Time 0.022 (0.072)	Loss 4.0352 (3.8670)	Acc@1 0.000 (1.220)	Acc@5 0.000 (6.707)	Mem 157MB
[2022-11-04 17:54:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [50/1575]	Time 0.020 (0.062)	Loss 3.8828 (3.8750)	Acc@1 0.000 (0.980)	Acc@5 0.000 (5.882)	Mem 157MB
[2022-11-04 17:54:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [60/1575]	Time 0.018 (0.055)	Loss 3.8613 (3.8805)	Acc@1 0.000 (0.820)	Acc@5 0.000 (6.557)	Mem 157MB
[2022-11-04 17:54:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [70/1575]	Time 0.018 (0.050)	Loss 3.7676 (3.8750)	Acc@1 0.000 (1.056)	Acc@5 0.000 (7.746)	Mem 157MB
[2022-11-04 17:54:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [80/1575]	Time 0.019 (0.046)	Loss 3.6719 (3.8662)	Acc@1 0.000 (0.926)	Acc@5 0.000 (8.025)	Mem 157MB
[2022-11-04 17:54:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [90/1575]	Time 0.019 (0.043)	Loss 3.8555 (3.8694)	Acc@1 0.000 (0.824)	Acc@5 0.000 (7.692)	Mem 157MB
[2022-11-04 17:54:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [100/1575]	Time 0.019 (0.041)	Loss 4.0156 (3.8678)	Acc@1 0.000 (0.743)	Acc@5 0.000 (7.426)	Mem 157MB
[2022-11-04 17:54:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [110/1575]	Time 0.023 (0.039)	Loss 3.8086 (3.8651)	Acc@1 0.000 (0.676)	Acc@5 0.000 (6.982)	Mem 157MB
[2022-11-04 17:54:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [120/1575]	Time 0.020 (0.037)	Loss 3.9043 (3.8618)	Acc@1 0.000 (0.826)	Acc@5 0.000 (7.025)	Mem 157MB
[2022-11-04 17:54:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [130/1575]	Time 0.018 (0.036)	Loss 4.0234 (3.8566)	Acc@1 0.000 (0.763)	Acc@5 0.000 (7.443)	Mem 157MB
[2022-11-04 17:54:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [140/1575]	Time 0.018 (0.035)	Loss 3.8262 (3.8507)	Acc@1 0.000 (0.887)	Acc@5 0.000 (7.447)	Mem 157MB
[2022-11-04 17:54:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [150/1575]	Time 0.020 (0.034)	Loss 3.9414 (3.8601)	Acc@1 0.000 (0.828)	Acc@5 0.000 (7.616)	Mem 157MB
[2022-11-04 17:54:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [160/1575]	Time 0.020 (0.033)	Loss 4.0938 (3.8635)	Acc@1 0.000 (0.776)	Acc@5 0.000 (7.453)	Mem 157MB
[2022-11-04 17:54:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [170/1575]	Time 0.021 (0.032)	Loss 3.7422 (3.8676)	Acc@1 0.000 (0.877)	Acc@5 0.000 (7.310)	Mem 157MB
[2022-11-04 17:54:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [180/1575]	Time 0.020 (0.032)	Loss 3.8652 (3.8655)	Acc@1 0.000 (0.829)	Acc@5 0.000 (7.182)	Mem 157MB
[2022-11-04 17:54:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [190/1575]	Time 0.023 (0.031)	Loss 3.5078 (3.8572)	Acc@1 0.000 (1.047)	Acc@5 25.000 (8.115)	Mem 157MB
[2022-11-04 17:54:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [200/1575]	Time 0.019 (0.031)	Loss 3.9531 (3.8560)	Acc@1 0.000 (0.995)	Acc@5 0.000 (8.209)	Mem 157MB
[2022-11-04 17:54:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [210/1575]	Time 0.021 (0.030)	Loss 3.7188 (3.8529)	Acc@1 0.000 (0.948)	Acc@5 25.000 (8.412)	Mem 157MB
[2022-11-04 17:54:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [220/1575]	Time 0.021 (0.030)	Loss 3.7070 (3.8446)	Acc@1 0.000 (1.018)	Acc@5 25.000 (8.710)	Mem 157MB
[2022-11-04 17:54:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [230/1575]	Time 0.021 (0.029)	Loss 3.7266 (3.8375)	Acc@1 25.000 (1.082)	Acc@5 25.000 (8.983)	Mem 157MB
[2022-11-04 17:54:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [240/1575]	Time 0.021 (0.029)	Loss 3.6855 (3.8287)	Acc@1 0.000 (1.037)	Acc@5 0.000 (9.336)	Mem 157MB
[2022-11-04 17:54:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [250/1575]	Time 0.023 (0.029)	Loss 4.1328 (3.8328)	Acc@1 0.000 (0.996)	Acc@5 0.000 (9.064)	Mem 157MB
[2022-11-04 17:54:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [260/1575]	Time 0.019 (0.028)	Loss 3.9141 (3.8405)	Acc@1 0.000 (0.958)	Acc@5 0.000 (8.716)	Mem 157MB
[2022-11-04 17:54:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [270/1575]	Time 0.019 (0.028)	Loss 3.9668 (3.8501)	Acc@1 0.000 (0.923)	Acc@5 0.000 (8.487)	Mem 157MB
[2022-11-04 17:54:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [280/1575]	Time 0.021 (0.028)	Loss 3.5078 (3.8557)	Acc@1 25.000 (0.979)	Acc@5 50.000 (8.452)	Mem 157MB
[2022-11-04 17:54:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [290/1575]	Time 0.019 (0.028)	Loss 3.3457 (3.8401)	Acc@1 0.000 (1.460)	Acc@5 75.000 (10.481)	Mem 157MB
[2022-11-04 17:54:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [300/1575]	Time 0.020 (0.027)	Loss 3.4434 (3.8239)	Acc@1 0.000 (2.243)	Acc@5 50.000 (12.126)	Mem 157MB
[2022-11-04 17:54:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [310/1575]	Time 0.019 (0.027)	Loss 3.3887 (3.8131)	Acc@1 25.000 (2.653)	Acc@5 50.000 (13.183)	Mem 157MB
[2022-11-04 17:54:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [320/1575]	Time 0.018 (0.027)	Loss 4.1602 (3.8127)	Acc@1 0.000 (2.960)	Acc@5 0.000 (13.707)	Mem 157MB
[2022-11-04 17:54:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [330/1575]	Time 0.019 (0.027)	Loss 3.8809 (3.8175)	Acc@1 0.000 (2.946)	Acc@5 0.000 (13.671)	Mem 157MB
[2022-11-04 17:54:36 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [340/1575]	Time 0.019 (0.026)	Loss 4.3047 (3.8255)	Acc@1 0.000 (2.859)	Acc@5 0.000 (13.343)	Mem 157MB
[2022-11-04 17:54:36 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [350/1575]	Time 0.019 (0.026)	Loss 4.1914 (3.8343)	Acc@1 0.000 (2.920)	Acc@5 0.000 (13.177)	Mem 157MB
[2022-11-04 17:54:36 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [360/1575]	Time 0.021 (0.026)	Loss 4.2031 (3.8435)	Acc@1 0.000 (2.839)	Acc@5 0.000 (12.812)	Mem 157MB
[2022-11-04 17:54:36 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [370/1575]	Time 0.020 (0.026)	Loss 4.4023 (3.8533)	Acc@1 0.000 (2.763)	Acc@5 0.000 (12.466)	Mem 157MB
[2022-11-04 17:54:36 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [380/1575]	Time 0.021 (0.026)	Loss 4.0703 (3.8622)	Acc@1 0.000 (2.690)	Acc@5 0.000 (12.205)	Mem 157MB
[2022-11-04 17:54:37 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [390/1575]	Time 0.018 (0.026)	Loss 3.6602 (3.8593)	Acc@1 0.000 (2.685)	Acc@5 0.000 (12.596)	Mem 157MB
[2022-11-04 17:54:37 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [400/1575]	Time 0.020 (0.025)	Loss 3.6035 (3.8496)	Acc@1 0.000 (2.868)	Acc@5 25.000 (13.342)	Mem 157MB
[2022-11-04 17:54:37 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [410/1575]	Time 0.019 (0.025)	Loss 3.3262 (3.8383)	Acc@1 50.000 (3.041)	Acc@5 50.000 (13.929)	Mem 157MB
[2022-11-04 17:54:37 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [420/1575]	Time 0.020 (0.025)	Loss 3.7539 (3.8293)	Acc@1 0.000 (3.385)	Acc@5 0.000 (14.430)	Mem 157MB
[2022-11-04 17:54:37 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [430/1575]	Time 0.019 (0.025)	Loss 3.6328 (3.8269)	Acc@1 0.000 (3.364)	Acc@5 25.000 (14.443)	Mem 157MB
[2022-11-04 17:54:38 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [440/1575]	Time 0.021 (0.025)	Loss 3.6602 (3.8249)	Acc@1 25.000 (3.515)	Acc@5 25.000 (14.512)	Mem 157MB
[2022-11-04 17:54:38 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [450/1575]	Time 0.020 (0.025)	Loss 3.6973 (3.8219)	Acc@1 0.000 (3.603)	Acc@5 0.000 (14.523)	Mem 157MB
[2022-11-04 17:54:38 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [460/1575]	Time 0.021 (0.025)	Loss 3.6758 (3.8202)	Acc@1 0.000 (3.579)	Acc@5 50.000 (14.588)	Mem 157MB
[2022-11-04 17:54:38 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [470/1575]	Time 0.020 (0.025)	Loss 3.8027 (3.8214)	Acc@1 0.000 (3.556)	Acc@5 25.000 (14.384)	Mem 157MB
[2022-11-04 17:54:38 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [480/1575]	Time 0.021 (0.025)	Loss 3.6602 (3.8200)	Acc@1 0.000 (3.482)	Acc@5 25.000 (14.553)	Mem 157MB
[2022-11-04 17:54:39 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [490/1575]	Time 0.021 (0.024)	Loss 4.3281 (3.8209)	Acc@1 0.000 (3.411)	Acc@5 0.000 (14.358)	Mem 157MB
[2022-11-04 17:54:39 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [500/1575]	Time 0.020 (0.024)	Loss 4.1523 (3.8264)	Acc@1 0.000 (3.343)	Acc@5 0.000 (14.122)	Mem 157MB
[2022-11-04 17:54:39 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [510/1575]	Time 0.021 (0.024)	Loss 4.4570 (3.8315)	Acc@1 0.000 (3.278)	Acc@5 0.000 (13.992)	Mem 157MB
[2022-11-04 17:54:39 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [520/1575]	Time 0.023 (0.024)	Loss 4.3984 (3.8376)	Acc@1 0.000 (3.215)	Acc@5 0.000 (13.724)	Mem 157MB
[2022-11-04 17:54:40 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [530/1575]	Time 0.021 (0.024)	Loss 4.0859 (3.8409)	Acc@1 0.000 (3.154)	Acc@5 0.000 (13.512)	Mem 157MB
[2022-11-04 17:54:40 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [540/1575]	Time 0.020 (0.024)	Loss 3.9297 (3.8435)	Acc@1 0.000 (3.096)	Acc@5 0.000 (13.309)	Mem 157MB
[2022-11-04 17:54:40 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [550/1575]	Time 0.021 (0.024)	Loss 4.0859 (3.8462)	Acc@1 0.000 (3.040)	Acc@5 0.000 (13.067)	Mem 157MB
[2022-11-04 17:54:40 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [560/1575]	Time 0.021 (0.024)	Loss 3.9648 (3.8490)	Acc@1 0.000 (2.986)	Acc@5 0.000 (12.834)	Mem 157MB
[2022-11-04 17:54:40 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [570/1575]	Time 0.020 (0.024)	Loss 4.4766 (3.8551)	Acc@1 0.000 (2.933)	Acc@5 0.000 (12.609)	Mem 157MB
[2022-11-04 17:54:41 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [580/1575]	Time 0.020 (0.024)	Loss 4.0586 (3.8610)	Acc@1 0.000 (2.883)	Acc@5 0.000 (12.392)	Mem 157MB
[2022-11-04 17:54:41 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [590/1575]	Time 0.019 (0.024)	Loss 4.3125 (3.8669)	Acc@1 0.000 (2.834)	Acc@5 0.000 (12.183)	Mem 157MB
[2022-11-04 17:54:41 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [600/1575]	Time 0.022 (0.024)	Loss 4.2188 (3.8706)	Acc@1 0.000 (2.787)	Acc@5 0.000 (11.980)	Mem 157MB
[2022-11-04 17:54:41 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [610/1575]	Time 0.024 (0.024)	Loss 4.0625 (3.8722)	Acc@1 0.000 (2.741)	Acc@5 0.000 (11.784)	Mem 157MB
[2022-11-04 17:54:41 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [620/1575]	Time 0.020 (0.024)	Loss 3.8242 (3.8737)	Acc@1 0.000 (2.697)	Acc@5 0.000 (11.634)	Mem 157MB
[2022-11-04 17:54:42 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [630/1575]	Time 0.018 (0.024)	Loss 3.9824 (3.8758)	Acc@1 0.000 (2.655)	Acc@5 0.000 (11.450)	Mem 157MB
[2022-11-04 17:54:42 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [640/1575]	Time 0.018 (0.024)	Loss 3.9648 (3.8758)	Acc@1 0.000 (2.613)	Acc@5 25.000 (11.349)	Mem 157MB
[2022-11-04 17:54:42 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [650/1575]	Time 0.018 (0.023)	Loss 3.6719 (3.8751)	Acc@1 0.000 (2.573)	Acc@5 25.000 (11.406)	Mem 157MB
[2022-11-04 17:54:42 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [660/1575]	Time 0.026 (0.023)	Loss 3.7188 (3.8748)	Acc@1 0.000 (2.534)	Acc@5 25.000 (11.309)	Mem 157MB
[2022-11-04 17:54:42 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [670/1575]	Time 0.019 (0.023)	Loss 3.3359 (3.8723)	Acc@1 25.000 (2.683)	Acc@5 50.000 (11.438)	Mem 157MB
[2022-11-04 17:54:43 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [680/1575]	Time 0.019 (0.023)	Loss 3.6309 (3.8675)	Acc@1 0.000 (2.900)	Acc@5 25.000 (11.858)	Mem 157MB
[2022-11-04 17:54:43 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [690/1575]	Time 0.021 (0.023)	Loss 3.4395 (3.8621)	Acc@1 0.000 (2.931)	Acc@5 75.000 (12.337)	Mem 157MB
[2022-11-04 17:54:43 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [700/1575]	Time 0.021 (0.023)	Loss 4.0547 (3.8577)	Acc@1 0.000 (2.996)	Acc@5 0.000 (12.696)	Mem 157MB
[2022-11-04 17:54:43 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [710/1575]	Time 0.019 (0.023)	Loss 3.7090 (3.8583)	Acc@1 0.000 (2.954)	Acc@5 25.000 (12.623)	Mem 157MB
[2022-11-04 17:54:43 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [720/1575]	Time 0.019 (0.023)	Loss 3.8633 (3.8599)	Acc@1 0.000 (2.913)	Acc@5 0.000 (12.448)	Mem 157MB
[2022-11-04 17:54:44 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [730/1575]	Time 0.019 (0.023)	Loss 4.0195 (3.8604)	Acc@1 0.000 (2.873)	Acc@5 0.000 (12.312)	Mem 157MB
[2022-11-04 17:54:44 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [740/1575]	Time 0.018 (0.023)	Loss 4.1250 (3.8611)	Acc@1 0.000 (2.868)	Acc@5 0.000 (12.213)	Mem 157MB
[2022-11-04 17:54:44 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [750/1575]	Time 0.018 (0.023)	Loss 3.5762 (3.8615)	Acc@1 0.000 (2.830)	Acc@5 0.000 (12.117)	Mem 157MB
[2022-11-04 17:54:44 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [760/1575]	Time 0.018 (0.023)	Loss 4.0938 (3.8621)	Acc@1 0.000 (2.792)	Acc@5 0.000 (11.991)	Mem 157MB
[2022-11-04 17:54:44 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [770/1575]	Time 0.019 (0.023)	Loss 3.7480 (3.8621)	Acc@1 0.000 (2.756)	Acc@5 25.000 (11.933)	Mem 157MB
[2022-11-04 17:54:45 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [780/1575]	Time 0.021 (0.023)	Loss 3.7246 (3.8597)	Acc@1 0.000 (2.785)	Acc@5 25.000 (12.004)	Mem 157MB
[2022-11-04 17:54:45 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [790/1575]	Time 0.021 (0.023)	Loss 3.6172 (3.8577)	Acc@1 0.000 (2.845)	Acc@5 0.000 (12.137)	Mem 157MB
[2022-11-04 17:54:45 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [800/1575]	Time 0.020 (0.023)	Loss 3.5410 (3.8559)	Acc@1 0.000 (2.809)	Acc@5 50.000 (12.203)	Mem 157MB
[2022-11-04 17:57:23 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 17:57:23 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: true
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 17:57:23 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": true, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 17:57:23 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 17:57:24 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 17:57:24 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 17:57:24 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 17:57:24 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 17:57:26 swin_tiny_patch4_window7_224_resisc45] (utils.py 15): INFO ==============> Resuming form ./pretrained/swin_tiny_patch4_window7_224.pth....................
[2022-11-04 17:57:59 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 17:57:59 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: true
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 17:57:59 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": true, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 17:57:59 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 17:57:59 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 17:57:59 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 17:57:59 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 17:57:59 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 17:58:01 swin_tiny_patch4_window7_224_resisc45] (utils.py 15): INFO ==============> Resuming form ./pretrained/swin_tiny_patch4_window7_224.pth....................
[2022-11-04 17:58:57 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 17:58:57 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: true
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 17:58:57 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": true, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 17:58:57 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 17:58:58 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 17:58:58 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 17:58:58 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 17:58:58 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 17:58:59 swin_tiny_patch4_window7_224_resisc45] (utils.py 15): INFO ==============> Resuming form ./pretrained/swin_tiny_patch4_window7_224.pth....................
[2022-11-04 20:09:46 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 20:09:46 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: true
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 20:09:46 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": true, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 20:09:46 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 20:09:47 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 20:09:47 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 20:09:47 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 20:09:47 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 20:09:49 swin_tiny_patch4_window7_224_resisc45] (utils.py 15): INFO ==============> Resuming form ./pretrained/swin_tiny_patch4_window7_224.pth....................
[2022-11-04 20:09:49 swin_tiny_patch4_window7_224_resisc45] (utils.py 30): INFO <All keys matched successfully>
[2022-11-04 20:09:51 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [0/1575]	Time 2.017 (2.017)	Loss 4.0117 (4.0117)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 215MB
[2022-11-04 20:09:51 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [10/1575]	Time 0.021 (0.203)	Loss 3.9766 (3.9210)	Acc@1 0.000 (0.000)	Acc@5 0.000 (4.545)	Mem 215MB
[2022-11-04 20:09:51 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [20/1575]	Time 0.020 (0.116)	Loss 3.6895 (3.8459)	Acc@1 25.000 (1.190)	Acc@5 25.000 (5.952)	Mem 215MB
[2022-11-04 20:09:51 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [30/1575]	Time 0.022 (0.086)	Loss 3.8223 (3.8621)	Acc@1 0.000 (0.806)	Acc@5 0.000 (6.452)	Mem 215MB
[2022-11-04 20:09:52 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [40/1575]	Time 0.021 (0.070)	Loss 4.0352 (3.8670)	Acc@1 0.000 (1.220)	Acc@5 0.000 (6.707)	Mem 215MB
[2022-11-04 20:09:52 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [50/1575]	Time 0.019 (0.060)	Loss 3.8828 (3.8750)	Acc@1 0.000 (0.980)	Acc@5 0.000 (5.882)	Mem 215MB
[2022-11-04 20:09:52 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [60/1575]	Time 0.020 (0.054)	Loss 3.8613 (3.8805)	Acc@1 0.000 (0.820)	Acc@5 0.000 (6.557)	Mem 215MB
[2022-11-04 20:09:52 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [70/1575]	Time 0.019 (0.049)	Loss 3.7676 (3.8750)	Acc@1 0.000 (1.056)	Acc@5 0.000 (7.746)	Mem 215MB
[2022-11-04 20:09:52 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [80/1575]	Time 0.021 (0.045)	Loss 3.6719 (3.8662)	Acc@1 0.000 (0.926)	Acc@5 0.000 (8.025)	Mem 215MB
[2022-11-04 20:09:53 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [90/1575]	Time 0.020 (0.043)	Loss 3.8555 (3.8694)	Acc@1 0.000 (0.824)	Acc@5 0.000 (7.692)	Mem 215MB
[2022-11-04 20:09:53 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [100/1575]	Time 0.021 (0.040)	Loss 4.0156 (3.8678)	Acc@1 0.000 (0.743)	Acc@5 0.000 (7.426)	Mem 215MB
[2022-11-04 20:09:53 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [110/1575]	Time 0.019 (0.039)	Loss 3.8086 (3.8651)	Acc@1 0.000 (0.676)	Acc@5 0.000 (6.982)	Mem 215MB
[2022-11-04 20:09:53 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [120/1575]	Time 0.019 (0.037)	Loss 3.9043 (3.8618)	Acc@1 0.000 (0.826)	Acc@5 0.000 (7.025)	Mem 215MB
[2022-11-04 20:09:53 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [130/1575]	Time 0.020 (0.036)	Loss 4.0234 (3.8566)	Acc@1 0.000 (0.763)	Acc@5 0.000 (7.443)	Mem 215MB
[2022-11-04 20:09:54 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [140/1575]	Time 0.019 (0.034)	Loss 3.8262 (3.8507)	Acc@1 0.000 (0.887)	Acc@5 0.000 (7.447)	Mem 215MB
[2022-11-04 20:09:54 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [150/1575]	Time 0.019 (0.034)	Loss 3.9414 (3.8601)	Acc@1 0.000 (0.828)	Acc@5 0.000 (7.616)	Mem 215MB
[2022-11-04 20:09:54 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [160/1575]	Time 0.019 (0.033)	Loss 4.0938 (3.8635)	Acc@1 0.000 (0.776)	Acc@5 0.000 (7.453)	Mem 215MB
[2022-11-04 20:09:54 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [170/1575]	Time 0.019 (0.032)	Loss 3.7422 (3.8676)	Acc@1 0.000 (0.877)	Acc@5 0.000 (7.310)	Mem 215MB
[2022-11-04 20:09:54 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [180/1575]	Time 0.019 (0.031)	Loss 3.8652 (3.8655)	Acc@1 0.000 (0.829)	Acc@5 0.000 (7.182)	Mem 215MB
[2022-11-04 20:09:55 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [190/1575]	Time 0.020 (0.031)	Loss 3.5078 (3.8572)	Acc@1 0.000 (1.047)	Acc@5 25.000 (8.115)	Mem 215MB
[2022-11-04 20:09:55 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [200/1575]	Time 0.022 (0.030)	Loss 3.9531 (3.8560)	Acc@1 0.000 (0.995)	Acc@5 0.000 (8.209)	Mem 215MB
[2022-11-04 20:09:55 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [210/1575]	Time 0.021 (0.030)	Loss 3.7188 (3.8529)	Acc@1 0.000 (0.948)	Acc@5 25.000 (8.412)	Mem 215MB
[2022-11-04 20:09:55 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [220/1575]	Time 0.021 (0.029)	Loss 3.7070 (3.8446)	Acc@1 0.000 (1.018)	Acc@5 25.000 (8.710)	Mem 215MB
[2022-11-04 20:09:56 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [230/1575]	Time 0.023 (0.029)	Loss 3.7266 (3.8375)	Acc@1 25.000 (1.082)	Acc@5 25.000 (8.983)	Mem 215MB
[2022-11-04 20:09:56 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [240/1575]	Time 0.023 (0.029)	Loss 3.6855 (3.8287)	Acc@1 0.000 (1.037)	Acc@5 0.000 (9.336)	Mem 215MB
[2022-11-04 20:09:56 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [250/1575]	Time 0.021 (0.029)	Loss 4.1328 (3.8328)	Acc@1 0.000 (0.996)	Acc@5 0.000 (9.064)	Mem 215MB
[2022-11-04 20:09:56 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [260/1575]	Time 0.021 (0.028)	Loss 3.9141 (3.8405)	Acc@1 0.000 (0.958)	Acc@5 0.000 (8.716)	Mem 215MB
[2022-11-04 20:09:56 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [270/1575]	Time 0.023 (0.028)	Loss 3.9668 (3.8501)	Acc@1 0.000 (0.923)	Acc@5 0.000 (8.487)	Mem 215MB
[2022-11-04 20:09:57 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [280/1575]	Time 0.021 (0.028)	Loss 3.5078 (3.8557)	Acc@1 25.000 (0.979)	Acc@5 50.000 (8.452)	Mem 215MB
[2022-11-04 20:09:57 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [290/1575]	Time 0.022 (0.028)	Loss 3.3457 (3.8401)	Acc@1 0.000 (1.460)	Acc@5 75.000 (10.481)	Mem 215MB
[2022-11-04 20:09:57 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [300/1575]	Time 0.024 (0.027)	Loss 3.4434 (3.8239)	Acc@1 0.000 (2.243)	Acc@5 50.000 (12.126)	Mem 215MB
[2022-11-04 20:09:57 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [310/1575]	Time 0.021 (0.027)	Loss 3.3887 (3.8131)	Acc@1 25.000 (2.653)	Acc@5 50.000 (13.183)	Mem 215MB
[2022-11-04 20:09:58 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [320/1575]	Time 0.020 (0.027)	Loss 4.1602 (3.8127)	Acc@1 0.000 (2.960)	Acc@5 0.000 (13.707)	Mem 215MB
[2022-11-04 20:09:58 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [330/1575]	Time 0.022 (0.027)	Loss 3.8809 (3.8175)	Acc@1 0.000 (2.946)	Acc@5 0.000 (13.671)	Mem 215MB
[2022-11-04 20:09:58 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [340/1575]	Time 0.021 (0.027)	Loss 4.3047 (3.8255)	Acc@1 0.000 (2.859)	Acc@5 0.000 (13.343)	Mem 215MB
[2022-11-04 20:09:58 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [350/1575]	Time 0.021 (0.027)	Loss 4.1914 (3.8343)	Acc@1 0.000 (2.920)	Acc@5 0.000 (13.177)	Mem 215MB
[2022-11-04 20:09:58 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [360/1575]	Time 0.020 (0.026)	Loss 4.2031 (3.8435)	Acc@1 0.000 (2.839)	Acc@5 0.000 (12.812)	Mem 215MB
[2022-11-04 20:09:59 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [370/1575]	Time 0.021 (0.026)	Loss 4.4023 (3.8533)	Acc@1 0.000 (2.763)	Acc@5 0.000 (12.466)	Mem 215MB
[2022-11-04 20:09:59 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [380/1575]	Time 0.021 (0.026)	Loss 4.0703 (3.8622)	Acc@1 0.000 (2.690)	Acc@5 0.000 (12.205)	Mem 215MB
[2022-11-04 20:09:59 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [390/1575]	Time 0.019 (0.026)	Loss 3.6602 (3.8593)	Acc@1 0.000 (2.685)	Acc@5 0.000 (12.596)	Mem 215MB
[2022-11-04 20:09:59 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [400/1575]	Time 0.019 (0.026)	Loss 3.6035 (3.8496)	Acc@1 0.000 (2.868)	Acc@5 25.000 (13.342)	Mem 215MB
[2022-11-04 20:09:59 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [410/1575]	Time 0.018 (0.026)	Loss 3.3262 (3.8383)	Acc@1 50.000 (3.041)	Acc@5 50.000 (13.929)	Mem 215MB
[2022-11-04 20:10:00 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [420/1575]	Time 0.019 (0.026)	Loss 3.7539 (3.8293)	Acc@1 0.000 (3.385)	Acc@5 0.000 (14.430)	Mem 215MB
[2022-11-04 20:10:00 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [430/1575]	Time 0.019 (0.025)	Loss 3.6328 (3.8269)	Acc@1 0.000 (3.364)	Acc@5 25.000 (14.443)	Mem 215MB
[2022-11-04 20:10:00 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [440/1575]	Time 0.019 (0.025)	Loss 3.6602 (3.8249)	Acc@1 25.000 (3.515)	Acc@5 25.000 (14.512)	Mem 215MB
[2022-11-04 20:10:00 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [450/1575]	Time 0.019 (0.025)	Loss 3.6973 (3.8219)	Acc@1 0.000 (3.603)	Acc@5 0.000 (14.523)	Mem 215MB
[2022-11-04 20:10:00 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [460/1575]	Time 0.020 (0.025)	Loss 3.6758 (3.8202)	Acc@1 0.000 (3.579)	Acc@5 50.000 (14.588)	Mem 215MB
[2022-11-04 20:10:01 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [470/1575]	Time 0.021 (0.025)	Loss 3.8027 (3.8214)	Acc@1 0.000 (3.556)	Acc@5 25.000 (14.384)	Mem 215MB
[2022-11-04 20:10:01 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [480/1575]	Time 0.023 (0.025)	Loss 3.6602 (3.8200)	Acc@1 0.000 (3.482)	Acc@5 25.000 (14.553)	Mem 215MB
[2022-11-04 20:10:01 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [490/1575]	Time 0.023 (0.025)	Loss 4.3281 (3.8209)	Acc@1 0.000 (3.411)	Acc@5 0.000 (14.358)	Mem 215MB
[2022-11-04 20:10:01 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [500/1575]	Time 0.021 (0.025)	Loss 4.1523 (3.8264)	Acc@1 0.000 (3.343)	Acc@5 0.000 (14.122)	Mem 215MB
[2022-11-04 20:10:01 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [510/1575]	Time 0.020 (0.025)	Loss 4.4570 (3.8315)	Acc@1 0.000 (3.278)	Acc@5 0.000 (13.992)	Mem 215MB
[2022-11-04 20:10:02 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [520/1575]	Time 0.021 (0.025)	Loss 4.3984 (3.8376)	Acc@1 0.000 (3.215)	Acc@5 0.000 (13.724)	Mem 215MB
[2022-11-04 20:10:02 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [530/1575]	Time 0.022 (0.025)	Loss 4.0859 (3.8409)	Acc@1 0.000 (3.154)	Acc@5 0.000 (13.512)	Mem 215MB
[2022-11-04 20:10:02 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [540/1575]	Time 0.019 (0.025)	Loss 3.9297 (3.8435)	Acc@1 0.000 (3.096)	Acc@5 0.000 (13.309)	Mem 215MB
[2022-11-04 20:10:02 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [550/1575]	Time 0.019 (0.024)	Loss 4.0859 (3.8462)	Acc@1 0.000 (3.040)	Acc@5 0.000 (13.067)	Mem 215MB
[2022-11-04 20:10:02 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [560/1575]	Time 0.020 (0.024)	Loss 3.9648 (3.8490)	Acc@1 0.000 (2.986)	Acc@5 0.000 (12.834)	Mem 215MB
[2022-11-04 20:10:03 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [570/1575]	Time 0.019 (0.024)	Loss 4.4766 (3.8551)	Acc@1 0.000 (2.933)	Acc@5 0.000 (12.609)	Mem 215MB
[2022-11-04 20:10:03 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [580/1575]	Time 0.020 (0.024)	Loss 4.0586 (3.8610)	Acc@1 0.000 (2.883)	Acc@5 0.000 (12.392)	Mem 215MB
[2022-11-04 20:10:03 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [590/1575]	Time 0.023 (0.024)	Loss 4.3125 (3.8669)	Acc@1 0.000 (2.834)	Acc@5 0.000 (12.183)	Mem 215MB
[2022-11-04 20:10:03 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [600/1575]	Time 0.022 (0.024)	Loss 4.2188 (3.8706)	Acc@1 0.000 (2.787)	Acc@5 0.000 (11.980)	Mem 215MB
[2022-11-04 20:10:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [610/1575]	Time 0.021 (0.024)	Loss 4.0625 (3.8722)	Acc@1 0.000 (2.741)	Acc@5 0.000 (11.784)	Mem 215MB
[2022-11-04 20:10:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [620/1575]	Time 0.020 (0.024)	Loss 3.8242 (3.8737)	Acc@1 0.000 (2.697)	Acc@5 0.000 (11.634)	Mem 215MB
[2022-11-04 20:10:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [630/1575]	Time 0.020 (0.024)	Loss 3.9824 (3.8758)	Acc@1 0.000 (2.655)	Acc@5 0.000 (11.450)	Mem 215MB
[2022-11-04 20:10:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [640/1575]	Time 0.021 (0.024)	Loss 3.9648 (3.8758)	Acc@1 0.000 (2.613)	Acc@5 25.000 (11.349)	Mem 215MB
[2022-11-04 20:10:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [650/1575]	Time 0.021 (0.024)	Loss 3.6719 (3.8751)	Acc@1 0.000 (2.573)	Acc@5 25.000 (11.406)	Mem 215MB
[2022-11-04 20:10:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [660/1575]	Time 0.020 (0.024)	Loss 3.7188 (3.8748)	Acc@1 0.000 (2.534)	Acc@5 25.000 (11.309)	Mem 215MB
[2022-11-04 20:10:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [670/1575]	Time 0.020 (0.024)	Loss 3.3359 (3.8723)	Acc@1 25.000 (2.683)	Acc@5 50.000 (11.438)	Mem 215MB
[2022-11-04 20:10:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [680/1575]	Time 0.019 (0.024)	Loss 3.6309 (3.8675)	Acc@1 0.000 (2.900)	Acc@5 25.000 (11.858)	Mem 215MB
[2022-11-04 20:10:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [690/1575]	Time 0.019 (0.024)	Loss 3.4395 (3.8621)	Acc@1 0.000 (2.931)	Acc@5 75.000 (12.337)	Mem 215MB
[2022-11-04 20:10:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [700/1575]	Time 0.019 (0.024)	Loss 4.0547 (3.8577)	Acc@1 0.000 (2.996)	Acc@5 0.000 (12.696)	Mem 215MB
[2022-11-04 20:10:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [710/1575]	Time 0.019 (0.024)	Loss 3.7090 (3.8583)	Acc@1 0.000 (2.954)	Acc@5 25.000 (12.623)	Mem 215MB
[2022-11-04 20:10:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [720/1575]	Time 0.018 (0.024)	Loss 3.8633 (3.8599)	Acc@1 0.000 (2.913)	Acc@5 0.000 (12.448)	Mem 215MB
[2022-11-04 20:10:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [730/1575]	Time 0.019 (0.023)	Loss 4.0195 (3.8604)	Acc@1 0.000 (2.873)	Acc@5 0.000 (12.312)	Mem 215MB
[2022-11-04 20:10:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [740/1575]	Time 0.021 (0.023)	Loss 4.1250 (3.8611)	Acc@1 0.000 (2.868)	Acc@5 0.000 (12.213)	Mem 215MB
[2022-11-04 20:10:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [750/1575]	Time 0.020 (0.023)	Loss 3.5762 (3.8615)	Acc@1 0.000 (2.830)	Acc@5 0.000 (12.117)	Mem 215MB
[2022-11-04 20:10:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [760/1575]	Time 0.021 (0.023)	Loss 4.0938 (3.8621)	Acc@1 0.000 (2.792)	Acc@5 0.000 (11.991)	Mem 215MB
[2022-11-04 20:10:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [770/1575]	Time 0.020 (0.023)	Loss 3.7480 (3.8621)	Acc@1 0.000 (2.756)	Acc@5 25.000 (11.933)	Mem 215MB
[2022-11-04 20:10:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [780/1575]	Time 0.020 (0.023)	Loss 3.7246 (3.8597)	Acc@1 0.000 (2.785)	Acc@5 25.000 (12.004)	Mem 215MB
[2022-11-04 20:10:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [790/1575]	Time 0.020 (0.023)	Loss 3.6172 (3.8577)	Acc@1 0.000 (2.845)	Acc@5 0.000 (12.137)	Mem 215MB
[2022-11-04 20:10:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [800/1575]	Time 0.021 (0.023)	Loss 3.5410 (3.8559)	Acc@1 0.000 (2.809)	Acc@5 50.000 (12.203)	Mem 215MB
[2022-11-04 20:10:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [810/1575]	Time 0.020 (0.023)	Loss 3.7266 (3.8547)	Acc@1 0.000 (2.805)	Acc@5 0.000 (12.207)	Mem 215MB
[2022-11-04 20:10:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [820/1575]	Time 0.022 (0.023)	Loss 3.9961 (3.8550)	Acc@1 0.000 (2.771)	Acc@5 0.000 (12.119)	Mem 215MB
[2022-11-04 20:10:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [830/1575]	Time 0.023 (0.023)	Loss 4.0508 (3.8557)	Acc@1 0.000 (2.738)	Acc@5 0.000 (12.004)	Mem 215MB
[2022-11-04 20:10:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [840/1575]	Time 0.021 (0.023)	Loss 3.7031 (3.8556)	Acc@1 0.000 (2.705)	Acc@5 0.000 (11.920)	Mem 215MB
[2022-11-04 20:10:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [850/1575]	Time 0.020 (0.023)	Loss 3.6035 (3.8565)	Acc@1 0.000 (2.673)	Acc@5 25.000 (11.810)	Mem 215MB
[2022-11-04 20:10:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [860/1575]	Time 0.021 (0.023)	Loss 3.8047 (3.8570)	Acc@1 0.000 (2.642)	Acc@5 25.000 (11.760)	Mem 215MB
[2022-11-04 20:10:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [870/1575]	Time 0.021 (0.023)	Loss 3.6582 (3.8560)	Acc@1 0.000 (2.641)	Acc@5 50.000 (11.797)	Mem 215MB
[2022-11-04 20:10:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [880/1575]	Time 0.021 (0.023)	Loss 4.0469 (3.8572)	Acc@1 0.000 (2.611)	Acc@5 0.000 (11.720)	Mem 215MB
[2022-11-04 20:10:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [890/1575]	Time 0.020 (0.023)	Loss 4.0938 (3.8598)	Acc@1 0.000 (2.581)	Acc@5 0.000 (11.588)	Mem 215MB
[2022-11-04 20:10:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [900/1575]	Time 0.020 (0.023)	Loss 4.1758 (3.8614)	Acc@1 0.000 (2.553)	Acc@5 0.000 (11.515)	Mem 215MB
[2022-11-04 20:10:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [910/1575]	Time 0.020 (0.023)	Loss 3.7305 (3.8627)	Acc@1 0.000 (2.525)	Acc@5 0.000 (11.389)	Mem 215MB
[2022-11-04 20:10:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [920/1575]	Time 0.021 (0.023)	Loss 3.5156 (3.8610)	Acc@1 0.000 (2.497)	Acc@5 50.000 (11.401)	Mem 215MB
[2022-11-04 20:10:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [930/1575]	Time 0.021 (0.023)	Loss 3.8613 (3.8596)	Acc@1 0.000 (2.497)	Acc@5 0.000 (11.386)	Mem 215MB
[2022-11-04 20:10:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [940/1575]	Time 0.021 (0.023)	Loss 3.7578 (3.8590)	Acc@1 0.000 (2.471)	Acc@5 0.000 (11.371)	Mem 215MB
[2022-11-04 20:10:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [950/1575]	Time 0.025 (0.023)	Loss 4.0547 (3.8608)	Acc@1 0.000 (2.445)	Acc@5 0.000 (11.304)	Mem 215MB
[2022-11-04 20:10:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [960/1575]	Time 0.028 (0.023)	Loss 4.3906 (3.8642)	Acc@1 0.000 (2.419)	Acc@5 0.000 (11.186)	Mem 215MB
[2022-11-04 20:10:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [970/1575]	Time 0.020 (0.023)	Loss 4.2148 (3.8684)	Acc@1 0.000 (2.394)	Acc@5 0.000 (11.071)	Mem 215MB
[2022-11-04 20:10:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [980/1575]	Time 0.020 (0.023)	Loss 3.6934 (3.8713)	Acc@1 0.000 (2.370)	Acc@5 25.000 (10.984)	Mem 215MB
[2022-11-04 20:10:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [990/1575]	Time 0.023 (0.023)	Loss 3.5527 (3.8685)	Acc@1 0.000 (2.371)	Acc@5 25.000 (11.150)	Mem 215MB
[2022-11-04 20:10:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1000/1575]	Time 0.020 (0.023)	Loss 3.6445 (3.8661)	Acc@1 0.000 (2.348)	Acc@5 25.000 (11.289)	Mem 215MB
[2022-11-04 20:10:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1010/1575]	Time 0.022 (0.023)	Loss 3.5703 (3.8633)	Acc@1 0.000 (2.349)	Acc@5 50.000 (11.449)	Mem 215MB
[2022-11-04 20:10:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1020/1575]	Time 0.019 (0.023)	Loss 4.0273 (3.8631)	Acc@1 0.000 (2.326)	Acc@5 0.000 (11.459)	Mem 215MB
[2022-11-04 20:10:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1030/1575]	Time 0.023 (0.023)	Loss 4.0234 (3.8633)	Acc@1 0.000 (2.304)	Acc@5 0.000 (11.397)	Mem 215MB
[2022-11-04 20:10:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1040/1575]	Time 0.019 (0.023)	Loss 3.6992 (3.8635)	Acc@1 0.000 (2.305)	Acc@5 0.000 (11.311)	Mem 215MB
[2022-11-04 20:10:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1050/1575]	Time 0.019 (0.023)	Loss 3.6367 (3.8635)	Acc@1 0.000 (2.284)	Acc@5 25.000 (11.275)	Mem 215MB
[2022-11-04 20:10:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1060/1575]	Time 0.019 (0.023)	Loss 3.5488 (3.8619)	Acc@1 0.000 (2.286)	Acc@5 25.000 (11.334)	Mem 215MB
[2022-11-04 20:10:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1070/1575]	Time 0.019 (0.023)	Loss 3.4258 (3.8597)	Acc@1 25.000 (2.334)	Acc@5 25.000 (11.391)	Mem 215MB
[2022-11-04 20:10:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1080/1575]	Time 0.022 (0.023)	Loss 3.6289 (3.8581)	Acc@1 0.000 (2.313)	Acc@5 50.000 (11.401)	Mem 215MB
[2022-11-04 20:10:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1090/1575]	Time 0.019 (0.023)	Loss 3.5508 (3.8567)	Acc@1 0.000 (2.291)	Acc@5 50.000 (11.434)	Mem 215MB
[2022-11-04 20:10:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1100/1575]	Time 0.019 (0.023)	Loss 3.8945 (3.8554)	Acc@1 0.000 (2.293)	Acc@5 0.000 (11.467)	Mem 215MB
[2022-11-04 20:10:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1110/1575]	Time 0.019 (0.023)	Loss 3.5117 (3.8545)	Acc@1 0.000 (2.273)	Acc@5 50.000 (11.544)	Mem 215MB
[2022-11-04 20:10:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1120/1575]	Time 0.020 (0.022)	Loss 3.9199 (3.8525)	Acc@1 0.000 (2.275)	Acc@5 0.000 (11.597)	Mem 215MB
[2022-11-04 20:10:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1130/1575]	Time 0.021 (0.022)	Loss 3.9512 (3.8527)	Acc@1 0.000 (2.255)	Acc@5 0.000 (11.494)	Mem 215MB
[2022-11-04 20:10:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1140/1575]	Time 0.020 (0.022)	Loss 3.8164 (3.8521)	Acc@1 0.000 (2.235)	Acc@5 0.000 (11.459)	Mem 215MB
[2022-11-04 20:10:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1150/1575]	Time 0.019 (0.022)	Loss 3.7012 (3.8522)	Acc@1 0.000 (2.215)	Acc@5 50.000 (11.425)	Mem 215MB
[2022-11-04 20:10:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1160/1575]	Time 0.019 (0.022)	Loss 4.1641 (3.8527)	Acc@1 0.000 (2.196)	Acc@5 0.000 (11.348)	Mem 215MB
[2022-11-04 20:10:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1170/1575]	Time 0.019 (0.022)	Loss 3.9805 (3.8543)	Acc@1 0.000 (2.178)	Acc@5 0.000 (11.251)	Mem 215MB
[2022-11-04 20:10:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1180/1575]	Time 0.018 (0.022)	Loss 4.0781 (3.8556)	Acc@1 0.000 (2.159)	Acc@5 0.000 (11.156)	Mem 215MB
[2022-11-04 20:10:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1190/1575]	Time 0.018 (0.022)	Loss 3.9062 (3.8565)	Acc@1 0.000 (2.141)	Acc@5 0.000 (11.083)	Mem 215MB
[2022-11-04 20:10:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1200/1575]	Time 0.019 (0.022)	Loss 3.6914 (3.8559)	Acc@1 0.000 (2.123)	Acc@5 25.000 (11.053)	Mem 215MB
[2022-11-04 20:10:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1210/1575]	Time 0.019 (0.022)	Loss 4.0742 (3.8559)	Acc@1 0.000 (2.106)	Acc@5 0.000 (11.065)	Mem 215MB
[2022-11-04 20:10:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1220/1575]	Time 0.018 (0.022)	Loss 3.5938 (3.8554)	Acc@1 25.000 (2.109)	Acc@5 25.000 (11.036)	Mem 215MB
[2022-11-04 20:10:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1230/1575]	Time 0.019 (0.022)	Loss 3.6602 (3.8546)	Acc@1 0.000 (2.092)	Acc@5 0.000 (11.068)	Mem 215MB
[2022-11-04 20:10:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1240/1575]	Time 0.019 (0.022)	Loss 3.6836 (3.8535)	Acc@1 0.000 (2.075)	Acc@5 0.000 (11.080)	Mem 215MB
[2022-11-04 20:10:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1250/1575]	Time 0.019 (0.022)	Loss 3.6094 (3.8522)	Acc@1 0.000 (2.078)	Acc@5 0.000 (11.111)	Mem 215MB
[2022-11-04 20:10:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1260/1575]	Time 0.019 (0.022)	Loss 3.5684 (3.8507)	Acc@1 0.000 (2.062)	Acc@5 25.000 (11.162)	Mem 215MB
[2022-11-04 20:10:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1270/1575]	Time 0.019 (0.022)	Loss 3.6367 (3.8496)	Acc@1 0.000 (2.065)	Acc@5 25.000 (11.231)	Mem 215MB
[2022-11-04 20:10:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1280/1575]	Time 0.019 (0.022)	Loss 3.5723 (3.8482)	Acc@1 0.000 (2.049)	Acc@5 25.000 (11.280)	Mem 215MB
[2022-11-04 20:10:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1290/1575]	Time 0.020 (0.022)	Loss 3.9590 (3.8473)	Acc@1 0.000 (2.053)	Acc@5 0.000 (11.328)	Mem 215MB
[2022-11-04 20:10:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1300/1575]	Time 0.019 (0.022)	Loss 3.8281 (3.8468)	Acc@1 0.000 (2.037)	Acc@5 0.000 (11.280)	Mem 215MB
[2022-11-04 20:10:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1310/1575]	Time 0.020 (0.022)	Loss 3.7695 (3.8464)	Acc@1 0.000 (2.021)	Acc@5 0.000 (11.213)	Mem 215MB
[2022-11-04 20:10:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1320/1575]	Time 0.020 (0.022)	Loss 3.7812 (3.8457)	Acc@1 0.000 (2.025)	Acc@5 25.000 (11.223)	Mem 215MB
[2022-11-04 20:10:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1330/1575]	Time 0.020 (0.022)	Loss 4.4844 (3.8454)	Acc@1 0.000 (2.010)	Acc@5 0.000 (11.213)	Mem 215MB
[2022-11-04 20:10:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1340/1575]	Time 0.021 (0.022)	Loss 4.4023 (3.8489)	Acc@1 0.000 (1.995)	Acc@5 0.000 (11.130)	Mem 215MB
[2022-11-04 20:10:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1350/1575]	Time 0.021 (0.022)	Loss 4.4883 (3.8529)	Acc@1 0.000 (1.980)	Acc@5 0.000 (11.047)	Mem 215MB
[2022-11-04 20:10:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1360/1575]	Time 0.020 (0.022)	Loss 3.9688 (3.8566)	Acc@1 0.000 (1.965)	Acc@5 0.000 (10.966)	Mem 215MB
[2022-11-04 20:10:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1370/1575]	Time 0.019 (0.022)	Loss 3.7832 (3.8583)	Acc@1 0.000 (1.951)	Acc@5 0.000 (10.886)	Mem 215MB
[2022-11-04 20:10:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1380/1575]	Time 0.018 (0.022)	Loss 4.2266 (3.8593)	Acc@1 0.000 (1.937)	Acc@5 0.000 (10.825)	Mem 215MB
[2022-11-04 20:10:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1390/1575]	Time 0.018 (0.022)	Loss 4.1328 (3.8598)	Acc@1 0.000 (1.923)	Acc@5 0.000 (10.748)	Mem 215MB
[2022-11-04 20:10:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1400/1575]	Time 0.019 (0.022)	Loss 4.1562 (3.8606)	Acc@1 0.000 (1.909)	Acc@5 0.000 (10.671)	Mem 215MB
[2022-11-04 20:10:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1410/1575]	Time 0.020 (0.022)	Loss 4.1250 (3.8618)	Acc@1 0.000 (1.896)	Acc@5 0.000 (10.631)	Mem 215MB
[2022-11-04 20:10:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1420/1575]	Time 0.020 (0.022)	Loss 3.8242 (3.8632)	Acc@1 0.000 (1.882)	Acc@5 0.000 (10.556)	Mem 215MB
[2022-11-04 20:10:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1430/1575]	Time 0.018 (0.022)	Loss 4.0938 (3.8643)	Acc@1 0.000 (1.869)	Acc@5 0.000 (10.482)	Mem 215MB
[2022-11-04 20:10:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1440/1575]	Time 0.019 (0.022)	Loss 3.9102 (3.8646)	Acc@1 0.000 (1.856)	Acc@5 25.000 (10.461)	Mem 215MB
[2022-11-04 20:10:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1450/1575]	Time 0.020 (0.022)	Loss 3.9023 (3.8640)	Acc@1 0.000 (1.844)	Acc@5 0.000 (10.407)	Mem 215MB
[2022-11-04 20:10:21 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1460/1575]	Time 0.019 (0.022)	Loss 3.6035 (3.8631)	Acc@1 0.000 (1.831)	Acc@5 25.000 (10.387)	Mem 215MB
[2022-11-04 20:10:21 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1470/1575]	Time 0.020 (0.022)	Loss 4.0078 (3.8620)	Acc@1 0.000 (1.818)	Acc@5 0.000 (10.452)	Mem 215MB
[2022-11-04 20:12:59 swin_tiny_patch4_window7_224_resisc45] (main.py 400): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 20:12:59 swin_tiny_patch4_window7_224_resisc45] (main.py 403): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224_resisc45.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 20:12:59 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224_resisc45.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 20:12:59 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 20:13:00 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 20:13:00 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 20:13:00 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 20:13:00 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 20:13:01 swin_tiny_patch4_window7_224_resisc45] (utils.py 15): INFO ==============> Resuming form ./pretrained/swin_tiny_patch4_window7_224_resisc45.pth....................
[2022-11-04 20:13:01 swin_tiny_patch4_window7_224_resisc45] (utils.py 30): INFO <All keys matched successfully>
[2022-11-04 20:13:03 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [0/1575]	Time 2.001 (2.001)	Loss 4.0117 (4.0117)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 215MB
[2022-11-04 20:13:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [10/1575]	Time 0.021 (0.202)	Loss 3.9766 (3.9210)	Acc@1 0.000 (0.000)	Acc@5 0.000 (4.545)	Mem 215MB
[2022-11-04 20:13:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [20/1575]	Time 0.021 (0.116)	Loss 3.6895 (3.8459)	Acc@1 25.000 (1.190)	Acc@5 25.000 (5.952)	Mem 215MB
[2022-11-04 20:13:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [30/1575]	Time 0.019 (0.085)	Loss 3.8223 (3.8621)	Acc@1 0.000 (0.806)	Acc@5 0.000 (6.452)	Mem 215MB
[2022-11-04 20:13:04 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [40/1575]	Time 0.022 (0.069)	Loss 4.0352 (3.8670)	Acc@1 0.000 (1.220)	Acc@5 0.000 (6.707)	Mem 215MB
[2022-11-04 20:13:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [50/1575]	Time 0.020 (0.060)	Loss 3.8828 (3.8750)	Acc@1 0.000 (0.980)	Acc@5 0.000 (5.882)	Mem 215MB
[2022-11-04 20:13:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [60/1575]	Time 0.020 (0.053)	Loss 3.8613 (3.8805)	Acc@1 0.000 (0.820)	Acc@5 0.000 (6.557)	Mem 215MB
[2022-11-04 20:13:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [70/1575]	Time 0.019 (0.049)	Loss 3.7676 (3.8750)	Acc@1 0.000 (1.056)	Acc@5 0.000 (7.746)	Mem 215MB
[2022-11-04 20:13:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [80/1575]	Time 0.020 (0.045)	Loss 3.6719 (3.8662)	Acc@1 0.000 (0.926)	Acc@5 0.000 (8.025)	Mem 215MB
[2022-11-04 20:13:05 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [90/1575]	Time 0.020 (0.042)	Loss 3.8555 (3.8694)	Acc@1 0.000 (0.824)	Acc@5 0.000 (7.692)	Mem 215MB
[2022-11-04 20:13:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [100/1575]	Time 0.020 (0.040)	Loss 4.0156 (3.8678)	Acc@1 0.000 (0.743)	Acc@5 0.000 (7.426)	Mem 215MB
[2022-11-04 20:13:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [110/1575]	Time 0.020 (0.038)	Loss 3.8086 (3.8651)	Acc@1 0.000 (0.676)	Acc@5 0.000 (6.982)	Mem 215MB
[2022-11-04 20:13:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [120/1575]	Time 0.019 (0.037)	Loss 3.9043 (3.8618)	Acc@1 0.000 (0.826)	Acc@5 0.000 (7.025)	Mem 215MB
[2022-11-04 20:13:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [130/1575]	Time 0.019 (0.036)	Loss 4.0234 (3.8566)	Acc@1 0.000 (0.763)	Acc@5 0.000 (7.443)	Mem 215MB
[2022-11-04 20:13:06 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [140/1575]	Time 0.022 (0.035)	Loss 3.8262 (3.8507)	Acc@1 0.000 (0.887)	Acc@5 0.000 (7.447)	Mem 215MB
[2022-11-04 20:13:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [150/1575]	Time 0.019 (0.034)	Loss 3.9414 (3.8601)	Acc@1 0.000 (0.828)	Acc@5 0.000 (7.616)	Mem 215MB
[2022-11-04 20:13:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [160/1575]	Time 0.021 (0.033)	Loss 4.0938 (3.8635)	Acc@1 0.000 (0.776)	Acc@5 0.000 (7.453)	Mem 215MB
[2022-11-04 20:13:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [170/1575]	Time 0.021 (0.032)	Loss 3.7422 (3.8676)	Acc@1 0.000 (0.877)	Acc@5 0.000 (7.310)	Mem 215MB
[2022-11-04 20:13:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [180/1575]	Time 0.020 (0.031)	Loss 3.8652 (3.8655)	Acc@1 0.000 (0.829)	Acc@5 0.000 (7.182)	Mem 215MB
[2022-11-04 20:13:07 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [190/1575]	Time 0.022 (0.031)	Loss 3.5078 (3.8572)	Acc@1 0.000 (1.047)	Acc@5 25.000 (8.115)	Mem 215MB
[2022-11-04 20:13:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [200/1575]	Time 0.024 (0.030)	Loss 3.9531 (3.8560)	Acc@1 0.000 (0.995)	Acc@5 0.000 (8.209)	Mem 215MB
[2022-11-04 20:13:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [210/1575]	Time 0.021 (0.030)	Loss 3.7188 (3.8529)	Acc@1 0.000 (0.948)	Acc@5 25.000 (8.412)	Mem 215MB
[2022-11-04 20:13:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [220/1575]	Time 0.020 (0.030)	Loss 3.7070 (3.8446)	Acc@1 0.000 (1.018)	Acc@5 25.000 (8.710)	Mem 215MB
[2022-11-04 20:13:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [230/1575]	Time 0.019 (0.029)	Loss 3.7266 (3.8375)	Acc@1 25.000 (1.082)	Acc@5 25.000 (8.983)	Mem 215MB
[2022-11-04 20:13:08 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [240/1575]	Time 0.019 (0.029)	Loss 3.6855 (3.8287)	Acc@1 0.000 (1.037)	Acc@5 0.000 (9.336)	Mem 215MB
[2022-11-04 20:13:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [250/1575]	Time 0.019 (0.028)	Loss 4.1328 (3.8328)	Acc@1 0.000 (0.996)	Acc@5 0.000 (9.064)	Mem 215MB
[2022-11-04 20:13:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [260/1575]	Time 0.022 (0.028)	Loss 3.9141 (3.8405)	Acc@1 0.000 (0.958)	Acc@5 0.000 (8.716)	Mem 215MB
[2022-11-04 20:13:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [270/1575]	Time 0.021 (0.028)	Loss 3.9668 (3.8501)	Acc@1 0.000 (0.923)	Acc@5 0.000 (8.487)	Mem 215MB
[2022-11-04 20:13:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [280/1575]	Time 0.022 (0.028)	Loss 3.5078 (3.8557)	Acc@1 25.000 (0.979)	Acc@5 50.000 (8.452)	Mem 215MB
[2022-11-04 20:13:09 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [290/1575]	Time 0.021 (0.027)	Loss 3.3457 (3.8401)	Acc@1 0.000 (1.460)	Acc@5 75.000 (10.481)	Mem 215MB
[2022-11-04 20:13:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [300/1575]	Time 0.022 (0.027)	Loss 3.4434 (3.8239)	Acc@1 0.000 (2.243)	Acc@5 50.000 (12.126)	Mem 215MB
[2022-11-04 20:13:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [310/1575]	Time 0.021 (0.027)	Loss 3.3887 (3.8131)	Acc@1 25.000 (2.653)	Acc@5 50.000 (13.183)	Mem 215MB
[2022-11-04 20:13:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [320/1575]	Time 0.028 (0.027)	Loss 4.1602 (3.8127)	Acc@1 0.000 (2.960)	Acc@5 0.000 (13.707)	Mem 215MB
[2022-11-04 20:13:10 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [330/1575]	Time 0.021 (0.027)	Loss 3.8809 (3.8175)	Acc@1 0.000 (2.946)	Acc@5 0.000 (13.671)	Mem 215MB
[2022-11-04 20:13:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [340/1575]	Time 0.021 (0.027)	Loss 4.3047 (3.8255)	Acc@1 0.000 (2.859)	Acc@5 0.000 (13.343)	Mem 215MB
[2022-11-04 20:13:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [350/1575]	Time 0.025 (0.027)	Loss 4.1914 (3.8343)	Acc@1 0.000 (2.920)	Acc@5 0.000 (13.177)	Mem 215MB
[2022-11-04 20:13:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [360/1575]	Time 0.022 (0.026)	Loss 4.2031 (3.8435)	Acc@1 0.000 (2.839)	Acc@5 0.000 (12.812)	Mem 215MB
[2022-11-04 20:13:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [370/1575]	Time 0.021 (0.026)	Loss 4.4023 (3.8533)	Acc@1 0.000 (2.763)	Acc@5 0.000 (12.466)	Mem 215MB
[2022-11-04 20:13:11 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [380/1575]	Time 0.021 (0.026)	Loss 4.0703 (3.8622)	Acc@1 0.000 (2.690)	Acc@5 0.000 (12.205)	Mem 215MB
[2022-11-04 20:13:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [390/1575]	Time 0.020 (0.026)	Loss 3.6602 (3.8593)	Acc@1 0.000 (2.685)	Acc@5 0.000 (12.596)	Mem 215MB
[2022-11-04 20:13:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [400/1575]	Time 0.020 (0.026)	Loss 3.6035 (3.8496)	Acc@1 0.000 (2.868)	Acc@5 25.000 (13.342)	Mem 215MB
[2022-11-04 20:13:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [410/1575]	Time 0.023 (0.026)	Loss 3.3262 (3.8383)	Acc@1 50.000 (3.041)	Acc@5 50.000 (13.929)	Mem 215MB
[2022-11-04 20:13:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [420/1575]	Time 0.020 (0.026)	Loss 3.7539 (3.8293)	Acc@1 0.000 (3.385)	Acc@5 0.000 (14.430)	Mem 215MB
[2022-11-04 20:13:12 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [430/1575]	Time 0.019 (0.025)	Loss 3.6328 (3.8269)	Acc@1 0.000 (3.364)	Acc@5 25.000 (14.443)	Mem 215MB
[2022-11-04 20:13:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [440/1575]	Time 0.021 (0.025)	Loss 3.6602 (3.8249)	Acc@1 25.000 (3.515)	Acc@5 25.000 (14.512)	Mem 215MB
[2022-11-04 20:13:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [450/1575]	Time 0.020 (0.025)	Loss 3.6973 (3.8219)	Acc@1 0.000 (3.603)	Acc@5 0.000 (14.523)	Mem 215MB
[2022-11-04 20:13:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [460/1575]	Time 0.021 (0.025)	Loss 3.6758 (3.8202)	Acc@1 0.000 (3.579)	Acc@5 50.000 (14.588)	Mem 215MB
[2022-11-04 20:13:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [470/1575]	Time 0.021 (0.025)	Loss 3.8027 (3.8214)	Acc@1 0.000 (3.556)	Acc@5 25.000 (14.384)	Mem 215MB
[2022-11-04 20:13:13 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [480/1575]	Time 0.021 (0.025)	Loss 3.6602 (3.8200)	Acc@1 0.000 (3.482)	Acc@5 25.000 (14.553)	Mem 215MB
[2022-11-04 20:13:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [490/1575]	Time 0.020 (0.025)	Loss 4.3281 (3.8209)	Acc@1 0.000 (3.411)	Acc@5 0.000 (14.358)	Mem 215MB
[2022-11-04 20:13:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [500/1575]	Time 0.020 (0.025)	Loss 4.1523 (3.8264)	Acc@1 0.000 (3.343)	Acc@5 0.000 (14.122)	Mem 215MB
[2022-11-04 20:13:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [510/1575]	Time 0.020 (0.025)	Loss 4.4570 (3.8315)	Acc@1 0.000 (3.278)	Acc@5 0.000 (13.992)	Mem 215MB
[2022-11-04 20:13:14 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [520/1575]	Time 0.019 (0.025)	Loss 4.3984 (3.8376)	Acc@1 0.000 (3.215)	Acc@5 0.000 (13.724)	Mem 215MB
[2022-11-04 20:13:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [530/1575]	Time 0.022 (0.025)	Loss 4.0859 (3.8409)	Acc@1 0.000 (3.154)	Acc@5 0.000 (13.512)	Mem 215MB
[2022-11-04 20:13:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [540/1575]	Time 0.019 (0.024)	Loss 3.9297 (3.8435)	Acc@1 0.000 (3.096)	Acc@5 0.000 (13.309)	Mem 215MB
[2022-11-04 20:13:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [550/1575]	Time 0.020 (0.024)	Loss 4.0859 (3.8462)	Acc@1 0.000 (3.040)	Acc@5 0.000 (13.067)	Mem 215MB
[2022-11-04 20:13:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [560/1575]	Time 0.019 (0.024)	Loss 3.9648 (3.8490)	Acc@1 0.000 (2.986)	Acc@5 0.000 (12.834)	Mem 215MB
[2022-11-04 20:13:15 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [570/1575]	Time 0.020 (0.024)	Loss 4.4766 (3.8551)	Acc@1 0.000 (2.933)	Acc@5 0.000 (12.609)	Mem 215MB
[2022-11-04 20:13:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [580/1575]	Time 0.019 (0.024)	Loss 4.0586 (3.8610)	Acc@1 0.000 (2.883)	Acc@5 0.000 (12.392)	Mem 215MB
[2022-11-04 20:13:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [590/1575]	Time 0.020 (0.024)	Loss 4.3125 (3.8669)	Acc@1 0.000 (2.834)	Acc@5 0.000 (12.183)	Mem 215MB
[2022-11-04 20:13:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [600/1575]	Time 0.021 (0.024)	Loss 4.2188 (3.8706)	Acc@1 0.000 (2.787)	Acc@5 0.000 (11.980)	Mem 215MB
[2022-11-04 20:13:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [610/1575]	Time 0.019 (0.024)	Loss 4.0625 (3.8722)	Acc@1 0.000 (2.741)	Acc@5 0.000 (11.784)	Mem 215MB
[2022-11-04 20:13:16 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [620/1575]	Time 0.020 (0.024)	Loss 3.8242 (3.8737)	Acc@1 0.000 (2.697)	Acc@5 0.000 (11.634)	Mem 215MB
[2022-11-04 20:13:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [630/1575]	Time 0.020 (0.024)	Loss 3.9824 (3.8758)	Acc@1 0.000 (2.655)	Acc@5 0.000 (11.450)	Mem 215MB
[2022-11-04 20:13:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [640/1575]	Time 0.021 (0.024)	Loss 3.9648 (3.8758)	Acc@1 0.000 (2.613)	Acc@5 25.000 (11.349)	Mem 215MB
[2022-11-04 20:13:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [650/1575]	Time 0.020 (0.024)	Loss 3.6719 (3.8751)	Acc@1 0.000 (2.573)	Acc@5 25.000 (11.406)	Mem 215MB
[2022-11-04 20:13:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [660/1575]	Time 0.021 (0.024)	Loss 3.7188 (3.8748)	Acc@1 0.000 (2.534)	Acc@5 25.000 (11.309)	Mem 215MB
[2022-11-04 20:13:17 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [670/1575]	Time 0.020 (0.024)	Loss 3.3359 (3.8723)	Acc@1 25.000 (2.683)	Acc@5 50.000 (11.438)	Mem 215MB
[2022-11-04 20:13:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [680/1575]	Time 0.020 (0.024)	Loss 3.6309 (3.8675)	Acc@1 0.000 (2.900)	Acc@5 25.000 (11.858)	Mem 215MB
[2022-11-04 20:13:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [690/1575]	Time 0.021 (0.024)	Loss 3.4395 (3.8621)	Acc@1 0.000 (2.931)	Acc@5 75.000 (12.337)	Mem 215MB
[2022-11-04 20:13:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [700/1575]	Time 0.019 (0.024)	Loss 4.0547 (3.8577)	Acc@1 0.000 (2.996)	Acc@5 0.000 (12.696)	Mem 215MB
[2022-11-04 20:13:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [710/1575]	Time 0.019 (0.023)	Loss 3.7090 (3.8583)	Acc@1 0.000 (2.954)	Acc@5 25.000 (12.623)	Mem 215MB
[2022-11-04 20:13:18 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [720/1575]	Time 0.020 (0.023)	Loss 3.8633 (3.8599)	Acc@1 0.000 (2.913)	Acc@5 0.000 (12.448)	Mem 215MB
[2022-11-04 20:13:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [730/1575]	Time 0.019 (0.023)	Loss 4.0195 (3.8604)	Acc@1 0.000 (2.873)	Acc@5 0.000 (12.312)	Mem 215MB
[2022-11-04 20:13:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [740/1575]	Time 0.019 (0.023)	Loss 4.1250 (3.8611)	Acc@1 0.000 (2.868)	Acc@5 0.000 (12.213)	Mem 215MB
[2022-11-04 20:13:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [750/1575]	Time 0.019 (0.023)	Loss 3.5762 (3.8615)	Acc@1 0.000 (2.830)	Acc@5 0.000 (12.117)	Mem 215MB
[2022-11-04 20:13:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [760/1575]	Time 0.019 (0.023)	Loss 4.0938 (3.8621)	Acc@1 0.000 (2.792)	Acc@5 0.000 (11.991)	Mem 215MB
[2022-11-04 20:13:19 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [770/1575]	Time 0.019 (0.023)	Loss 3.7480 (3.8621)	Acc@1 0.000 (2.756)	Acc@5 25.000 (11.933)	Mem 215MB
[2022-11-04 20:13:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [780/1575]	Time 0.019 (0.023)	Loss 3.7246 (3.8597)	Acc@1 0.000 (2.785)	Acc@5 25.000 (12.004)	Mem 215MB
[2022-11-04 20:13:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [790/1575]	Time 0.021 (0.023)	Loss 3.6172 (3.8577)	Acc@1 0.000 (2.845)	Acc@5 0.000 (12.137)	Mem 215MB
[2022-11-04 20:13:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [800/1575]	Time 0.021 (0.023)	Loss 3.5410 (3.8559)	Acc@1 0.000 (2.809)	Acc@5 50.000 (12.203)	Mem 215MB
[2022-11-04 20:13:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [810/1575]	Time 0.022 (0.023)	Loss 3.7266 (3.8547)	Acc@1 0.000 (2.805)	Acc@5 0.000 (12.207)	Mem 215MB
[2022-11-04 20:13:20 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [820/1575]	Time 0.022 (0.023)	Loss 3.9961 (3.8550)	Acc@1 0.000 (2.771)	Acc@5 0.000 (12.119)	Mem 215MB
[2022-11-04 20:13:21 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [830/1575]	Time 0.020 (0.023)	Loss 4.0508 (3.8557)	Acc@1 0.000 (2.738)	Acc@5 0.000 (12.004)	Mem 215MB
[2022-11-04 20:13:21 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [840/1575]	Time 0.020 (0.023)	Loss 3.7031 (3.8556)	Acc@1 0.000 (2.705)	Acc@5 0.000 (11.920)	Mem 215MB
[2022-11-04 20:13:21 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [850/1575]	Time 0.019 (0.023)	Loss 3.6035 (3.8565)	Acc@1 0.000 (2.673)	Acc@5 25.000 (11.810)	Mem 215MB
[2022-11-04 20:13:21 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [860/1575]	Time 0.019 (0.023)	Loss 3.8047 (3.8570)	Acc@1 0.000 (2.642)	Acc@5 25.000 (11.760)	Mem 215MB
[2022-11-04 20:13:21 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [870/1575]	Time 0.019 (0.023)	Loss 3.6582 (3.8560)	Acc@1 0.000 (2.641)	Acc@5 50.000 (11.797)	Mem 215MB
[2022-11-04 20:13:22 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [880/1575]	Time 0.019 (0.023)	Loss 4.0469 (3.8572)	Acc@1 0.000 (2.611)	Acc@5 0.000 (11.720)	Mem 215MB
[2022-11-04 20:13:22 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [890/1575]	Time 0.020 (0.023)	Loss 4.0938 (3.8598)	Acc@1 0.000 (2.581)	Acc@5 0.000 (11.588)	Mem 215MB
[2022-11-04 20:13:22 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [900/1575]	Time 0.021 (0.023)	Loss 4.1758 (3.8614)	Acc@1 0.000 (2.553)	Acc@5 0.000 (11.515)	Mem 215MB
[2022-11-04 20:13:22 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [910/1575]	Time 0.020 (0.023)	Loss 3.7305 (3.8627)	Acc@1 0.000 (2.525)	Acc@5 0.000 (11.389)	Mem 215MB
[2022-11-04 20:13:22 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [920/1575]	Time 0.019 (0.023)	Loss 3.5156 (3.8610)	Acc@1 0.000 (2.497)	Acc@5 50.000 (11.401)	Mem 215MB
[2022-11-04 20:13:23 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [930/1575]	Time 0.019 (0.023)	Loss 3.8613 (3.8596)	Acc@1 0.000 (2.497)	Acc@5 0.000 (11.386)	Mem 215MB
[2022-11-04 20:13:23 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [940/1575]	Time 0.018 (0.023)	Loss 3.7578 (3.8590)	Acc@1 0.000 (2.471)	Acc@5 0.000 (11.371)	Mem 215MB
[2022-11-04 20:13:23 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [950/1575]	Time 0.020 (0.023)	Loss 4.0547 (3.8608)	Acc@1 0.000 (2.445)	Acc@5 0.000 (11.304)	Mem 215MB
[2022-11-04 20:13:23 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [960/1575]	Time 0.018 (0.023)	Loss 4.3906 (3.8642)	Acc@1 0.000 (2.419)	Acc@5 0.000 (11.186)	Mem 215MB
[2022-11-04 20:13:23 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [970/1575]	Time 0.018 (0.023)	Loss 4.2148 (3.8684)	Acc@1 0.000 (2.394)	Acc@5 0.000 (11.071)	Mem 215MB
[2022-11-04 20:13:24 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [980/1575]	Time 0.018 (0.023)	Loss 3.6934 (3.8713)	Acc@1 0.000 (2.370)	Acc@5 25.000 (10.984)	Mem 215MB
[2022-11-04 20:13:24 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [990/1575]	Time 0.018 (0.023)	Loss 3.5527 (3.8685)	Acc@1 0.000 (2.371)	Acc@5 25.000 (11.150)	Mem 215MB
[2022-11-04 20:13:24 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1000/1575]	Time 0.020 (0.022)	Loss 3.6445 (3.8661)	Acc@1 0.000 (2.348)	Acc@5 25.000 (11.289)	Mem 215MB
[2022-11-04 20:13:24 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1010/1575]	Time 0.020 (0.022)	Loss 3.5703 (3.8633)	Acc@1 0.000 (2.349)	Acc@5 50.000 (11.449)	Mem 215MB
[2022-11-04 20:13:24 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1020/1575]	Time 0.021 (0.022)	Loss 4.0273 (3.8631)	Acc@1 0.000 (2.326)	Acc@5 0.000 (11.459)	Mem 215MB
[2022-11-04 20:13:25 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1030/1575]	Time 0.020 (0.022)	Loss 4.0234 (3.8633)	Acc@1 0.000 (2.304)	Acc@5 0.000 (11.397)	Mem 215MB
[2022-11-04 20:13:25 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1040/1575]	Time 0.021 (0.022)	Loss 3.6992 (3.8635)	Acc@1 0.000 (2.305)	Acc@5 0.000 (11.311)	Mem 215MB
[2022-11-04 20:13:25 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1050/1575]	Time 0.020 (0.022)	Loss 3.6367 (3.8635)	Acc@1 0.000 (2.284)	Acc@5 25.000 (11.275)	Mem 215MB
[2022-11-04 20:13:25 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1060/1575]	Time 0.020 (0.022)	Loss 3.5488 (3.8619)	Acc@1 0.000 (2.286)	Acc@5 25.000 (11.334)	Mem 215MB
[2022-11-04 20:13:25 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1070/1575]	Time 0.019 (0.022)	Loss 3.4258 (3.8597)	Acc@1 25.000 (2.334)	Acc@5 25.000 (11.391)	Mem 215MB
[2022-11-04 20:13:26 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1080/1575]	Time 0.020 (0.022)	Loss 3.6289 (3.8581)	Acc@1 0.000 (2.313)	Acc@5 50.000 (11.401)	Mem 215MB
[2022-11-04 20:13:26 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1090/1575]	Time 0.020 (0.022)	Loss 3.5508 (3.8567)	Acc@1 0.000 (2.291)	Acc@5 50.000 (11.434)	Mem 215MB
[2022-11-04 20:13:26 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1100/1575]	Time 0.020 (0.022)	Loss 3.8945 (3.8554)	Acc@1 0.000 (2.293)	Acc@5 0.000 (11.467)	Mem 215MB
[2022-11-04 20:13:26 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1110/1575]	Time 0.022 (0.022)	Loss 3.5117 (3.8545)	Acc@1 0.000 (2.273)	Acc@5 50.000 (11.544)	Mem 215MB
[2022-11-04 20:13:26 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1120/1575]	Time 0.020 (0.022)	Loss 3.9199 (3.8525)	Acc@1 0.000 (2.275)	Acc@5 0.000 (11.597)	Mem 215MB
[2022-11-04 20:13:27 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1130/1575]	Time 0.021 (0.022)	Loss 3.9512 (3.8527)	Acc@1 0.000 (2.255)	Acc@5 0.000 (11.494)	Mem 215MB
[2022-11-04 20:13:27 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1140/1575]	Time 0.023 (0.022)	Loss 3.8164 (3.8521)	Acc@1 0.000 (2.235)	Acc@5 0.000 (11.459)	Mem 215MB
[2022-11-04 20:13:27 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1150/1575]	Time 0.023 (0.022)	Loss 3.7012 (3.8522)	Acc@1 0.000 (2.215)	Acc@5 50.000 (11.425)	Mem 215MB
[2022-11-04 20:13:27 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1160/1575]	Time 0.022 (0.022)	Loss 4.1641 (3.8527)	Acc@1 0.000 (2.196)	Acc@5 0.000 (11.348)	Mem 215MB
[2022-11-04 20:13:28 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1170/1575]	Time 0.021 (0.022)	Loss 3.9805 (3.8543)	Acc@1 0.000 (2.178)	Acc@5 0.000 (11.251)	Mem 215MB
[2022-11-04 20:13:28 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1180/1575]	Time 0.022 (0.022)	Loss 4.0781 (3.8556)	Acc@1 0.000 (2.159)	Acc@5 0.000 (11.156)	Mem 215MB
[2022-11-04 20:13:28 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1190/1575]	Time 0.021 (0.022)	Loss 3.9062 (3.8565)	Acc@1 0.000 (2.141)	Acc@5 0.000 (11.083)	Mem 215MB
[2022-11-04 20:13:28 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1200/1575]	Time 0.022 (0.022)	Loss 3.6914 (3.8559)	Acc@1 0.000 (2.123)	Acc@5 25.000 (11.053)	Mem 215MB
[2022-11-04 20:13:28 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1210/1575]	Time 0.019 (0.022)	Loss 4.0742 (3.8559)	Acc@1 0.000 (2.106)	Acc@5 0.000 (11.065)	Mem 215MB
[2022-11-04 20:13:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1220/1575]	Time 0.021 (0.022)	Loss 3.5938 (3.8554)	Acc@1 25.000 (2.109)	Acc@5 25.000 (11.036)	Mem 215MB
[2022-11-04 20:13:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1230/1575]	Time 0.020 (0.022)	Loss 3.6602 (3.8546)	Acc@1 0.000 (2.092)	Acc@5 0.000 (11.068)	Mem 215MB
[2022-11-04 20:13:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1240/1575]	Time 0.020 (0.022)	Loss 3.6836 (3.8535)	Acc@1 0.000 (2.075)	Acc@5 0.000 (11.080)	Mem 215MB
[2022-11-04 20:13:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1250/1575]	Time 0.020 (0.022)	Loss 3.6094 (3.8522)	Acc@1 0.000 (2.078)	Acc@5 0.000 (11.111)	Mem 215MB
[2022-11-04 20:13:29 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1260/1575]	Time 0.020 (0.022)	Loss 3.5684 (3.8507)	Acc@1 0.000 (2.062)	Acc@5 25.000 (11.162)	Mem 215MB
[2022-11-04 20:13:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1270/1575]	Time 0.019 (0.022)	Loss 3.6367 (3.8496)	Acc@1 0.000 (2.065)	Acc@5 25.000 (11.231)	Mem 215MB
[2022-11-04 20:13:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1280/1575]	Time 0.023 (0.022)	Loss 3.5723 (3.8482)	Acc@1 0.000 (2.049)	Acc@5 25.000 (11.280)	Mem 215MB
[2022-11-04 20:13:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1290/1575]	Time 0.019 (0.022)	Loss 3.9590 (3.8473)	Acc@1 0.000 (2.053)	Acc@5 0.000 (11.328)	Mem 215MB
[2022-11-04 20:13:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1300/1575]	Time 0.020 (0.022)	Loss 3.8281 (3.8468)	Acc@1 0.000 (2.037)	Acc@5 0.000 (11.280)	Mem 215MB
[2022-11-04 20:13:30 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1310/1575]	Time 0.020 (0.022)	Loss 3.7695 (3.8464)	Acc@1 0.000 (2.021)	Acc@5 0.000 (11.213)	Mem 215MB
[2022-11-04 20:13:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1320/1575]	Time 0.020 (0.022)	Loss 3.7812 (3.8457)	Acc@1 0.000 (2.025)	Acc@5 25.000 (11.223)	Mem 215MB
[2022-11-04 20:13:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1330/1575]	Time 0.021 (0.022)	Loss 4.4844 (3.8454)	Acc@1 0.000 (2.010)	Acc@5 0.000 (11.213)	Mem 215MB
[2022-11-04 20:13:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1340/1575]	Time 0.019 (0.022)	Loss 4.4023 (3.8489)	Acc@1 0.000 (1.995)	Acc@5 0.000 (11.130)	Mem 215MB
[2022-11-04 20:13:31 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1350/1575]	Time 0.020 (0.022)	Loss 4.4883 (3.8529)	Acc@1 0.000 (1.980)	Acc@5 0.000 (11.047)	Mem 215MB
[2022-11-04 20:13:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1360/1575]	Time 0.020 (0.022)	Loss 3.9688 (3.8566)	Acc@1 0.000 (1.965)	Acc@5 0.000 (10.966)	Mem 215MB
[2022-11-04 20:13:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1370/1575]	Time 0.019 (0.022)	Loss 3.7832 (3.8583)	Acc@1 0.000 (1.951)	Acc@5 0.000 (10.886)	Mem 215MB
[2022-11-04 20:13:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1380/1575]	Time 0.021 (0.022)	Loss 4.2266 (3.8593)	Acc@1 0.000 (1.937)	Acc@5 0.000 (10.825)	Mem 215MB
[2022-11-04 20:13:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1390/1575]	Time 0.020 (0.022)	Loss 4.1328 (3.8598)	Acc@1 0.000 (1.923)	Acc@5 0.000 (10.748)	Mem 215MB
[2022-11-04 20:13:32 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1400/1575]	Time 0.022 (0.022)	Loss 4.1562 (3.8606)	Acc@1 0.000 (1.909)	Acc@5 0.000 (10.671)	Mem 215MB
[2022-11-04 20:13:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1410/1575]	Time 0.022 (0.022)	Loss 4.1250 (3.8618)	Acc@1 0.000 (1.896)	Acc@5 0.000 (10.631)	Mem 215MB
[2022-11-04 20:13:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1420/1575]	Time 0.022 (0.022)	Loss 3.8242 (3.8632)	Acc@1 0.000 (1.882)	Acc@5 0.000 (10.556)	Mem 215MB
[2022-11-04 20:13:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1430/1575]	Time 0.021 (0.022)	Loss 4.0938 (3.8643)	Acc@1 0.000 (1.869)	Acc@5 0.000 (10.482)	Mem 215MB
[2022-11-04 20:13:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1440/1575]	Time 0.018 (0.022)	Loss 3.9102 (3.8646)	Acc@1 0.000 (1.856)	Acc@5 25.000 (10.461)	Mem 215MB
[2022-11-04 20:13:33 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1450/1575]	Time 0.021 (0.022)	Loss 3.9023 (3.8640)	Acc@1 0.000 (1.844)	Acc@5 0.000 (10.407)	Mem 215MB
[2022-11-04 20:13:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1460/1575]	Time 0.018 (0.022)	Loss 3.6035 (3.8631)	Acc@1 0.000 (1.831)	Acc@5 25.000 (10.387)	Mem 215MB
[2022-11-04 20:13:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1470/1575]	Time 0.018 (0.022)	Loss 4.0078 (3.8620)	Acc@1 0.000 (1.818)	Acc@5 0.000 (10.452)	Mem 215MB
[2022-11-04 20:13:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1480/1575]	Time 0.019 (0.022)	Loss 3.8984 (3.8633)	Acc@1 0.000 (1.806)	Acc@5 0.000 (10.381)	Mem 215MB
[2022-11-04 20:13:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1490/1575]	Time 0.019 (0.022)	Loss 4.1562 (3.8646)	Acc@1 0.000 (1.794)	Acc@5 0.000 (10.329)	Mem 215MB
[2022-11-04 20:13:34 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1500/1575]	Time 0.020 (0.022)	Loss 4.0781 (3.8659)	Acc@1 0.000 (1.782)	Acc@5 0.000 (10.260)	Mem 215MB
[2022-11-04 20:13:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1510/1575]	Time 0.021 (0.022)	Loss 3.9121 (3.8656)	Acc@1 0.000 (1.770)	Acc@5 0.000 (10.242)	Mem 215MB
[2022-11-04 20:13:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1520/1575]	Time 0.019 (0.022)	Loss 3.9004 (3.8642)	Acc@1 0.000 (1.775)	Acc@5 0.000 (10.322)	Mem 215MB
[2022-11-04 20:13:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1530/1575]	Time 0.020 (0.022)	Loss 3.8281 (3.8636)	Acc@1 0.000 (1.764)	Acc@5 0.000 (10.255)	Mem 215MB
[2022-11-04 20:13:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1540/1575]	Time 0.020 (0.022)	Loss 3.9102 (3.8625)	Acc@1 0.000 (1.752)	Acc@5 25.000 (10.318)	Mem 215MB
[2022-11-04 20:13:35 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1550/1575]	Time 0.021 (0.022)	Loss 3.8613 (3.8626)	Acc@1 0.000 (1.757)	Acc@5 0.000 (10.284)	Mem 215MB
[2022-11-04 20:13:36 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1560/1575]	Time 0.022 (0.022)	Loss 3.8008 (3.8627)	Acc@1 0.000 (1.762)	Acc@5 0.000 (10.250)	Mem 215MB
[2022-11-04 20:13:36 swin_tiny_patch4_window7_224_resisc45] (main.py 301): INFO Test: [1570/1575]	Time 0.021 (0.022)	Loss 3.6758 (3.8625)	Acc@1 0.000 (1.766)	Acc@5 0.000 (10.232)	Mem 215MB
[2022-11-04 20:13:36 swin_tiny_patch4_window7_224_resisc45] (main.py 307): INFO  * Acc@1 1.778 Acc@5 10.222
[2022-11-04 20:13:36 swin_tiny_patch4_window7_224_resisc45] (main.py 162): INFO Accuracy (Top 1%) of the network on the 6300 test images: 1.8%
[2022-11-04 20:13:36 swin_tiny_patch4_window7_224_resisc45] (main.py 163): INFO Accuracy (Top 5%) of the network on the 6300 test images: 10.2%
[2022-11-04 20:13:36 swin_tiny_patch4_window7_224_resisc45] (main.py 176): INFO Start training
[2022-11-04 20:19:47 swin_tiny_patch4_window7_224_resisc45] (main.py 401): INFO Full config saved to output/swin_tiny_patch4_window7_224_resisc45/default/config.json
[2022-11-04 20:19:47 swin_tiny_patch4_window7_224_resisc45] (main.py 404): INFO AMP_ENABLE: false
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 4
  CACHE_MODE: 'no'
  DATASET: resisc45
  DATA_PATH: ./data/RESISC45/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 1
  PIN_MEMORY: false
  ZIP_MODE: true
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.0
  NAME: swin_tiny_patch4_window7_224_resisc45
  NUM_CLASSES: 45
  PRETRAINED: ''
  RESUME: ./pretrained/swin_tiny_patch4_window7_224_resisc45.pth
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FREEZE_LAYERS: true
    FREEZE_LAYER_INDEX:
    - 0
    - 1
    - 2
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_resisc45/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: true
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: false
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 25
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 3.90625e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: sgd
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 3.90625e-09
  WEIGHT_DECAY: 0.05
USE_Distributed_Data_Parallel: false

[2022-11-04 20:19:47 swin_tiny_patch4_window7_224_resisc45] (main.py 405): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_resisc45.yaml", "opts": null, "batch_size": null, "data_path": "./data/RESISC45/", "zip": true, "cache_mode": "no", "pretrained": null, "resume": "./pretrained/swin_tiny_patch4_window7_224_resisc45.pth", "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": null, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2022-11-04 20:19:47 swin_tiny_patch4_window7_224_resisc45] (main.py 91): INFO Creating model:swin/swin_tiny_patch4_window7_224_resisc45
[2022-11-04 20:19:47 swin_tiny_patch4_window7_224_resisc45] (main.py 93): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.127)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.145)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.164)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.182)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=45, bias=True)
)
[2022-11-04 20:19:47 swin_tiny_patch4_window7_224_resisc45] (main.py 97): INFO Freezing Layers: [0, 1, 2]
[2022-11-04 20:19:47 swin_tiny_patch4_window7_224_resisc45] (main.py 114): INFO number of params: 14219997
[2022-11-04 20:19:47 swin_tiny_patch4_window7_224_resisc45] (main.py 117): INFO number of GFLOPs: 4.49367168
[2022-11-04 20:19:49 swin_tiny_patch4_window7_224_resisc45] (utils.py 15): INFO ==============> Resuming form ./pretrained/swin_tiny_patch4_window7_224_resisc45.pth....................
[2022-11-04 20:19:49 swin_tiny_patch4_window7_224_resisc45] (utils.py 30): INFO <All keys matched successfully>
[2022-11-04 20:19:51 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [0/1575]	Time 1.989 (1.989)	Loss 4.0120 (4.0120)	Acc@1 0.000 (0.000)	Acc@5 0.000 (0.000)	Mem 215MB
[2022-11-04 20:19:52 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [10/1575]	Time 0.018 (0.204)	Loss 3.9767 (3.9207)	Acc@1 0.000 (0.000)	Acc@5 0.000 (4.545)	Mem 215MB
[2022-11-04 20:19:52 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [20/1575]	Time 0.018 (0.115)	Loss 3.6896 (3.8456)	Acc@1 25.000 (1.190)	Acc@5 25.000 (5.952)	Mem 215MB
[2022-11-04 20:19:52 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [30/1575]	Time 0.019 (0.084)	Loss 3.8215 (3.8620)	Acc@1 0.000 (0.806)	Acc@5 0.000 (6.452)	Mem 215MB
[2022-11-04 20:19:52 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [40/1575]	Time 0.018 (0.068)	Loss 4.0370 (3.8671)	Acc@1 0.000 (1.220)	Acc@5 0.000 (6.707)	Mem 215MB
[2022-11-04 20:19:52 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [50/1575]	Time 0.019 (0.058)	Loss 3.8835 (3.8751)	Acc@1 0.000 (0.980)	Acc@5 0.000 (5.882)	Mem 215MB
[2022-11-04 20:19:52 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [60/1575]	Time 0.019 (0.052)	Loss 3.8614 (3.8806)	Acc@1 0.000 (0.820)	Acc@5 0.000 (6.557)	Mem 215MB
[2022-11-04 20:19:53 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [70/1575]	Time 0.018 (0.047)	Loss 3.7670 (3.8751)	Acc@1 0.000 (1.056)	Acc@5 0.000 (7.746)	Mem 215MB
[2022-11-04 20:19:53 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [80/1575]	Time 0.018 (0.043)	Loss 3.6709 (3.8663)	Acc@1 0.000 (0.926)	Acc@5 0.000 (8.025)	Mem 215MB
[2022-11-04 20:19:53 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [90/1575]	Time 0.019 (0.041)	Loss 3.8564 (3.8694)	Acc@1 0.000 (0.824)	Acc@5 0.000 (7.692)	Mem 215MB
[2022-11-04 20:19:53 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [100/1575]	Time 0.018 (0.038)	Loss 4.0126 (3.8678)	Acc@1 0.000 (0.743)	Acc@5 0.000 (7.426)	Mem 215MB
[2022-11-04 20:19:53 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [110/1575]	Time 0.019 (0.037)	Loss 3.8076 (3.8650)	Acc@1 0.000 (0.676)	Acc@5 0.000 (6.982)	Mem 215MB
[2022-11-04 20:19:54 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [120/1575]	Time 0.018 (0.035)	Loss 3.9044 (3.8618)	Acc@1 0.000 (0.826)	Acc@5 0.000 (7.025)	Mem 215MB
[2022-11-04 20:19:54 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [130/1575]	Time 0.018 (0.034)	Loss 4.0255 (3.8566)	Acc@1 0.000 (0.763)	Acc@5 0.000 (7.443)	Mem 215MB
[2022-11-04 20:19:54 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [140/1575]	Time 0.018 (0.033)	Loss 3.8242 (3.8506)	Acc@1 0.000 (0.887)	Acc@5 0.000 (7.447)	Mem 215MB
[2022-11-04 20:19:54 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [150/1575]	Time 0.018 (0.032)	Loss 3.9419 (3.8601)	Acc@1 0.000 (0.828)	Acc@5 0.000 (7.616)	Mem 215MB
[2022-11-04 20:19:54 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [160/1575]	Time 0.020 (0.031)	Loss 4.0920 (3.8635)	Acc@1 0.000 (0.776)	Acc@5 0.000 (7.453)	Mem 215MB
[2022-11-04 20:19:54 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [170/1575]	Time 0.018 (0.030)	Loss 3.7431 (3.8676)	Acc@1 0.000 (0.877)	Acc@5 0.000 (7.310)	Mem 215MB
[2022-11-04 20:19:55 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [180/1575]	Time 0.018 (0.029)	Loss 3.8656 (3.8655)	Acc@1 0.000 (0.829)	Acc@5 0.000 (7.182)	Mem 215MB
[2022-11-04 20:19:55 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [190/1575]	Time 0.018 (0.029)	Loss 3.5074 (3.8571)	Acc@1 0.000 (1.047)	Acc@5 25.000 (8.115)	Mem 215MB
[2022-11-04 20:19:55 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [200/1575]	Time 0.019 (0.028)	Loss 3.9542 (3.8560)	Acc@1 0.000 (0.995)	Acc@5 0.000 (8.209)	Mem 215MB
[2022-11-04 20:19:55 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [210/1575]	Time 0.017 (0.028)	Loss 3.7188 (3.8529)	Acc@1 0.000 (0.948)	Acc@5 25.000 (8.412)	Mem 215MB
[2022-11-04 20:19:55 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [220/1575]	Time 0.019 (0.027)	Loss 3.7063 (3.8446)	Acc@1 0.000 (1.018)	Acc@5 25.000 (8.710)	Mem 215MB
[2022-11-04 20:19:56 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [230/1575]	Time 0.018 (0.027)	Loss 3.7263 (3.8375)	Acc@1 25.000 (1.082)	Acc@5 25.000 (8.983)	Mem 215MB
[2022-11-04 20:19:56 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [240/1575]	Time 0.019 (0.027)	Loss 3.6846 (3.8287)	Acc@1 0.000 (1.037)	Acc@5 0.000 (9.336)	Mem 215MB
[2022-11-04 20:19:56 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [250/1575]	Time 0.019 (0.026)	Loss 4.1309 (3.8328)	Acc@1 0.000 (0.996)	Acc@5 0.000 (9.064)	Mem 215MB
[2022-11-04 20:19:56 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [260/1575]	Time 0.019 (0.026)	Loss 3.9134 (3.8405)	Acc@1 0.000 (0.958)	Acc@5 0.000 (8.716)	Mem 215MB
[2022-11-04 20:19:56 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [270/1575]	Time 0.018 (0.026)	Loss 3.9671 (3.8501)	Acc@1 0.000 (0.923)	Acc@5 0.000 (8.487)	Mem 215MB
[2022-11-04 20:19:56 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [280/1575]	Time 0.018 (0.026)	Loss 3.5067 (3.8557)	Acc@1 25.000 (0.979)	Acc@5 50.000 (8.452)	Mem 215MB
[2022-11-04 20:19:57 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [290/1575]	Time 0.018 (0.025)	Loss 3.3462 (3.8401)	Acc@1 0.000 (1.460)	Acc@5 75.000 (10.481)	Mem 215MB
[2022-11-04 20:19:57 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [300/1575]	Time 0.018 (0.025)	Loss 3.4437 (3.8239)	Acc@1 0.000 (2.243)	Acc@5 50.000 (12.126)	Mem 215MB
[2022-11-04 20:19:57 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [310/1575]	Time 0.018 (0.025)	Loss 3.3879 (3.8131)	Acc@1 25.000 (2.653)	Acc@5 50.000 (13.183)	Mem 215MB
[2022-11-04 20:19:57 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [320/1575]	Time 0.018 (0.025)	Loss 4.1606 (3.8127)	Acc@1 0.000 (2.960)	Acc@5 0.000 (13.707)	Mem 215MB
[2022-11-04 20:19:57 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [330/1575]	Time 0.018 (0.024)	Loss 3.8822 (3.8176)	Acc@1 0.000 (2.946)	Acc@5 0.000 (13.671)	Mem 215MB
[2022-11-04 20:19:58 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [340/1575]	Time 0.018 (0.024)	Loss 4.3059 (3.8256)	Acc@1 0.000 (2.859)	Acc@5 0.000 (13.343)	Mem 215MB
[2022-11-04 20:19:58 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [350/1575]	Time 0.018 (0.024)	Loss 4.1926 (3.8344)	Acc@1 0.000 (2.920)	Acc@5 0.000 (13.177)	Mem 215MB
[2022-11-04 20:19:58 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [360/1575]	Time 0.019 (0.024)	Loss 4.2001 (3.8436)	Acc@1 0.000 (2.839)	Acc@5 0.000 (12.812)	Mem 215MB
[2022-11-04 20:19:58 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [370/1575]	Time 0.019 (0.024)	Loss 4.4033 (3.8533)	Acc@1 0.000 (2.763)	Acc@5 0.000 (12.466)	Mem 215MB
[2022-11-04 20:19:58 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [380/1575]	Time 0.018 (0.024)	Loss 4.0715 (3.8623)	Acc@1 0.000 (2.690)	Acc@5 0.000 (12.205)	Mem 215MB
[2022-11-04 20:19:58 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [390/1575]	Time 0.019 (0.024)	Loss 3.6599 (3.8594)	Acc@1 0.000 (2.685)	Acc@5 0.000 (12.596)	Mem 215MB
[2022-11-04 20:19:59 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [400/1575]	Time 0.018 (0.023)	Loss 3.6040 (3.8497)	Acc@1 0.000 (2.868)	Acc@5 25.000 (13.342)	Mem 215MB
[2022-11-04 20:19:59 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [410/1575]	Time 0.018 (0.023)	Loss 3.3267 (3.8384)	Acc@1 50.000 (3.041)	Acc@5 50.000 (13.929)	Mem 215MB
[2022-11-04 20:19:59 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [420/1575]	Time 0.018 (0.023)	Loss 3.7545 (3.8294)	Acc@1 0.000 (3.385)	Acc@5 0.000 (14.430)	Mem 215MB
[2022-11-04 20:19:59 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [430/1575]	Time 0.018 (0.023)	Loss 3.6328 (3.8270)	Acc@1 0.000 (3.364)	Acc@5 25.000 (14.443)	Mem 215MB
[2022-11-04 20:19:59 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [440/1575]	Time 0.018 (0.023)	Loss 3.6599 (3.8249)	Acc@1 25.000 (3.515)	Acc@5 25.000 (14.512)	Mem 215MB
[2022-11-04 20:20:00 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [450/1575]	Time 0.018 (0.023)	Loss 3.6971 (3.8220)	Acc@1 0.000 (3.603)	Acc@5 0.000 (14.523)	Mem 215MB
[2022-11-04 20:20:00 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [460/1575]	Time 0.018 (0.023)	Loss 3.6761 (3.8203)	Acc@1 0.000 (3.579)	Acc@5 50.000 (14.588)	Mem 215MB
[2022-11-04 20:20:00 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [470/1575]	Time 0.018 (0.023)	Loss 3.8026 (3.8215)	Acc@1 0.000 (3.556)	Acc@5 25.000 (14.384)	Mem 215MB
[2022-11-04 20:20:00 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [480/1575]	Time 0.019 (0.023)	Loss 3.6618 (3.8201)	Acc@1 0.000 (3.482)	Acc@5 25.000 (14.553)	Mem 215MB
[2022-11-04 20:20:00 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [490/1575]	Time 0.018 (0.022)	Loss 4.3267 (3.8210)	Acc@1 0.000 (3.411)	Acc@5 0.000 (14.358)	Mem 215MB
[2022-11-04 20:20:01 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [500/1575]	Time 0.019 (0.022)	Loss 4.1516 (3.8265)	Acc@1 0.000 (3.343)	Acc@5 0.000 (14.122)	Mem 215MB
[2022-11-04 20:20:01 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [510/1575]	Time 0.019 (0.022)	Loss 4.4596 (3.8316)	Acc@1 0.000 (3.278)	Acc@5 0.000 (13.992)	Mem 215MB
[2022-11-04 20:20:01 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [520/1575]	Time 0.019 (0.022)	Loss 4.4003 (3.8377)	Acc@1 0.000 (3.215)	Acc@5 0.000 (13.724)	Mem 215MB
[2022-11-04 20:20:01 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [530/1575]	Time 0.018 (0.022)	Loss 4.0867 (3.8410)	Acc@1 0.000 (3.154)	Acc@5 0.000 (13.512)	Mem 215MB
[2022-11-04 20:20:01 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [540/1575]	Time 0.017 (0.022)	Loss 3.9277 (3.8436)	Acc@1 0.000 (3.096)	Acc@5 0.000 (13.309)	Mem 215MB
[2022-11-04 20:20:01 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [550/1575]	Time 0.018 (0.022)	Loss 4.0867 (3.8462)	Acc@1 0.000 (3.040)	Acc@5 0.000 (13.067)	Mem 215MB
[2022-11-04 20:20:02 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [560/1575]	Time 0.017 (0.022)	Loss 3.9647 (3.8491)	Acc@1 0.000 (2.986)	Acc@5 0.000 (12.834)	Mem 215MB
[2022-11-04 20:20:02 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [570/1575]	Time 0.018 (0.022)	Loss 4.4783 (3.8552)	Acc@1 0.000 (2.933)	Acc@5 0.000 (12.609)	Mem 215MB
[2022-11-04 20:20:02 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [580/1575]	Time 0.018 (0.022)	Loss 4.0591 (3.8610)	Acc@1 0.000 (2.883)	Acc@5 0.000 (12.392)	Mem 215MB
[2022-11-04 20:20:02 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [590/1575]	Time 0.018 (0.022)	Loss 4.3115 (3.8670)	Acc@1 0.000 (2.834)	Acc@5 0.000 (12.183)	Mem 215MB
[2022-11-04 20:20:02 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [600/1575]	Time 0.018 (0.022)	Loss 4.2213 (3.8707)	Acc@1 0.000 (2.787)	Acc@5 0.000 (11.980)	Mem 215MB
[2022-11-04 20:20:03 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [610/1575]	Time 0.020 (0.022)	Loss 4.0614 (3.8723)	Acc@1 0.000 (2.741)	Acc@5 0.000 (11.784)	Mem 215MB
[2022-11-04 20:20:03 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [620/1575]	Time 0.018 (0.022)	Loss 3.8249 (3.8738)	Acc@1 0.000 (2.697)	Acc@5 0.000 (11.634)	Mem 215MB
[2022-11-04 20:20:03 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [630/1575]	Time 0.018 (0.022)	Loss 3.9817 (3.8759)	Acc@1 0.000 (2.655)	Acc@5 0.000 (11.450)	Mem 215MB
[2022-11-04 20:20:03 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [640/1575]	Time 0.017 (0.022)	Loss 3.9644 (3.8759)	Acc@1 0.000 (2.613)	Acc@5 25.000 (11.349)	Mem 215MB
[2022-11-04 20:20:03 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [650/1575]	Time 0.018 (0.022)	Loss 3.6725 (3.8752)	Acc@1 0.000 (2.573)	Acc@5 25.000 (11.406)	Mem 215MB
[2022-11-04 20:20:03 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [660/1575]	Time 0.019 (0.021)	Loss 3.7194 (3.8749)	Acc@1 0.000 (2.534)	Acc@5 25.000 (11.309)	Mem 215MB
[2022-11-04 20:20:04 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [670/1575]	Time 0.018 (0.021)	Loss 3.3378 (3.8724)	Acc@1 25.000 (2.683)	Acc@5 50.000 (11.438)	Mem 215MB
[2022-11-04 20:20:04 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [680/1575]	Time 0.019 (0.021)	Loss 3.6315 (3.8676)	Acc@1 0.000 (2.900)	Acc@5 25.000 (11.821)	Mem 215MB
[2022-11-04 20:20:04 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [690/1575]	Time 0.019 (0.021)	Loss 3.4400 (3.8622)	Acc@1 0.000 (2.931)	Acc@5 75.000 (12.301)	Mem 215MB
[2022-11-04 20:20:04 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [700/1575]	Time 0.019 (0.021)	Loss 4.0554 (3.8578)	Acc@1 0.000 (2.996)	Acc@5 0.000 (12.660)	Mem 215MB
[2022-11-04 20:20:04 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [710/1575]	Time 0.019 (0.021)	Loss 3.7091 (3.8584)	Acc@1 0.000 (2.954)	Acc@5 25.000 (12.588)	Mem 215MB
[2022-11-04 20:20:05 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [720/1575]	Time 0.019 (0.021)	Loss 3.8648 (3.8600)	Acc@1 0.000 (2.913)	Acc@5 0.000 (12.413)	Mem 215MB
[2022-11-04 20:20:05 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [730/1575]	Time 0.018 (0.021)	Loss 4.0208 (3.8605)	Acc@1 0.000 (2.873)	Acc@5 0.000 (12.278)	Mem 215MB
[2022-11-04 20:20:05 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [740/1575]	Time 0.018 (0.021)	Loss 4.1243 (3.8612)	Acc@1 0.000 (2.868)	Acc@5 0.000 (12.179)	Mem 215MB
[2022-11-04 20:20:05 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [750/1575]	Time 0.018 (0.021)	Loss 3.5772 (3.8616)	Acc@1 0.000 (2.830)	Acc@5 0.000 (12.084)	Mem 215MB
[2022-11-04 20:20:05 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [760/1575]	Time 0.018 (0.021)	Loss 4.0956 (3.8622)	Acc@1 0.000 (2.792)	Acc@5 0.000 (11.958)	Mem 215MB
[2022-11-04 20:20:05 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [770/1575]	Time 0.019 (0.021)	Loss 3.7485 (3.8622)	Acc@1 0.000 (2.756)	Acc@5 25.000 (11.868)	Mem 215MB
[2022-11-04 20:20:06 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [780/1575]	Time 0.019 (0.021)	Loss 3.7236 (3.8598)	Acc@1 0.000 (2.785)	Acc@5 25.000 (11.940)	Mem 215MB
[2022-11-04 20:20:06 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [790/1575]	Time 0.019 (0.021)	Loss 3.6174 (3.8578)	Acc@1 0.000 (2.845)	Acc@5 0.000 (12.073)	Mem 215MB
[2022-11-04 20:20:06 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [800/1575]	Time 0.019 (0.021)	Loss 3.5413 (3.8560)	Acc@1 0.000 (2.809)	Acc@5 50.000 (12.141)	Mem 215MB
[2022-11-04 20:20:06 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [810/1575]	Time 0.019 (0.021)	Loss 3.7270 (3.8548)	Acc@1 0.000 (2.805)	Acc@5 0.000 (12.145)	Mem 215MB
[2022-11-04 20:20:06 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [820/1575]	Time 0.019 (0.021)	Loss 3.9953 (3.8551)	Acc@1 0.000 (2.771)	Acc@5 0.000 (12.058)	Mem 215MB
[2022-11-04 20:20:07 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [830/1575]	Time 0.019 (0.021)	Loss 4.0509 (3.8559)	Acc@1 0.000 (2.738)	Acc@5 0.000 (11.943)	Mem 215MB
[2022-11-04 20:20:07 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [840/1575]	Time 0.018 (0.021)	Loss 3.7027 (3.8557)	Acc@1 0.000 (2.705)	Acc@5 0.000 (11.861)	Mem 215MB
[2022-11-04 20:20:07 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [850/1575]	Time 0.018 (0.021)	Loss 3.6033 (3.8566)	Acc@1 0.000 (2.673)	Acc@5 25.000 (11.751)	Mem 215MB
[2022-11-04 20:20:07 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [860/1575]	Time 0.018 (0.021)	Loss 3.8063 (3.8571)	Acc@1 0.000 (2.642)	Acc@5 25.000 (11.702)	Mem 215MB
[2022-11-04 20:20:07 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [870/1575]	Time 0.018 (0.021)	Loss 3.6585 (3.8561)	Acc@1 0.000 (2.641)	Acc@5 50.000 (11.739)	Mem 215MB
[2022-11-04 20:20:08 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [880/1575]	Time 0.020 (0.021)	Loss 4.0461 (3.8574)	Acc@1 0.000 (2.611)	Acc@5 0.000 (11.663)	Mem 215MB
[2022-11-04 20:20:08 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [890/1575]	Time 0.019 (0.021)	Loss 4.0917 (3.8599)	Acc@1 0.000 (2.581)	Acc@5 0.000 (11.532)	Mem 215MB
[2022-11-04 20:20:08 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [900/1575]	Time 0.019 (0.021)	Loss 4.1762 (3.8615)	Acc@1 0.000 (2.553)	Acc@5 0.000 (11.459)	Mem 215MB
[2022-11-04 20:20:08 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [910/1575]	Time 0.019 (0.021)	Loss 3.7313 (3.8628)	Acc@1 0.000 (2.525)	Acc@5 0.000 (11.334)	Mem 215MB
[2022-11-04 20:20:08 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [920/1575]	Time 0.019 (0.021)	Loss 3.5175 (3.8611)	Acc@1 0.000 (2.497)	Acc@5 50.000 (11.346)	Mem 215MB
[2022-11-04 20:20:08 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [930/1575]	Time 0.019 (0.021)	Loss 3.8616 (3.8597)	Acc@1 0.000 (2.497)	Acc@5 0.000 (11.332)	Mem 215MB
[2022-11-04 20:20:09 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [940/1575]	Time 0.018 (0.021)	Loss 3.7571 (3.8591)	Acc@1 0.000 (2.471)	Acc@5 0.000 (11.318)	Mem 215MB
[2022-11-04 20:20:09 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [950/1575]	Time 0.018 (0.021)	Loss 4.0535 (3.8609)	Acc@1 0.000 (2.445)	Acc@5 0.000 (11.251)	Mem 215MB
[2022-11-04 20:20:09 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [960/1575]	Time 0.018 (0.020)	Loss 4.3918 (3.8643)	Acc@1 0.000 (2.419)	Acc@5 0.000 (11.134)	Mem 215MB
[2022-11-04 20:20:09 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [970/1575]	Time 0.018 (0.020)	Loss 4.2160 (3.8685)	Acc@1 0.000 (2.394)	Acc@5 0.000 (11.020)	Mem 215MB
[2022-11-04 20:20:09 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [980/1575]	Time 0.018 (0.020)	Loss 3.6929 (3.8714)	Acc@1 0.000 (2.370)	Acc@5 25.000 (10.933)	Mem 215MB
[2022-11-04 20:20:10 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [990/1575]	Time 0.018 (0.020)	Loss 3.5521 (3.8686)	Acc@1 0.000 (2.371)	Acc@5 25.000 (11.100)	Mem 215MB
[2022-11-04 20:20:10 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1000/1575]	Time 0.018 (0.020)	Loss 3.6425 (3.8662)	Acc@1 0.000 (2.348)	Acc@5 25.000 (11.239)	Mem 215MB
[2022-11-04 20:20:10 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1010/1575]	Time 0.018 (0.020)	Loss 3.5696 (3.8635)	Acc@1 0.000 (2.349)	Acc@5 50.000 (11.400)	Mem 215MB
[2022-11-04 20:20:10 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1020/1575]	Time 0.018 (0.020)	Loss 4.0291 (3.8632)	Acc@1 0.000 (2.326)	Acc@5 0.000 (11.410)	Mem 215MB
[2022-11-04 20:20:10 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1030/1575]	Time 0.018 (0.020)	Loss 4.0240 (3.8634)	Acc@1 0.000 (2.304)	Acc@5 0.000 (11.348)	Mem 215MB
[2022-11-04 20:20:10 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1040/1575]	Time 0.019 (0.020)	Loss 3.6990 (3.8636)	Acc@1 0.000 (2.305)	Acc@5 0.000 (11.263)	Mem 215MB
[2022-11-04 20:20:11 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1050/1575]	Time 0.018 (0.020)	Loss 3.6375 (3.8636)	Acc@1 0.000 (2.284)	Acc@5 25.000 (11.227)	Mem 215MB
[2022-11-04 20:20:11 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1060/1575]	Time 0.019 (0.020)	Loss 3.5486 (3.8620)	Acc@1 0.000 (2.286)	Acc@5 25.000 (11.287)	Mem 215MB
[2022-11-04 20:20:11 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1070/1575]	Time 0.019 (0.020)	Loss 3.4265 (3.8598)	Acc@1 25.000 (2.334)	Acc@5 25.000 (11.345)	Mem 215MB
[2022-11-04 20:20:11 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1080/1575]	Time 0.019 (0.020)	Loss 3.6279 (3.8582)	Acc@1 0.000 (2.313)	Acc@5 50.000 (11.355)	Mem 215MB
[2022-11-04 20:20:11 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1090/1575]	Time 0.019 (0.020)	Loss 3.5514 (3.8568)	Acc@1 0.000 (2.291)	Acc@5 50.000 (11.389)	Mem 215MB
[2022-11-04 20:20:12 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1100/1575]	Time 0.020 (0.020)	Loss 3.8931 (3.8555)	Acc@1 0.000 (2.293)	Acc@5 0.000 (11.421)	Mem 215MB
[2022-11-04 20:20:12 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1110/1575]	Time 0.019 (0.020)	Loss 3.5106 (3.8545)	Acc@1 0.000 (2.273)	Acc@5 50.000 (11.499)	Mem 215MB
[2022-11-04 20:20:12 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1120/1575]	Time 0.019 (0.020)	Loss 3.9205 (3.8526)	Acc@1 0.000 (2.275)	Acc@5 0.000 (11.552)	Mem 215MB
[2022-11-04 20:20:12 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1130/1575]	Time 0.019 (0.020)	Loss 3.9519 (3.8528)	Acc@1 0.000 (2.255)	Acc@5 0.000 (11.450)	Mem 215MB
[2022-11-04 20:20:12 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1140/1575]	Time 0.018 (0.020)	Loss 3.8178 (3.8522)	Acc@1 0.000 (2.235)	Acc@5 0.000 (11.415)	Mem 215MB
[2022-11-04 20:20:13 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1150/1575]	Time 0.018 (0.020)	Loss 3.7003 (3.8523)	Acc@1 0.000 (2.215)	Acc@5 50.000 (11.381)	Mem 215MB
[2022-11-04 20:20:13 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1160/1575]	Time 0.018 (0.020)	Loss 4.1620 (3.8528)	Acc@1 0.000 (2.196)	Acc@5 0.000 (11.305)	Mem 215MB
[2022-11-04 20:20:13 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1170/1575]	Time 0.018 (0.020)	Loss 3.9801 (3.8544)	Acc@1 0.000 (2.178)	Acc@5 0.000 (11.208)	Mem 215MB
[2022-11-04 20:20:13 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1180/1575]	Time 0.018 (0.020)	Loss 4.0801 (3.8557)	Acc@1 0.000 (2.159)	Acc@5 0.000 (11.113)	Mem 215MB
[2022-11-04 20:20:13 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1190/1575]	Time 0.018 (0.020)	Loss 3.9067 (3.8566)	Acc@1 0.000 (2.141)	Acc@5 0.000 (11.041)	Mem 215MB
[2022-11-04 20:20:13 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1200/1575]	Time 0.019 (0.020)	Loss 3.6912 (3.8561)	Acc@1 0.000 (2.123)	Acc@5 25.000 (11.012)	Mem 215MB
[2022-11-04 20:20:14 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1210/1575]	Time 0.019 (0.020)	Loss 4.0742 (3.8560)	Acc@1 0.000 (2.106)	Acc@5 0.000 (11.024)	Mem 215MB
[2022-11-04 20:20:14 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1220/1575]	Time 0.018 (0.020)	Loss 3.5946 (3.8555)	Acc@1 25.000 (2.109)	Acc@5 25.000 (10.995)	Mem 215MB
[2022-11-04 20:20:14 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1230/1575]	Time 0.018 (0.020)	Loss 3.6610 (3.8547)	Acc@1 0.000 (2.092)	Acc@5 0.000 (11.028)	Mem 215MB
[2022-11-04 20:20:14 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1240/1575]	Time 0.018 (0.020)	Loss 3.6826 (3.8536)	Acc@1 0.000 (2.075)	Acc@5 0.000 (11.039)	Mem 215MB
[2022-11-04 20:20:14 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1250/1575]	Time 0.018 (0.020)	Loss 3.6100 (3.8523)	Acc@1 0.000 (2.078)	Acc@5 0.000 (11.071)	Mem 215MB
[2022-11-04 20:20:15 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1260/1575]	Time 0.018 (0.020)	Loss 3.5672 (3.8508)	Acc@1 0.000 (2.062)	Acc@5 25.000 (11.122)	Mem 215MB
[2022-11-04 20:20:15 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1270/1575]	Time 0.018 (0.020)	Loss 3.6377 (3.8497)	Acc@1 0.000 (2.065)	Acc@5 25.000 (11.192)	Mem 215MB
[2022-11-04 20:20:15 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1280/1575]	Time 0.019 (0.020)	Loss 3.5721 (3.8483)	Acc@1 0.000 (2.049)	Acc@5 25.000 (11.241)	Mem 215MB
[2022-11-04 20:20:15 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1290/1575]	Time 0.019 (0.020)	Loss 3.9595 (3.8474)	Acc@1 0.000 (2.053)	Acc@5 0.000 (11.290)	Mem 215MB
[2022-11-04 20:20:15 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1300/1575]	Time 0.019 (0.020)	Loss 3.8296 (3.8469)	Acc@1 0.000 (2.037)	Acc@5 0.000 (11.241)	Mem 215MB
[2022-11-04 20:20:15 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1310/1575]	Time 0.019 (0.020)	Loss 3.7710 (3.8465)	Acc@1 0.000 (2.021)	Acc@5 0.000 (11.175)	Mem 215MB
[2022-11-04 20:20:16 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1320/1575]	Time 0.019 (0.020)	Loss 3.7819 (3.8458)	Acc@1 0.000 (2.025)	Acc@5 25.000 (11.185)	Mem 215MB
[2022-11-04 20:20:16 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1330/1575]	Time 0.018 (0.020)	Loss 4.4857 (3.8456)	Acc@1 0.000 (2.010)	Acc@5 0.000 (11.176)	Mem 215MB
[2022-11-04 20:20:16 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1340/1575]	Time 0.018 (0.020)	Loss 4.4026 (3.8490)	Acc@1 0.000 (1.995)	Acc@5 0.000 (11.092)	Mem 215MB
[2022-11-04 20:20:16 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1350/1575]	Time 0.018 (0.020)	Loss 4.4879 (3.8530)	Acc@1 0.000 (1.980)	Acc@5 0.000 (11.010)	Mem 215MB
[2022-11-04 20:20:16 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1360/1575]	Time 0.019 (0.020)	Loss 3.9685 (3.8567)	Acc@1 0.000 (1.965)	Acc@5 0.000 (10.929)	Mem 215MB
[2022-11-04 20:20:17 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1370/1575]	Time 0.018 (0.020)	Loss 3.7833 (3.8584)	Acc@1 0.000 (1.951)	Acc@5 0.000 (10.850)	Mem 215MB
[2022-11-04 20:20:17 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1380/1575]	Time 0.019 (0.020)	Loss 4.2253 (3.8594)	Acc@1 0.000 (1.937)	Acc@5 0.000 (10.789)	Mem 215MB
[2022-11-04 20:20:17 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1390/1575]	Time 0.019 (0.020)	Loss 4.1332 (3.8599)	Acc@1 0.000 (1.923)	Acc@5 0.000 (10.712)	Mem 215MB
[2022-11-04 20:20:17 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1400/1575]	Time 0.019 (0.020)	Loss 4.1581 (3.8607)	Acc@1 0.000 (1.909)	Acc@5 0.000 (10.635)	Mem 215MB
[2022-11-04 20:20:17 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1410/1575]	Time 0.018 (0.020)	Loss 4.1249 (3.8619)	Acc@1 0.000 (1.896)	Acc@5 0.000 (10.595)	Mem 215MB
[2022-11-04 20:20:18 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1420/1575]	Time 0.018 (0.020)	Loss 3.8251 (3.8633)	Acc@1 0.000 (1.882)	Acc@5 0.000 (10.521)	Mem 215MB
[2022-11-04 20:20:18 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1430/1575]	Time 0.018 (0.020)	Loss 4.0960 (3.8644)	Acc@1 0.000 (1.869)	Acc@5 0.000 (10.447)	Mem 215MB
[2022-11-04 20:20:18 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1440/1575]	Time 0.018 (0.020)	Loss 3.9098 (3.8647)	Acc@1 0.000 (1.856)	Acc@5 25.000 (10.427)	Mem 215MB
[2022-11-04 20:20:18 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1450/1575]	Time 0.021 (0.020)	Loss 3.9011 (3.8641)	Acc@1 0.000 (1.844)	Acc@5 0.000 (10.372)	Mem 215MB
[2022-11-04 20:20:18 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1460/1575]	Time 0.019 (0.020)	Loss 3.6028 (3.8632)	Acc@1 0.000 (1.831)	Acc@5 25.000 (10.370)	Mem 215MB
[2022-11-04 20:20:18 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1470/1575]	Time 0.018 (0.020)	Loss 4.0060 (3.8621)	Acc@1 0.000 (1.818)	Acc@5 0.000 (10.435)	Mem 215MB
[2022-11-04 20:20:19 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1480/1575]	Time 0.018 (0.020)	Loss 3.8994 (3.8634)	Acc@1 0.000 (1.806)	Acc@5 0.000 (10.365)	Mem 215MB
[2022-11-04 20:20:19 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1490/1575]	Time 0.018 (0.020)	Loss 4.1568 (3.8647)	Acc@1 0.000 (1.794)	Acc@5 0.000 (10.312)	Mem 215MB
[2022-11-04 20:20:19 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1500/1575]	Time 0.018 (0.020)	Loss 4.0785 (3.8660)	Acc@1 0.000 (1.782)	Acc@5 0.000 (10.243)	Mem 215MB
[2022-11-04 20:20:19 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1510/1575]	Time 0.019 (0.020)	Loss 3.9105 (3.8657)	Acc@1 0.000 (1.770)	Acc@5 0.000 (10.225)	Mem 215MB
[2022-11-04 20:20:19 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1520/1575]	Time 0.018 (0.020)	Loss 3.8991 (3.8643)	Acc@1 0.000 (1.775)	Acc@5 0.000 (10.306)	Mem 215MB
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1530/1575]	Time 0.018 (0.020)	Loss 3.8269 (3.8637)	Acc@1 0.000 (1.764)	Acc@5 0.000 (10.238)	Mem 215MB
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1540/1575]	Time 0.018 (0.020)	Loss 3.9102 (3.8626)	Acc@1 0.000 (1.752)	Acc@5 25.000 (10.302)	Mem 215MB
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1550/1575]	Time 0.019 (0.020)	Loss 3.8624 (3.8627)	Acc@1 0.000 (1.757)	Acc@5 0.000 (10.268)	Mem 215MB
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1560/1575]	Time 0.019 (0.020)	Loss 3.8003 (3.8628)	Acc@1 0.000 (1.762)	Acc@5 0.000 (10.234)	Mem 215MB
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 302): INFO Test: [1570/1575]	Time 0.019 (0.020)	Loss 3.6765 (3.8626)	Acc@1 0.000 (1.766)	Acc@5 0.000 (10.216)	Mem 215MB
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 308): INFO  * Acc@1 1.778 Acc@5 10.206
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 162): INFO Accuracy (Top 1%) of the network on the 6300 test images: 1.8%
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 163): INFO Accuracy (Top 5%) of the network on the 6300 test images: 10.2%
[2022-11-04 20:20:20 swin_tiny_patch4_window7_224_resisc45] (main.py 176): INFO Start training
[2022-11-04 20:20:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][0/6300]	eta 0:43:35 lr 0.000008	 wd 0.0500	time 0.4152 (0.4152)	loss 3.8584 (3.8584)	grad_norm 12.7183 (12.7183)	loss_scale 65536.0000 (65536.0000)	mem 235MB
[2022-11-04 20:20:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][10/6300]	eta 0:06:48 lr 0.000008	 wd 0.0500	time 0.0278 (0.0649)	loss 3.5287 (3.7298)	grad_norm 14.2882 (14.4422)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][20/6300]	eta 0:04:58 lr 0.000008	 wd 0.0500	time 0.0279 (0.0475)	loss 3.5439 (3.7372)	grad_norm 15.3229 (14.2598)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][30/6300]	eta 0:04:19 lr 0.000008	 wd 0.0500	time 0.0273 (0.0413)	loss 3.5120 (3.7049)	grad_norm 16.0049 (14.3333)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][40/6300]	eta 0:03:58 lr 0.000008	 wd 0.0500	time 0.0284 (0.0382)	loss 3.9126 (3.6725)	grad_norm 14.7912 (14.4990)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][50/6300]	eta 0:03:47 lr 0.000008	 wd 0.0500	time 0.0291 (0.0363)	loss 3.6197 (3.6545)	grad_norm 13.8248 (14.2905)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][60/6300]	eta 0:03:38 lr 0.000008	 wd 0.0500	time 0.0277 (0.0350)	loss 3.6566 (3.6325)	grad_norm 14.6434 (14.2623)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][70/6300]	eta 0:03:33 lr 0.000008	 wd 0.0500	time 0.0287 (0.0343)	loss 3.7681 (3.6153)	grad_norm 13.6112 (14.4000)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][80/6300]	eta 0:03:28 lr 0.000008	 wd 0.0500	time 0.0286 (0.0335)	loss 3.4491 (3.5977)	grad_norm 14.8849 (14.3845)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][90/6300]	eta 0:03:25 lr 0.000008	 wd 0.0500	time 0.0276 (0.0330)	loss 3.5450 (3.5824)	grad_norm 12.8276 (14.3042)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][100/6300]	eta 0:03:21 lr 0.000008	 wd 0.0500	time 0.0280 (0.0326)	loss 3.3105 (3.5656)	grad_norm 15.8087 (14.3260)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][110/6300]	eta 0:03:19 lr 0.000008	 wd 0.0500	time 0.0283 (0.0322)	loss 3.1432 (3.5414)	grad_norm 16.4457 (14.3321)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][120/6300]	eta 0:03:16 lr 0.000008	 wd 0.0500	time 0.0284 (0.0319)	loss 3.6356 (3.5287)	grad_norm 14.2089 (14.3221)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][130/6300]	eta 0:03:15 lr 0.000008	 wd 0.0500	time 0.0275 (0.0316)	loss 3.0154 (3.5067)	grad_norm 15.4086 (14.3181)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][140/6300]	eta 0:03:14 lr 0.000008	 wd 0.0500	time 0.0288 (0.0315)	loss 3.9064 (3.4947)	grad_norm 14.5035 (14.3232)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][150/6300]	eta 0:03:12 lr 0.000008	 wd 0.0500	time 0.0307 (0.0314)	loss 3.9341 (3.5228)	grad_norm 16.4303 (14.4305)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][160/6300]	eta 0:03:12 lr 0.000008	 wd 0.0500	time 0.0294 (0.0314)	loss 4.0695 (3.5403)	grad_norm 14.7255 (14.4943)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][170/6300]	eta 0:03:11 lr 0.000008	 wd 0.0500	time 0.0280 (0.0312)	loss 3.7794 (3.5564)	grad_norm 18.6907 (14.5578)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][180/6300]	eta 0:03:10 lr 0.000008	 wd 0.0500	time 0.0301 (0.0311)	loss 3.6101 (3.5630)	grad_norm 16.7469 (14.6480)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][190/6300]	eta 0:03:09 lr 0.000008	 wd 0.0500	time 0.0289 (0.0309)	loss 3.3577 (3.5667)	grad_norm 17.6969 (14.6949)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][200/6300]	eta 0:03:08 lr 0.000008	 wd 0.0500	time 0.0300 (0.0309)	loss 3.5394 (3.5747)	grad_norm 16.3316 (14.7123)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][210/6300]	eta 0:03:08 lr 0.000008	 wd 0.0500	time 0.0342 (0.0309)	loss 3.6370 (3.5782)	grad_norm 16.2353 (14.7661)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][220/6300]	eta 0:03:08 lr 0.000008	 wd 0.0500	time 0.0372 (0.0310)	loss 3.3745 (3.5782)	grad_norm 14.2677 (14.7751)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][230/6300]	eta 0:03:09 lr 0.000008	 wd 0.0500	time 0.0311 (0.0312)	loss 3.5472 (3.5761)	grad_norm 15.8967 (14.8155)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][240/6300]	eta 0:03:08 lr 0.000008	 wd 0.0500	time 0.0281 (0.0312)	loss 3.9365 (3.5755)	grad_norm 14.9759 (14.8435)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][250/6300]	eta 0:03:08 lr 0.000008	 wd 0.0500	time 0.0315 (0.0312)	loss 3.5338 (3.5742)	grad_norm 15.7359 (14.8667)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][260/6300]	eta 0:03:08 lr 0.000008	 wd 0.0500	time 0.0289 (0.0312)	loss 3.2592 (3.5671)	grad_norm 13.6453 (14.8518)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][270/6300]	eta 0:03:07 lr 0.000008	 wd 0.0500	time 0.0292 (0.0311)	loss 3.4267 (3.5611)	grad_norm 16.7423 (14.8884)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][280/6300]	eta 0:03:07 lr 0.000008	 wd 0.0500	time 0.0355 (0.0311)	loss 3.6637 (3.5551)	grad_norm 14.7307 (14.9009)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][290/6300]	eta 0:03:07 lr 0.000008	 wd 0.0500	time 0.0319 (0.0312)	loss 3.8348 (3.5644)	grad_norm 14.5826 (14.8821)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][300/6300]	eta 0:03:07 lr 0.000008	 wd 0.0500	time 0.0330 (0.0312)	loss 3.7759 (3.5722)	grad_norm 15.7393 (14.8781)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][310/6300]	eta 0:03:06 lr 0.000008	 wd 0.0500	time 0.0318 (0.0312)	loss 3.9451 (3.5780)	grad_norm 15.0454 (14.8570)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][320/6300]	eta 0:03:06 lr 0.000008	 wd 0.0500	time 0.0287 (0.0312)	loss 3.6907 (3.5805)	grad_norm 15.5470 (14.8431)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][330/6300]	eta 0:03:05 lr 0.000008	 wd 0.0500	time 0.0280 (0.0311)	loss 3.5755 (3.5844)	grad_norm 12.1211 (14.8419)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][340/6300]	eta 0:03:05 lr 0.000008	 wd 0.0500	time 0.0280 (0.0311)	loss 3.5844 (3.5837)	grad_norm 16.0558 (14.8179)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][350/6300]	eta 0:03:04 lr 0.000008	 wd 0.0500	time 0.0286 (0.0311)	loss 3.3716 (3.5843)	grad_norm 15.0125 (14.8196)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][360/6300]	eta 0:03:04 lr 0.000008	 wd 0.0500	time 0.0283 (0.0310)	loss 3.7250 (3.5855)	grad_norm 15.1896 (14.8177)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][370/6300]	eta 0:03:03 lr 0.000008	 wd 0.0500	time 0.0307 (0.0310)	loss 3.5183 (3.5842)	grad_norm 13.4862 (14.7892)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][380/6300]	eta 0:03:03 lr 0.000008	 wd 0.0500	time 0.0284 (0.0309)	loss 3.5128 (3.5809)	grad_norm 13.0508 (14.7617)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][390/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.3271 (3.5799)	grad_norm 13.4527 (14.7221)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][400/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0302 (0.0309)	loss 3.6246 (3.5773)	grad_norm 14.4426 (14.7012)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][410/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0366 (0.0310)	loss 3.0867 (3.5739)	grad_norm 15.6711 (14.6998)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][420/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0324 (0.0311)	loss 3.6702 (3.5690)	grad_norm 13.3017 (14.6851)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][430/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0326 (0.0311)	loss 3.4239 (3.5712)	grad_norm 14.6826 (14.6708)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][440/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0319 (0.0312)	loss 3.8214 (3.5735)	grad_norm 14.6949 (14.6676)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][450/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0329 (0.0312)	loss 3.6363 (3.5763)	grad_norm 13.1767 (14.6491)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][460/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0323 (0.0312)	loss 3.5578 (3.5768)	grad_norm 15.7521 (14.6349)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][470/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0334 (0.0313)	loss 3.5429 (3.5758)	grad_norm 11.6289 (14.6194)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][480/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0340 (0.0314)	loss 3.3886 (3.5742)	grad_norm 14.1325 (14.6175)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][490/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0349 (0.0314)	loss 3.5808 (3.5741)	grad_norm 12.6229 (14.6181)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][500/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0316 (0.0315)	loss 3.6446 (3.5749)	grad_norm 13.4286 (14.6043)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][510/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0324 (0.0315)	loss 3.3225 (3.5740)	grad_norm 15.2164 (14.5933)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][520/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0325 (0.0315)	loss 3.6190 (3.5724)	grad_norm 11.8289 (14.5758)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][530/6300]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.0322 (0.0315)	loss 3.4797 (3.5692)	grad_norm 14.6488 (14.5666)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][540/6300]	eta 0:03:01 lr 0.000008	 wd 0.0500	time 0.0298 (0.0315)	loss 3.4395 (3.5662)	grad_norm 12.9310 (14.5544)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][550/6300]	eta 0:03:01 lr 0.000008	 wd 0.0500	time 0.0314 (0.0315)	loss 3.5618 (3.5631)	grad_norm 14.8965 (14.5385)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][560/6300]	eta 0:03:00 lr 0.000008	 wd 0.0500	time 0.0290 (0.0315)	loss 4.0420 (3.5591)	grad_norm 14.9338 (14.5261)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][570/6300]	eta 0:03:00 lr 0.000008	 wd 0.0500	time 0.0276 (0.0315)	loss 3.8951 (3.5647)	grad_norm 13.7387 (14.5164)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][580/6300]	eta 0:02:59 lr 0.000008	 wd 0.0500	time 0.0298 (0.0314)	loss 4.0381 (3.5719)	grad_norm 13.8008 (14.5020)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][590/6300]	eta 0:02:59 lr 0.000008	 wd 0.0500	time 0.0307 (0.0314)	loss 3.9374 (3.5769)	grad_norm 12.5469 (14.4838)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][600/6300]	eta 0:02:59 lr 0.000008	 wd 0.0500	time 0.0317 (0.0314)	loss 3.9495 (3.5818)	grad_norm 13.5198 (14.4745)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][610/6300]	eta 0:02:58 lr 0.000008	 wd 0.0500	time 0.0319 (0.0314)	loss 3.7465 (3.5862)	grad_norm 13.6880 (14.4576)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][620/6300]	eta 0:02:58 lr 0.000008	 wd 0.0500	time 0.0328 (0.0314)	loss 3.7122 (3.5889)	grad_norm 13.7202 (14.4494)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][630/6300]	eta 0:02:58 lr 0.000008	 wd 0.0500	time 0.0315 (0.0314)	loss 3.4980 (3.5919)	grad_norm 13.9152 (14.4472)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][640/6300]	eta 0:02:57 lr 0.000008	 wd 0.0500	time 0.0313 (0.0314)	loss 3.7245 (3.5943)	grad_norm 11.6654 (14.4353)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][650/6300]	eta 0:02:57 lr 0.000008	 wd 0.0500	time 0.0309 (0.0314)	loss 3.6764 (3.5960)	grad_norm 12.1497 (14.4101)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][660/6300]	eta 0:02:57 lr 0.000008	 wd 0.0500	time 0.0306 (0.0314)	loss 3.5170 (3.5969)	grad_norm 13.3665 (14.4017)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][670/6300]	eta 0:02:56 lr 0.000008	 wd 0.0500	time 0.0322 (0.0314)	loss 3.8308 (3.5982)	grad_norm 14.1632 (14.3877)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][680/6300]	eta 0:02:56 lr 0.000008	 wd 0.0500	time 0.0304 (0.0314)	loss 3.6451 (3.5992)	grad_norm 12.4782 (14.3808)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][690/6300]	eta 0:02:56 lr 0.000008	 wd 0.0500	time 0.0333 (0.0315)	loss 3.5907 (3.5989)	grad_norm 14.5196 (14.3698)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][700/6300]	eta 0:02:56 lr 0.000008	 wd 0.0500	time 0.0316 (0.0315)	loss 4.1462 (3.5975)	grad_norm 12.1061 (14.3584)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][710/6300]	eta 0:02:56 lr 0.000008	 wd 0.0500	time 0.0332 (0.0315)	loss 4.0425 (3.6006)	grad_norm 11.6669 (14.3496)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][720/6300]	eta 0:02:56 lr 0.000008	 wd 0.0500	time 0.0313 (0.0316)	loss 3.8094 (3.6045)	grad_norm 13.6750 (14.3388)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][730/6300]	eta 0:02:55 lr 0.000008	 wd 0.0500	time 0.0307 (0.0316)	loss 3.8046 (3.6074)	grad_norm 12.8586 (14.3306)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][740/6300]	eta 0:02:55 lr 0.000008	 wd 0.0500	time 0.0305 (0.0315)	loss 3.8066 (3.6115)	grad_norm 15.5839 (14.3273)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][750/6300]	eta 0:02:54 lr 0.000008	 wd 0.0500	time 0.0303 (0.0315)	loss 3.8042 (3.6150)	grad_norm 11.8514 (14.3170)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][760/6300]	eta 0:02:54 lr 0.000008	 wd 0.0500	time 0.0319 (0.0315)	loss 3.5978 (3.6166)	grad_norm 15.2582 (14.3200)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][770/6300]	eta 0:02:54 lr 0.000008	 wd 0.0500	time 0.0282 (0.0315)	loss 3.7917 (3.6173)	grad_norm 12.8102 (14.3101)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][780/6300]	eta 0:02:53 lr 0.000008	 wd 0.0500	time 0.0297 (0.0315)	loss 3.5705 (3.6182)	grad_norm 15.0829 (14.2993)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][790/6300]	eta 0:02:53 lr 0.000008	 wd 0.0500	time 0.0290 (0.0314)	loss 3.6637 (3.6181)	grad_norm 10.9135 (14.2900)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][800/6300]	eta 0:02:52 lr 0.000008	 wd 0.0500	time 0.0290 (0.0314)	loss 3.8293 (3.6200)	grad_norm 14.4984 (14.2818)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][810/6300]	eta 0:02:52 lr 0.000008	 wd 0.0500	time 0.0306 (0.0314)	loss 3.5032 (3.6200)	grad_norm 14.5602 (14.2789)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][820/6300]	eta 0:02:52 lr 0.000008	 wd 0.0500	time 0.0306 (0.0314)	loss 3.4457 (3.6196)	grad_norm 11.5999 (14.2762)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][830/6300]	eta 0:02:51 lr 0.000008	 wd 0.0500	time 0.0287 (0.0314)	loss 3.5219 (3.6183)	grad_norm 14.2264 (14.2683)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][840/6300]	eta 0:02:51 lr 0.000008	 wd 0.0500	time 0.0316 (0.0314)	loss 3.8511 (3.6179)	grad_norm 11.3067 (14.2590)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][850/6300]	eta 0:02:50 lr 0.000008	 wd 0.0500	time 0.0284 (0.0314)	loss 3.5323 (3.6190)	grad_norm 17.2969 (14.2752)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][860/6300]	eta 0:02:50 lr 0.000008	 wd 0.0500	time 0.0286 (0.0313)	loss 3.5995 (3.6194)	grad_norm 15.7780 (14.2954)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][870/6300]	eta 0:02:50 lr 0.000008	 wd 0.0500	time 0.0311 (0.0313)	loss 3.6456 (3.6199)	grad_norm 15.8404 (14.3112)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][880/6300]	eta 0:02:49 lr 0.000008	 wd 0.0500	time 0.0319 (0.0313)	loss 3.6479 (3.6201)	grad_norm 16.0328 (14.3247)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][890/6300]	eta 0:02:49 lr 0.000008	 wd 0.0500	time 0.0308 (0.0313)	loss 3.6872 (3.6202)	grad_norm 14.1121 (14.3295)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][900/6300]	eta 0:02:49 lr 0.000008	 wd 0.0500	time 0.0296 (0.0313)	loss 3.4970 (3.6205)	grad_norm 17.5351 (14.3398)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][910/6300]	eta 0:02:48 lr 0.000008	 wd 0.0500	time 0.0305 (0.0313)	loss 3.4196 (3.6195)	grad_norm 16.4555 (14.3559)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][920/6300]	eta 0:02:48 lr 0.000008	 wd 0.0500	time 0.0306 (0.0313)	loss 3.6433 (3.6174)	grad_norm 14.1287 (14.3726)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][930/6300]	eta 0:02:47 lr 0.000008	 wd 0.0500	time 0.0284 (0.0313)	loss 3.4326 (3.6157)	grad_norm 15.6816 (14.3850)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][940/6300]	eta 0:02:47 lr 0.000008	 wd 0.0500	time 0.0310 (0.0313)	loss 3.5418 (3.6131)	grad_norm 13.7712 (14.3929)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][950/6300]	eta 0:02:47 lr 0.000008	 wd 0.0500	time 0.0316 (0.0313)	loss 3.3203 (3.6101)	grad_norm 17.0756 (14.4080)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][960/6300]	eta 0:02:47 lr 0.000008	 wd 0.0500	time 0.0320 (0.0313)	loss 3.5250 (3.6074)	grad_norm 14.4724 (14.4212)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][970/6300]	eta 0:02:46 lr 0.000008	 wd 0.0500	time 0.0325 (0.0313)	loss 3.3993 (3.6042)	grad_norm 14.7151 (14.4327)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][980/6300]	eta 0:02:46 lr 0.000008	 wd 0.0500	time 0.0289 (0.0313)	loss 3.8789 (3.6012)	grad_norm 14.6350 (14.4428)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][990/6300]	eta 0:02:46 lr 0.000008	 wd 0.0500	time 0.0300 (0.0313)	loss 4.3650 (3.6065)	grad_norm 15.5832 (14.4590)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1000/6300]	eta 0:02:45 lr 0.000008	 wd 0.0500	time 0.0298 (0.0313)	loss 4.0843 (3.6110)	grad_norm 17.0335 (14.4772)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1010/6300]	eta 0:02:45 lr 0.000008	 wd 0.0500	time 0.0291 (0.0313)	loss 4.3585 (3.6153)	grad_norm 17.0126 (14.4840)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1020/6300]	eta 0:02:45 lr 0.000008	 wd 0.0500	time 0.0304 (0.0313)	loss 3.7436 (3.6183)	grad_norm 14.9522 (14.4974)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1030/6300]	eta 0:02:44 lr 0.000008	 wd 0.0500	time 0.0279 (0.0313)	loss 3.6483 (3.6214)	grad_norm 17.7502 (14.5099)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1040/6300]	eta 0:02:44 lr 0.000008	 wd 0.0500	time 0.0309 (0.0312)	loss 4.0105 (3.6241)	grad_norm 17.3351 (14.5266)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1050/6300]	eta 0:02:43 lr 0.000008	 wd 0.0500	time 0.0291 (0.0312)	loss 3.7418 (3.6266)	grad_norm 14.4042 (14.5383)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1060/6300]	eta 0:02:43 lr 0.000008	 wd 0.0500	time 0.0304 (0.0312)	loss 3.6480 (3.6281)	grad_norm 15.6197 (14.5467)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1070/6300]	eta 0:02:43 lr 0.000008	 wd 0.0500	time 0.0309 (0.0312)	loss 3.5631 (3.6298)	grad_norm 13.8551 (14.5577)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1080/6300]	eta 0:02:42 lr 0.000008	 wd 0.0500	time 0.0339 (0.0312)	loss 3.6519 (3.6307)	grad_norm 17.9127 (14.5610)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1090/6300]	eta 0:02:42 lr 0.000008	 wd 0.0500	time 0.0288 (0.0312)	loss 3.7012 (3.6319)	grad_norm 16.8606 (14.5701)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1100/6300]	eta 0:02:42 lr 0.000008	 wd 0.0500	time 0.0297 (0.0312)	loss 3.6778 (3.6326)	grad_norm 15.7815 (14.5798)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1110/6300]	eta 0:02:41 lr 0.000008	 wd 0.0500	time 0.0292 (0.0312)	loss 3.7172 (3.6330)	grad_norm 13.7594 (14.5915)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1120/6300]	eta 0:02:41 lr 0.000008	 wd 0.0500	time 0.0318 (0.0312)	loss 3.6519 (3.6325)	grad_norm 14.2095 (14.5949)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1130/6300]	eta 0:02:41 lr 0.000008	 wd 0.0500	time 0.0300 (0.0312)	loss 3.3749 (3.6317)	grad_norm 15.5678 (14.5997)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1140/6300]	eta 0:02:40 lr 0.000008	 wd 0.0500	time 0.0299 (0.0312)	loss 3.6431 (3.6314)	grad_norm 15.1944 (14.6078)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1150/6300]	eta 0:02:40 lr 0.000008	 wd 0.0500	time 0.0287 (0.0311)	loss 3.4773 (3.6306)	grad_norm 17.0695 (14.6090)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1160/6300]	eta 0:02:39 lr 0.000008	 wd 0.0500	time 0.0284 (0.0311)	loss 3.3261 (3.6291)	grad_norm 14.0251 (14.6150)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1170/6300]	eta 0:02:39 lr 0.000008	 wd 0.0500	time 0.0301 (0.0311)	loss 3.0865 (3.6272)	grad_norm 13.9092 (14.6229)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1180/6300]	eta 0:02:39 lr 0.000008	 wd 0.0500	time 0.0369 (0.0311)	loss 3.4945 (3.6261)	grad_norm 15.1849 (14.6258)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1190/6300]	eta 0:02:39 lr 0.000008	 wd 0.0500	time 0.0297 (0.0311)	loss 3.3840 (3.6249)	grad_norm 16.1009 (14.6338)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1200/6300]	eta 0:02:38 lr 0.000008	 wd 0.0500	time 0.0303 (0.0311)	loss 3.5820 (3.6233)	grad_norm 13.5569 (14.6373)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1210/6300]	eta 0:02:38 lr 0.000008	 wd 0.0500	time 0.0305 (0.0311)	loss 3.0837 (3.6203)	grad_norm 16.3705 (14.6460)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1220/6300]	eta 0:02:38 lr 0.000008	 wd 0.0500	time 0.0280 (0.0311)	loss 3.3377 (3.6175)	grad_norm 13.4518 (14.6459)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1230/6300]	eta 0:02:37 lr 0.000008	 wd 0.0500	time 0.0290 (0.0311)	loss 3.3922 (3.6153)	grad_norm 14.2374 (14.6474)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1240/6300]	eta 0:02:37 lr 0.000008	 wd 0.0500	time 0.0302 (0.0311)	loss 3.2658 (3.6131)	grad_norm 14.7829 (14.6511)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:20:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1250/6300]	eta 0:02:36 lr 0.000008	 wd 0.0500	time 0.0292 (0.0311)	loss 3.3021 (3.6099)	grad_norm 14.8387 (14.6534)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1260/6300]	eta 0:02:36 lr 0.000008	 wd 0.0500	time 0.0286 (0.0311)	loss 4.0968 (3.6081)	grad_norm 13.0973 (14.6522)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1270/6300]	eta 0:02:36 lr 0.000008	 wd 0.0500	time 0.0307 (0.0311)	loss 3.9763 (3.6120)	grad_norm 13.4272 (14.6430)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1280/6300]	eta 0:02:35 lr 0.000008	 wd 0.0500	time 0.0297 (0.0310)	loss 3.8920 (3.6158)	grad_norm 15.1541 (14.6282)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1290/6300]	eta 0:02:35 lr 0.000008	 wd 0.0500	time 0.0284 (0.0310)	loss 3.8888 (3.6193)	grad_norm 13.6008 (14.6188)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1300/6300]	eta 0:02:35 lr 0.000008	 wd 0.0500	time 0.0282 (0.0310)	loss 3.7295 (3.6224)	grad_norm 14.2913 (14.6094)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1310/6300]	eta 0:02:34 lr 0.000008	 wd 0.0500	time 0.0290 (0.0310)	loss 3.9055 (3.6253)	grad_norm 14.5513 (14.6017)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1320/6300]	eta 0:02:34 lr 0.000008	 wd 0.0500	time 0.0290 (0.0310)	loss 3.7366 (3.6266)	grad_norm 11.9552 (14.5918)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1330/6300]	eta 0:02:33 lr 0.000008	 wd 0.0500	time 0.0300 (0.0310)	loss 4.1782 (3.6288)	grad_norm 14.0255 (14.5848)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1340/6300]	eta 0:02:33 lr 0.000008	 wd 0.0500	time 0.0293 (0.0310)	loss 4.0272 (3.6301)	grad_norm 15.3072 (14.5798)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1350/6300]	eta 0:02:33 lr 0.000008	 wd 0.0500	time 0.0296 (0.0310)	loss 3.7627 (3.6323)	grad_norm 12.5722 (14.5721)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1360/6300]	eta 0:02:32 lr 0.000008	 wd 0.0500	time 0.0282 (0.0309)	loss 3.7433 (3.6339)	grad_norm 14.4064 (14.5641)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1370/6300]	eta 0:02:32 lr 0.000008	 wd 0.0500	time 0.0279 (0.0309)	loss 3.8188 (3.6351)	grad_norm 13.0985 (14.5505)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1380/6300]	eta 0:02:32 lr 0.000008	 wd 0.0500	time 0.0314 (0.0309)	loss 3.5950 (3.6357)	grad_norm 12.2096 (14.5393)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1390/6300]	eta 0:02:31 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.8191 (3.6367)	grad_norm 12.9644 (14.5297)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1400/6300]	eta 0:02:31 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 4.0746 (3.6374)	grad_norm 17.3812 (14.5246)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1410/6300]	eta 0:02:31 lr 0.000008	 wd 0.0500	time 0.0289 (0.0309)	loss 4.0127 (3.6399)	grad_norm 14.2941 (14.5327)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1420/6300]	eta 0:02:30 lr 0.000008	 wd 0.0500	time 0.0287 (0.0309)	loss 4.3259 (3.6430)	grad_norm 19.3900 (14.5469)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1430/6300]	eta 0:02:30 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.7450 (3.6451)	grad_norm 15.0094 (14.5541)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1440/6300]	eta 0:02:29 lr 0.000008	 wd 0.0500	time 0.0306 (0.0309)	loss 4.0580 (3.6469)	grad_norm 15.5225 (14.5639)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1450/6300]	eta 0:02:29 lr 0.000008	 wd 0.0500	time 0.0296 (0.0308)	loss 3.9867 (3.6485)	grad_norm 15.3647 (14.5706)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1460/6300]	eta 0:02:29 lr 0.000008	 wd 0.0500	time 0.0281 (0.0308)	loss 3.5448 (3.6493)	grad_norm 14.8701 (14.5803)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1470/6300]	eta 0:02:28 lr 0.000008	 wd 0.0500	time 0.0303 (0.0308)	loss 3.7829 (3.6503)	grad_norm 16.1814 (14.5854)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1480/6300]	eta 0:02:28 lr 0.000008	 wd 0.0500	time 0.0335 (0.0308)	loss 3.9426 (3.6509)	grad_norm 16.4237 (14.5967)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1490/6300]	eta 0:02:28 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 3.7910 (3.6516)	grad_norm 16.1146 (14.6070)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1500/6300]	eta 0:02:27 lr 0.000008	 wd 0.0500	time 0.0297 (0.0308)	loss 3.7295 (3.6517)	grad_norm 17.5815 (14.6160)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1510/6300]	eta 0:02:27 lr 0.000008	 wd 0.0500	time 0.0315 (0.0308)	loss 3.6642 (3.6515)	grad_norm 15.1181 (14.6261)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1520/6300]	eta 0:02:27 lr 0.000008	 wd 0.0500	time 0.0328 (0.0308)	loss 3.6030 (3.6508)	grad_norm 15.9120 (14.6337)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1530/6300]	eta 0:02:27 lr 0.000008	 wd 0.0500	time 0.0343 (0.0308)	loss 3.5313 (3.6505)	grad_norm 16.8740 (14.6397)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1540/6300]	eta 0:02:26 lr 0.000008	 wd 0.0500	time 0.0321 (0.0308)	loss 3.5092 (3.6495)	grad_norm 18.6710 (14.6488)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1550/6300]	eta 0:02:26 lr 0.000008	 wd 0.0500	time 0.0333 (0.0308)	loss 3.6517 (3.6502)	grad_norm 15.8834 (14.6587)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1560/6300]	eta 0:02:26 lr 0.000008	 wd 0.0500	time 0.0332 (0.0309)	loss 3.6770 (3.6508)	grad_norm 16.5997 (14.6701)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1570/6300]	eta 0:02:26 lr 0.000008	 wd 0.0500	time 0.0334 (0.0309)	loss 3.4872 (3.6510)	grad_norm 16.0006 (14.6796)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1580/6300]	eta 0:02:25 lr 0.000008	 wd 0.0500	time 0.0299 (0.0309)	loss 3.3687 (3.6508)	grad_norm 17.4676 (14.6926)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1590/6300]	eta 0:02:25 lr 0.000008	 wd 0.0500	time 0.0283 (0.0309)	loss 3.8642 (3.6506)	grad_norm 17.5161 (14.7008)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1600/6300]	eta 0:02:25 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 3.5399 (3.6497)	grad_norm 15.5964 (14.7138)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1610/6300]	eta 0:02:24 lr 0.000008	 wd 0.0500	time 0.0289 (0.0309)	loss 3.4468 (3.6486)	grad_norm 17.3802 (14.7226)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1620/6300]	eta 0:02:24 lr 0.000008	 wd 0.0500	time 0.0287 (0.0308)	loss 3.3181 (3.6474)	grad_norm 18.2383 (14.7393)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1630/6300]	eta 0:02:24 lr 0.000008	 wd 0.0500	time 0.0299 (0.0308)	loss 3.2554 (3.6461)	grad_norm 15.4347 (14.7488)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1640/6300]	eta 0:02:23 lr 0.000008	 wd 0.0500	time 0.0299 (0.0308)	loss 3.4160 (3.6447)	grad_norm 17.1648 (14.7606)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1650/6300]	eta 0:02:23 lr 0.000008	 wd 0.0500	time 0.0284 (0.0308)	loss 3.4055 (3.6433)	grad_norm 15.0898 (14.7730)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1660/6300]	eta 0:02:22 lr 0.000008	 wd 0.0500	time 0.0293 (0.0308)	loss 3.0299 (3.6411)	grad_norm 17.9888 (14.7847)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1670/6300]	eta 0:02:22 lr 0.000008	 wd 0.0500	time 0.0330 (0.0308)	loss 3.0783 (3.6386)	grad_norm 20.5026 (14.7986)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1680/6300]	eta 0:02:22 lr 0.000008	 wd 0.0500	time 0.0292 (0.0308)	loss 4.1555 (3.6367)	grad_norm 12.7514 (14.8110)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1690/6300]	eta 0:02:22 lr 0.000008	 wd 0.0500	time 0.0308 (0.0308)	loss 3.7327 (3.6377)	grad_norm 14.5409 (14.8041)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1700/6300]	eta 0:02:21 lr 0.000008	 wd 0.0500	time 0.0293 (0.0308)	loss 3.5849 (3.6391)	grad_norm 13.0799 (14.7984)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1710/6300]	eta 0:02:21 lr 0.000008	 wd 0.0500	time 0.0300 (0.0308)	loss 3.7307 (3.6402)	grad_norm 13.6440 (14.7890)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1720/6300]	eta 0:02:21 lr 0.000008	 wd 0.0500	time 0.0301 (0.0308)	loss 3.6884 (3.6409)	grad_norm 13.5897 (14.7778)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1730/6300]	eta 0:02:20 lr 0.000008	 wd 0.0500	time 0.0292 (0.0308)	loss 3.6620 (3.6412)	grad_norm 13.3971 (14.7673)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1740/6300]	eta 0:02:20 lr 0.000008	 wd 0.0500	time 0.0284 (0.0308)	loss 4.0182 (3.6417)	grad_norm 13.4189 (14.7610)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1750/6300]	eta 0:02:20 lr 0.000008	 wd 0.0500	time 0.0370 (0.0308)	loss 3.5187 (3.6419)	grad_norm 12.8198 (14.7522)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1760/6300]	eta 0:02:19 lr 0.000008	 wd 0.0500	time 0.0337 (0.0308)	loss 3.4571 (3.6418)	grad_norm 12.9444 (14.7439)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1770/6300]	eta 0:02:19 lr 0.000008	 wd 0.0500	time 0.0375 (0.0308)	loss 3.5875 (3.6419)	grad_norm 13.2699 (14.7386)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1780/6300]	eta 0:02:19 lr 0.000008	 wd 0.0500	time 0.0334 (0.0309)	loss 3.6574 (3.6416)	grad_norm 14.1494 (14.7340)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1790/6300]	eta 0:02:19 lr 0.000008	 wd 0.0500	time 0.0347 (0.0309)	loss 3.6103 (3.6414)	grad_norm 14.4362 (14.7284)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1800/6300]	eta 0:02:18 lr 0.000008	 wd 0.0500	time 0.0353 (0.0309)	loss 3.6831 (3.6412)	grad_norm 14.2212 (14.7210)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1810/6300]	eta 0:02:18 lr 0.000008	 wd 0.0500	time 0.0356 (0.0309)	loss 3.5649 (3.6408)	grad_norm 12.6898 (14.7144)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1820/6300]	eta 0:02:18 lr 0.000008	 wd 0.0500	time 0.0342 (0.0309)	loss 3.8504 (3.6401)	grad_norm 14.4434 (14.7064)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1830/6300]	eta 0:02:18 lr 0.000008	 wd 0.0500	time 0.0335 (0.0310)	loss 3.6243 (3.6410)	grad_norm 13.1699 (14.6989)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1840/6300]	eta 0:02:18 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 3.4806 (3.6415)	grad_norm 13.9165 (14.6888)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:18 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1850/6300]	eta 0:02:17 lr 0.000008	 wd 0.0500	time 0.0285 (0.0309)	loss 3.7276 (3.6418)	grad_norm 12.6083 (14.6820)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:18 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1860/6300]	eta 0:02:17 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.6083 (3.6415)	grad_norm 12.5987 (14.6700)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:18 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1870/6300]	eta 0:02:17 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.6775 (3.6412)	grad_norm 14.7103 (14.6632)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1880/6300]	eta 0:02:16 lr 0.000008	 wd 0.0500	time 0.0289 (0.0309)	loss 3.4534 (3.6406)	grad_norm 12.9630 (14.6534)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1890/6300]	eta 0:02:16 lr 0.000008	 wd 0.0500	time 0.0310 (0.0309)	loss 3.5606 (3.6401)	grad_norm 13.1004 (14.6418)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1900/6300]	eta 0:02:16 lr 0.000008	 wd 0.0500	time 0.0305 (0.0309)	loss 3.6453 (3.6393)	grad_norm 12.8207 (14.6327)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1910/6300]	eta 0:02:15 lr 0.000008	 wd 0.0500	time 0.0326 (0.0309)	loss 3.5310 (3.6385)	grad_norm 13.9739 (14.6221)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1920/6300]	eta 0:02:15 lr 0.000008	 wd 0.0500	time 0.0295 (0.0309)	loss 3.3399 (3.6372)	grad_norm 12.4915 (14.6156)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1930/6300]	eta 0:02:15 lr 0.000008	 wd 0.0500	time 0.0331 (0.0310)	loss 3.5326 (3.6359)	grad_norm 11.3535 (14.6094)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1940/6300]	eta 0:02:14 lr 0.000008	 wd 0.0500	time 0.0338 (0.0310)	loss 3.3505 (3.6345)	grad_norm 13.0489 (14.5995)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1950/6300]	eta 0:02:14 lr 0.000008	 wd 0.0500	time 0.0301 (0.0310)	loss 3.3228 (3.6326)	grad_norm 15.9327 (14.5952)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1960/6300]	eta 0:02:14 lr 0.000008	 wd 0.0500	time 0.0280 (0.0310)	loss 4.0672 (3.6312)	grad_norm 15.5558 (14.5909)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1970/6300]	eta 0:02:13 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 4.5043 (3.6338)	grad_norm 16.1948 (14.5935)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1980/6300]	eta 0:02:13 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.9921 (3.6354)	grad_norm 13.8893 (14.5887)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][1990/6300]	eta 0:02:13 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 3.9702 (3.6374)	grad_norm 13.1204 (14.5880)	loss_scale 65536.0000 (65536.0000)	mem 286MB
[2022-11-04 20:21:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2000/6300]	eta 0:02:13 lr 0.000008	 wd 0.0500	time 0.0301 (0.0309)	loss 4.0487 (3.6391)	grad_norm 16.1820 (14.5897)	loss_scale 131072.0000 (65601.5032)	mem 286MB
[2022-11-04 20:21:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2010/6300]	eta 0:02:12 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.8720 (3.6406)	grad_norm 13.5289 (14.5909)	loss_scale 131072.0000 (65927.0651)	mem 286MB
[2022-11-04 20:21:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2020/6300]	eta 0:02:12 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 3.8176 (3.6420)	grad_norm 13.2041 (14.5876)	loss_scale 131072.0000 (66249.4052)	mem 286MB
[2022-11-04 20:21:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2030/6300]	eta 0:02:12 lr 0.000008	 wd 0.0500	time 0.0283 (0.0309)	loss 3.9117 (3.6429)	grad_norm 14.9929 (14.5854)	loss_scale 131072.0000 (66568.5711)	mem 286MB
[2022-11-04 20:21:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2040/6300]	eta 0:02:11 lr 0.000008	 wd 0.0500	time 0.0315 (0.0309)	loss 3.8874 (3.6441)	grad_norm 12.4465 (14.5821)	loss_scale 131072.0000 (66884.6095)	mem 286MB
[2022-11-04 20:21:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2050/6300]	eta 0:02:11 lr 0.000008	 wd 0.0500	time 0.0292 (0.0309)	loss 3.9167 (3.6455)	grad_norm 11.7958 (14.5802)	loss_scale 131072.0000 (67197.5661)	mem 286MB
[2022-11-04 20:21:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2060/6300]	eta 0:02:11 lr 0.000008	 wd 0.0500	time 0.0286 (0.0309)	loss 3.6863 (3.6462)	grad_norm 15.6460 (14.5783)	loss_scale 131072.0000 (67507.4857)	mem 286MB
[2022-11-04 20:21:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2070/6300]	eta 0:02:10 lr 0.000008	 wd 0.0500	time 0.0284 (0.0309)	loss 3.9742 (3.6468)	grad_norm 13.6111 (14.5723)	loss_scale 131072.0000 (67814.4124)	mem 286MB
[2022-11-04 20:21:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2080/6300]	eta 0:02:10 lr 0.000008	 wd 0.0500	time 0.0347 (0.0309)	loss 3.6214 (3.6473)	grad_norm 14.3629 (14.5721)	loss_scale 131072.0000 (68118.3892)	mem 286MB
[2022-11-04 20:21:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2090/6300]	eta 0:02:10 lr 0.000008	 wd 0.0500	time 0.0341 (0.0309)	loss 3.8259 (3.6476)	grad_norm 15.6117 (14.5728)	loss_scale 131072.0000 (68419.4586)	mem 286MB
[2022-11-04 20:21:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2100/6300]	eta 0:02:09 lr 0.000008	 wd 0.0500	time 0.0292 (0.0309)	loss 3.9963 (3.6479)	grad_norm 12.9595 (14.5694)	loss_scale 131072.0000 (68717.6621)	mem 286MB
[2022-11-04 20:21:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2110/6300]	eta 0:02:09 lr 0.000008	 wd 0.0500	time 0.0287 (0.0309)	loss 4.1697 (3.6500)	grad_norm 17.4156 (14.5728)	loss_scale 131072.0000 (69013.0403)	mem 286MB
[2022-11-04 20:21:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2120/6300]	eta 0:02:09 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 4.1319 (3.6525)	grad_norm 16.6952 (14.5768)	loss_scale 131072.0000 (69305.6332)	mem 286MB
[2022-11-04 20:21:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2130/6300]	eta 0:02:08 lr 0.000008	 wd 0.0500	time 0.0314 (0.0309)	loss 4.1860 (3.6546)	grad_norm 15.3317 (14.5828)	loss_scale 131072.0000 (69595.4801)	mem 286MB
[2022-11-04 20:21:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2140/6300]	eta 0:02:08 lr 0.000008	 wd 0.0500	time 0.0295 (0.0309)	loss 4.1756 (3.6568)	grad_norm 14.9978 (14.5836)	loss_scale 131072.0000 (69882.6193)	mem 286MB
[2022-11-04 20:21:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2150/6300]	eta 0:02:08 lr 0.000008	 wd 0.0500	time 0.0300 (0.0309)	loss 3.7531 (3.6586)	grad_norm 11.8721 (14.5868)	loss_scale 131072.0000 (70167.0888)	mem 286MB
[2022-11-04 20:21:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2160/6300]	eta 0:02:07 lr 0.000008	 wd 0.0500	time 0.0317 (0.0309)	loss 3.9519 (3.6597)	grad_norm 12.6913 (14.5849)	loss_scale 131072.0000 (70448.9255)	mem 286MB
[2022-11-04 20:21:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2170/6300]	eta 0:02:07 lr 0.000008	 wd 0.0500	time 0.0334 (0.0309)	loss 3.6174 (3.6612)	grad_norm 15.8418 (14.5833)	loss_scale 131072.0000 (70728.1658)	mem 286MB
[2022-11-04 20:21:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2180/6300]	eta 0:02:07 lr 0.000008	 wd 0.0500	time 0.0343 (0.0309)	loss 4.2419 (3.6623)	grad_norm 15.5486 (14.5844)	loss_scale 131072.0000 (71004.8455)	mem 286MB
[2022-11-04 20:21:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2190/6300]	eta 0:02:06 lr 0.000008	 wd 0.0500	time 0.0317 (0.0309)	loss 3.7463 (3.6637)	grad_norm 15.4048 (14.5865)	loss_scale 131072.0000 (71278.9995)	mem 286MB
[2022-11-04 20:21:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2200/6300]	eta 0:02:06 lr 0.000008	 wd 0.0500	time 0.0306 (0.0309)	loss 3.9602 (3.6647)	grad_norm 16.9858 (14.5887)	loss_scale 131072.0000 (71550.6624)	mem 286MB
[2022-11-04 20:21:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2210/6300]	eta 0:02:06 lr 0.000008	 wd 0.0500	time 0.0304 (0.0309)	loss 3.6680 (3.6653)	grad_norm 13.0447 (14.5911)	loss_scale 131072.0000 (71819.8679)	mem 286MB
[2022-11-04 20:21:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2220/6300]	eta 0:02:05 lr 0.000008	 wd 0.0500	time 0.0291 (0.0309)	loss 4.0163 (3.6662)	grad_norm 12.8638 (14.5888)	loss_scale 131072.0000 (72086.6493)	mem 286MB
[2022-11-04 20:21:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2230/6300]	eta 0:02:05 lr 0.000008	 wd 0.0500	time 0.0292 (0.0309)	loss 3.4138 (3.6663)	grad_norm 13.5983 (14.5887)	loss_scale 131072.0000 (72351.0390)	mem 286MB
[2022-11-04 20:21:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2240/6300]	eta 0:02:05 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 3.9677 (3.6670)	grad_norm 16.4193 (14.5892)	loss_scale 131072.0000 (72613.0692)	mem 286MB
[2022-11-04 20:21:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2250/6300]	eta 0:02:04 lr 0.000008	 wd 0.0500	time 0.0287 (0.0309)	loss 4.6081 (3.6701)	grad_norm 16.4259 (14.5932)	loss_scale 131072.0000 (72872.7712)	mem 286MB
[2022-11-04 20:21:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2260/6300]	eta 0:02:04 lr 0.000008	 wd 0.0500	time 0.0288 (0.0309)	loss 4.2973 (3.6728)	grad_norm 15.0729 (14.5960)	loss_scale 131072.0000 (73130.1760)	mem 286MB
[2022-11-04 20:21:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2270/6300]	eta 0:02:04 lr 0.000008	 wd 0.0500	time 0.0289 (0.0309)	loss 4.1562 (3.6755)	grad_norm 15.7605 (14.6033)	loss_scale 131072.0000 (73385.3140)	mem 286MB
[2022-11-04 20:21:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2280/6300]	eta 0:02:03 lr 0.000008	 wd 0.0500	time 0.0301 (0.0308)	loss 4.3495 (3.6781)	grad_norm 16.6446 (14.6104)	loss_scale 131072.0000 (73638.2148)	mem 286MB
[2022-11-04 20:21:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2290/6300]	eta 0:02:03 lr 0.000008	 wd 0.0500	time 0.0308 (0.0308)	loss 4.4363 (3.6803)	grad_norm 15.7695 (14.6153)	loss_scale 131072.0000 (73888.9079)	mem 286MB
[2022-11-04 20:21:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2300/6300]	eta 0:02:03 lr 0.000008	 wd 0.0500	time 0.0302 (0.0308)	loss 4.1556 (3.6824)	grad_norm 13.9215 (14.6171)	loss_scale 131072.0000 (74137.4220)	mem 286MB
[2022-11-04 20:21:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2310/6300]	eta 0:02:03 lr 0.000008	 wd 0.0500	time 0.0289 (0.0308)	loss 4.0195 (3.6845)	grad_norm 13.8254 (14.6207)	loss_scale 131072.0000 (74383.7854)	mem 286MB
[2022-11-04 20:21:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2320/6300]	eta 0:02:02 lr 0.000008	 wd 0.0500	time 0.0294 (0.0308)	loss 4.4607 (3.6865)	grad_norm 17.9527 (14.6288)	loss_scale 131072.0000 (74628.0259)	mem 286MB
[2022-11-04 20:21:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2330/6300]	eta 0:02:02 lr 0.000008	 wd 0.0500	time 0.0291 (0.0308)	loss 4.1121 (3.6882)	grad_norm 14.4333 (14.6336)	loss_scale 131072.0000 (74870.1707)	mem 286MB
[2022-11-04 20:21:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2340/6300]	eta 0:02:02 lr 0.000008	 wd 0.0500	time 0.0289 (0.0308)	loss 3.8871 (3.6897)	grad_norm 19.0145 (14.6392)	loss_scale 131072.0000 (75110.2469)	mem 286MB
[2022-11-04 20:21:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2350/6300]	eta 0:02:01 lr 0.000008	 wd 0.0500	time 0.0287 (0.0308)	loss 3.9446 (3.6910)	grad_norm 15.9458 (14.6426)	loss_scale 131072.0000 (75348.2807)	mem 286MB
[2022-11-04 20:21:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2360/6300]	eta 0:02:01 lr 0.000008	 wd 0.0500	time 0.0309 (0.0308)	loss 4.1169 (3.6917)	grad_norm 17.4034 (14.6469)	loss_scale 131072.0000 (75584.2982)	mem 286MB
[2022-11-04 20:21:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2370/6300]	eta 0:02:01 lr 0.000008	 wd 0.0500	time 0.0286 (0.0308)	loss 3.7764 (3.6923)	grad_norm 14.0057 (14.6508)	loss_scale 131072.0000 (75818.3248)	mem 286MB
[2022-11-04 20:21:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2380/6300]	eta 0:02:00 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 4.0288 (3.6931)	grad_norm 13.0728 (14.6555)	loss_scale 131072.0000 (76050.3856)	mem 286MB
[2022-11-04 20:21:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2390/6300]	eta 0:02:00 lr 0.000008	 wd 0.0500	time 0.0287 (0.0308)	loss 3.9704 (3.6946)	grad_norm 15.3410 (14.6614)	loss_scale 131072.0000 (76280.5052)	mem 286MB
[2022-11-04 20:21:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2400/6300]	eta 0:02:00 lr 0.000008	 wd 0.0500	time 0.0290 (0.0308)	loss 3.8166 (3.6960)	grad_norm 16.7338 (14.6643)	loss_scale 131072.0000 (76508.7080)	mem 286MB
[2022-11-04 20:21:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2410/6300]	eta 0:01:59 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 4.0027 (3.6977)	grad_norm 16.3592 (14.6694)	loss_scale 131072.0000 (76735.0178)	mem 286MB
[2022-11-04 20:21:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2420/6300]	eta 0:01:59 lr 0.000008	 wd 0.0500	time 0.0306 (0.0308)	loss 3.8102 (3.6991)	grad_norm 14.9900 (14.6729)	loss_scale 131072.0000 (76959.4581)	mem 286MB
[2022-11-04 20:21:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2430/6300]	eta 0:01:59 lr 0.000008	 wd 0.0500	time 0.0290 (0.0308)	loss 3.8730 (3.7003)	grad_norm 15.3147 (14.6791)	loss_scale 131072.0000 (77182.0518)	mem 286MB
[2022-11-04 20:21:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2440/6300]	eta 0:01:58 lr 0.000008	 wd 0.0500	time 0.0313 (0.0308)	loss 3.9554 (3.7012)	grad_norm 15.3593 (14.6793)	loss_scale 131072.0000 (77402.8218)	mem 286MB
[2022-11-04 20:21:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2450/6300]	eta 0:01:58 lr 0.000008	 wd 0.0500	time 0.0297 (0.0308)	loss 3.9536 (3.7020)	grad_norm 14.0464 (14.6820)	loss_scale 131072.0000 (77621.7903)	mem 286MB
[2022-11-04 20:21:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2460/6300]	eta 0:01:58 lr 0.000008	 wd 0.0500	time 0.0336 (0.0308)	loss 3.8554 (3.7028)	grad_norm 16.4371 (14.6862)	loss_scale 131072.0000 (77838.9793)	mem 286MB
[2022-11-04 20:21:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2470/6300]	eta 0:01:57 lr 0.000008	 wd 0.0500	time 0.0301 (0.0308)	loss 3.9387 (3.7030)	grad_norm 15.5738 (14.6881)	loss_scale 131072.0000 (78054.4104)	mem 286MB
[2022-11-04 20:21:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2480/6300]	eta 0:01:57 lr 0.000008	 wd 0.0500	time 0.0291 (0.0308)	loss 3.7148 (3.7030)	grad_norm 13.7056 (14.6912)	loss_scale 131072.0000 (78268.1048)	mem 286MB
[2022-11-04 20:21:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2490/6300]	eta 0:01:57 lr 0.000008	 wd 0.0500	time 0.0313 (0.0308)	loss 3.6491 (3.7032)	grad_norm 15.2739 (14.6967)	loss_scale 131072.0000 (78480.0835)	mem 286MB
[2022-11-04 20:21:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2500/6300]	eta 0:01:56 lr 0.000008	 wd 0.0500	time 0.0307 (0.0308)	loss 4.1235 (3.7036)	grad_norm 15.0884 (14.7016)	loss_scale 131072.0000 (78690.3671)	mem 286MB
[2022-11-04 20:21:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2510/6300]	eta 0:01:56 lr 0.000008	 wd 0.0500	time 0.0294 (0.0308)	loss 3.7536 (3.7036)	grad_norm 15.1217 (14.7018)	loss_scale 131072.0000 (78898.9757)	mem 286MB
[2022-11-04 20:21:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2520/6300]	eta 0:01:56 lr 0.000008	 wd 0.0500	time 0.0311 (0.0308)	loss 4.0679 (3.7034)	grad_norm 15.9926 (14.7078)	loss_scale 131072.0000 (79105.9294)	mem 286MB
[2022-11-04 20:21:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2530/6300]	eta 0:01:56 lr 0.000008	 wd 0.0500	time 0.0299 (0.0308)	loss 4.0269 (3.7050)	grad_norm 16.1273 (14.7110)	loss_scale 131072.0000 (79311.2477)	mem 286MB
[2022-11-04 20:21:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2540/6300]	eta 0:01:55 lr 0.000008	 wd 0.0500	time 0.0303 (0.0308)	loss 3.9200 (3.7062)	grad_norm 16.2897 (14.7128)	loss_scale 131072.0000 (79514.9500)	mem 286MB
[2022-11-04 20:21:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2550/6300]	eta 0:01:55 lr 0.000008	 wd 0.0500	time 0.0290 (0.0308)	loss 3.8576 (3.7076)	grad_norm 15.6259 (14.7146)	loss_scale 131072.0000 (79717.0553)	mem 286MB
[2022-11-04 20:21:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2560/6300]	eta 0:01:55 lr 0.000008	 wd 0.0500	time 0.0286 (0.0308)	loss 4.2492 (3.7093)	grad_norm 15.8002 (14.7168)	loss_scale 131072.0000 (79917.5822)	mem 286MB
[2022-11-04 20:21:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2570/6300]	eta 0:01:54 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 4.0324 (3.7102)	grad_norm 15.6974 (14.7191)	loss_scale 131072.0000 (80116.5492)	mem 286MB
[2022-11-04 20:21:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2580/6300]	eta 0:01:54 lr 0.000008	 wd 0.0500	time 0.0314 (0.0308)	loss 3.7705 (3.7109)	grad_norm 15.8354 (14.7202)	loss_scale 131072.0000 (80313.9744)	mem 286MB
[2022-11-04 20:21:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2590/6300]	eta 0:01:54 lr 0.000008	 wd 0.0500	time 0.0317 (0.0308)	loss 3.7524 (3.7119)	grad_norm 15.4671 (14.7222)	loss_scale 131072.0000 (80509.8757)	mem 286MB
[2022-11-04 20:21:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2600/6300]	eta 0:01:53 lr 0.000008	 wd 0.0500	time 0.0351 (0.0308)	loss 3.6753 (3.7121)	grad_norm 16.0441 (14.7253)	loss_scale 131072.0000 (80704.2707)	mem 286MB
[2022-11-04 20:21:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2610/6300]	eta 0:01:53 lr 0.000008	 wd 0.0500	time 0.0310 (0.0308)	loss 3.8881 (3.7126)	grad_norm 15.1119 (14.7273)	loss_scale 131072.0000 (80897.1766)	mem 286MB
[2022-11-04 20:21:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2620/6300]	eta 0:01:53 lr 0.000008	 wd 0.0500	time 0.0296 (0.0308)	loss 3.5780 (3.7131)	grad_norm 16.2219 (14.7303)	loss_scale 131072.0000 (81088.6105)	mem 286MB
[2022-11-04 20:21:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2630/6300]	eta 0:01:52 lr 0.000008	 wd 0.0500	time 0.0284 (0.0308)	loss 3.8490 (3.7133)	grad_norm 15.1214 (14.7310)	loss_scale 131072.0000 (81278.5891)	mem 286MB
[2022-11-04 20:21:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2640/6300]	eta 0:01:52 lr 0.000008	 wd 0.0500	time 0.0294 (0.0308)	loss 3.6560 (3.7137)	grad_norm 14.4892 (14.7328)	loss_scale 131072.0000 (81467.1291)	mem 286MB
[2022-11-04 20:21:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2650/6300]	eta 0:01:52 lr 0.000008	 wd 0.0500	time 0.0325 (0.0308)	loss 3.8524 (3.7137)	grad_norm 16.1611 (14.7363)	loss_scale 131072.0000 (81654.2467)	mem 286MB
[2022-11-04 20:21:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2660/6300]	eta 0:01:52 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 3.8080 (3.7133)	grad_norm 17.1707 (14.7395)	loss_scale 131072.0000 (81839.9579)	mem 286MB
[2022-11-04 20:21:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2670/6300]	eta 0:01:51 lr 0.000008	 wd 0.0500	time 0.0294 (0.0308)	loss 3.6445 (3.7132)	grad_norm 14.5565 (14.7445)	loss_scale 131072.0000 (82024.2785)	mem 286MB
[2022-11-04 20:21:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2680/6300]	eta 0:01:51 lr 0.000008	 wd 0.0500	time 0.0287 (0.0308)	loss 3.8063 (3.7133)	grad_norm 17.3237 (14.7477)	loss_scale 131072.0000 (82207.2242)	mem 286MB
[2022-11-04 20:21:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2690/6300]	eta 0:01:51 lr 0.000008	 wd 0.0500	time 0.0306 (0.0308)	loss 3.8042 (3.7135)	grad_norm 17.7213 (14.7501)	loss_scale 131072.0000 (82388.8101)	mem 286MB
[2022-11-04 20:21:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2700/6300]	eta 0:01:50 lr 0.000008	 wd 0.0500	time 0.0292 (0.0308)	loss 3.3058 (3.7137)	grad_norm 16.6681 (14.7510)	loss_scale 131072.0000 (82569.0515)	mem 286MB
[2022-11-04 20:21:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2710/6300]	eta 0:01:50 lr 0.000008	 wd 0.0500	time 0.0315 (0.0308)	loss 3.3929 (3.7137)	grad_norm 15.7001 (14.7540)	loss_scale 131072.0000 (82747.9631)	mem 286MB
[2022-11-04 20:21:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2720/6300]	eta 0:01:50 lr 0.000008	 wd 0.0500	time 0.0299 (0.0308)	loss 3.8027 (3.7137)	grad_norm 16.7785 (14.7549)	loss_scale 131072.0000 (82925.5597)	mem 286MB
[2022-11-04 20:21:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2730/6300]	eta 0:01:49 lr 0.000008	 wd 0.0500	time 0.0316 (0.0308)	loss 3.4033 (3.7131)	grad_norm 16.1163 (14.7558)	loss_scale 131072.0000 (83101.8557)	mem 286MB
[2022-11-04 20:21:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2740/6300]	eta 0:01:49 lr 0.000008	 wd 0.0500	time 0.0288 (0.0308)	loss 3.6611 (3.7123)	grad_norm 15.0581 (14.7576)	loss_scale 131072.0000 (83276.8654)	mem 286MB
[2022-11-04 20:21:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2750/6300]	eta 0:01:49 lr 0.000008	 wd 0.0500	time 0.0329 (0.0308)	loss 3.3840 (3.7116)	grad_norm 14.3574 (14.7607)	loss_scale 131072.0000 (83450.6027)	mem 286MB
[2022-11-04 20:21:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2760/6300]	eta 0:01:48 lr 0.000008	 wd 0.0500	time 0.0289 (0.0308)	loss 3.2679 (3.7106)	grad_norm 15.0407 (14.7634)	loss_scale 131072.0000 (83623.0815)	mem 286MB
[2022-11-04 20:21:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2770/6300]	eta 0:01:48 lr 0.000008	 wd 0.0500	time 0.0284 (0.0308)	loss 3.4653 (3.7095)	grad_norm 12.9072 (14.7644)	loss_scale 131072.0000 (83794.3154)	mem 286MB
[2022-11-04 20:21:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2780/6300]	eta 0:01:48 lr 0.000008	 wd 0.0500	time 0.0317 (0.0308)	loss 3.2818 (3.7085)	grad_norm 17.6419 (14.7675)	loss_scale 131072.0000 (83964.3179)	mem 286MB
[2022-11-04 20:21:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2790/6300]	eta 0:01:47 lr 0.000008	 wd 0.0500	time 0.0316 (0.0308)	loss 3.1416 (3.7073)	grad_norm 16.5669 (14.7712)	loss_scale 131072.0000 (84133.1021)	mem 286MB
[2022-11-04 20:21:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2800/6300]	eta 0:01:47 lr 0.000008	 wd 0.0500	time 0.0301 (0.0308)	loss 4.0908 (3.7064)	grad_norm 13.0470 (14.7728)	loss_scale 131072.0000 (84300.6812)	mem 286MB
[2022-11-04 20:21:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2810/6300]	eta 0:01:47 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 4.0716 (3.7076)	grad_norm 14.7680 (14.7641)	loss_scale 131072.0000 (84467.0679)	mem 286MB
[2022-11-04 20:21:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2820/6300]	eta 0:01:47 lr 0.000008	 wd 0.0500	time 0.0292 (0.0308)	loss 3.9827 (3.7086)	grad_norm 11.7227 (14.7557)	loss_scale 131072.0000 (84632.2751)	mem 286MB
[2022-11-04 20:21:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2830/6300]	eta 0:01:46 lr 0.000008	 wd 0.0500	time 0.0290 (0.0308)	loss 3.8769 (3.7095)	grad_norm 12.8485 (14.7497)	loss_scale 131072.0000 (84796.3151)	mem 286MB
[2022-11-04 20:21:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2840/6300]	eta 0:01:46 lr 0.000008	 wd 0.0500	time 0.0299 (0.0308)	loss 3.9592 (3.7102)	grad_norm 13.5101 (14.7434)	loss_scale 131072.0000 (84959.2003)	mem 286MB
[2022-11-04 20:21:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2850/6300]	eta 0:01:46 lr 0.000008	 wd 0.0500	time 0.0294 (0.0307)	loss 3.8488 (3.7109)	grad_norm 10.8364 (14.7366)	loss_scale 131072.0000 (85120.9428)	mem 286MB
[2022-11-04 20:21:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2860/6300]	eta 0:01:45 lr 0.000008	 wd 0.0500	time 0.0289 (0.0307)	loss 3.8521 (3.7118)	grad_norm 13.9006 (14.7306)	loss_scale 131072.0000 (85281.5547)	mem 286MB
[2022-11-04 20:21:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2870/6300]	eta 0:01:45 lr 0.000008	 wd 0.0500	time 0.0296 (0.0307)	loss 3.7254 (3.7125)	grad_norm 12.6347 (14.7224)	loss_scale 131072.0000 (85441.0477)	mem 286MB
[2022-11-04 20:21:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2880/6300]	eta 0:01:45 lr 0.000008	 wd 0.0500	time 0.0341 (0.0307)	loss 3.8556 (3.7129)	grad_norm 10.9525 (14.7162)	loss_scale 131072.0000 (85599.4335)	mem 286MB
[2022-11-04 20:21:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2890/6300]	eta 0:01:44 lr 0.000008	 wd 0.0500	time 0.0343 (0.0308)	loss 3.7740 (3.7132)	grad_norm 12.2606 (14.7069)	loss_scale 131072.0000 (85756.7236)	mem 286MB
[2022-11-04 20:21:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2900/6300]	eta 0:01:44 lr 0.000008	 wd 0.0500	time 0.0365 (0.0308)	loss 3.7783 (3.7134)	grad_norm 13.3002 (14.6999)	loss_scale 131072.0000 (85912.9293)	mem 286MB
[2022-11-04 20:21:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2910/6300]	eta 0:01:44 lr 0.000008	 wd 0.0500	time 0.0360 (0.0308)	loss 3.8531 (3.7138)	grad_norm 14.2003 (14.6931)	loss_scale 131072.0000 (86068.0618)	mem 286MB
[2022-11-04 20:21:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2920/6300]	eta 0:01:44 lr 0.000008	 wd 0.0500	time 0.0335 (0.0308)	loss 3.4507 (3.7138)	grad_norm 13.2991 (14.6865)	loss_scale 131072.0000 (86222.1321)	mem 286MB
[2022-11-04 20:21:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2930/6300]	eta 0:01:43 lr 0.000008	 wd 0.0500	time 0.0298 (0.0308)	loss 3.5996 (3.7137)	grad_norm 14.2514 (14.6801)	loss_scale 131072.0000 (86375.1511)	mem 286MB
[2022-11-04 20:21:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2940/6300]	eta 0:01:43 lr 0.000008	 wd 0.0500	time 0.0292 (0.0308)	loss 3.9663 (3.7136)	grad_norm 15.8027 (14.6734)	loss_scale 131072.0000 (86527.1295)	mem 286MB
[2022-11-04 20:21:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2950/6300]	eta 0:01:43 lr 0.000008	 wd 0.0500	time 0.0290 (0.0308)	loss 3.9254 (3.7141)	grad_norm 13.0523 (14.6684)	loss_scale 131072.0000 (86678.0779)	mem 286MB
[2022-11-04 20:21:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2960/6300]	eta 0:01:42 lr 0.000008	 wd 0.0500	time 0.0297 (0.0308)	loss 3.8557 (3.7146)	grad_norm 12.7292 (14.6642)	loss_scale 131072.0000 (86828.0068)	mem 286MB
[2022-11-04 20:21:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2970/6300]	eta 0:01:42 lr 0.000008	 wd 0.0500	time 0.0298 (0.0308)	loss 3.7226 (3.7150)	grad_norm 15.0057 (14.6603)	loss_scale 131072.0000 (86976.9263)	mem 286MB
[2022-11-04 20:21:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2980/6300]	eta 0:01:42 lr 0.000008	 wd 0.0500	time 0.0297 (0.0308)	loss 3.8044 (3.7151)	grad_norm 10.5905 (14.6561)	loss_scale 131072.0000 (87124.8467)	mem 286MB
[2022-11-04 20:21:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][2990/6300]	eta 0:01:41 lr 0.000008	 wd 0.0500	time 0.0287 (0.0308)	loss 3.7252 (3.7154)	grad_norm 14.2144 (14.6537)	loss_scale 131072.0000 (87271.7780)	mem 286MB
[2022-11-04 20:21:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3000/6300]	eta 0:01:41 lr 0.000008	 wd 0.0500	time 0.0305 (0.0308)	loss 3.5170 (3.7153)	grad_norm 13.6590 (14.6489)	loss_scale 131072.0000 (87417.7301)	mem 286MB
[2022-11-04 20:21:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3010/6300]	eta 0:01:41 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 3.6671 (3.7154)	grad_norm 12.6939 (14.6427)	loss_scale 131072.0000 (87562.7127)	mem 286MB
[2022-11-04 20:21:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3020/6300]	eta 0:01:40 lr 0.000008	 wd 0.0500	time 0.0298 (0.0308)	loss 3.6575 (3.7154)	grad_norm 13.2999 (14.6383)	loss_scale 131072.0000 (87706.7355)	mem 286MB
[2022-11-04 20:21:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3030/6300]	eta 0:01:40 lr 0.000008	 wd 0.0500	time 0.0296 (0.0308)	loss 3.5932 (3.7154)	grad_norm 13.7491 (14.6336)	loss_scale 131072.0000 (87849.8080)	mem 286MB
[2022-11-04 20:21:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3040/6300]	eta 0:01:40 lr 0.000008	 wd 0.0500	time 0.0300 (0.0308)	loss 3.6644 (3.7152)	grad_norm 11.9794 (14.6302)	loss_scale 131072.0000 (87991.9395)	mem 286MB
[2022-11-04 20:21:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3050/6300]	eta 0:01:40 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 3.6133 (3.7148)	grad_norm 13.1950 (14.6259)	loss_scale 131072.0000 (88133.1393)	mem 286MB
[2022-11-04 20:21:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3060/6300]	eta 0:01:39 lr 0.000008	 wd 0.0500	time 0.0285 (0.0308)	loss 3.4774 (3.7144)	grad_norm 12.4930 (14.6203)	loss_scale 131072.0000 (88273.4165)	mem 286MB
[2022-11-04 20:21:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3070/6300]	eta 0:01:39 lr 0.000008	 wd 0.0500	time 0.0301 (0.0308)	loss 3.4649 (3.7138)	grad_norm 14.5961 (14.6155)	loss_scale 131072.0000 (88412.7802)	mem 286MB
[2022-11-04 20:21:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3080/6300]	eta 0:01:39 lr 0.000008	 wd 0.0500	time 0.0302 (0.0308)	loss 3.9890 (3.7133)	grad_norm 13.9525 (14.6103)	loss_scale 131072.0000 (88551.2392)	mem 286MB
[2022-11-04 20:21:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3090/6300]	eta 0:01:38 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 3.9227 (3.7140)	grad_norm 14.2052 (14.6045)	loss_scale 131072.0000 (88688.8023)	mem 286MB
[2022-11-04 20:21:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3100/6300]	eta 0:01:38 lr 0.000008	 wd 0.0500	time 0.0309 (0.0308)	loss 3.7628 (3.7146)	grad_norm 10.9729 (14.5993)	loss_scale 131072.0000 (88825.4782)	mem 286MB
[2022-11-04 20:21:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3110/6300]	eta 0:01:38 lr 0.000008	 wd 0.0500	time 0.0300 (0.0308)	loss 3.8616 (3.7149)	grad_norm 9.2041 (14.5887)	loss_scale 131072.0000 (88961.2755)	mem 286MB
[2022-11-04 20:21:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3120/6300]	eta 0:01:37 lr 0.000008	 wd 0.0500	time 0.0300 (0.0308)	loss 3.8034 (3.7152)	grad_norm 13.3323 (14.5831)	loss_scale 131072.0000 (89096.2025)	mem 286MB
[2022-11-04 20:21:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3130/6300]	eta 0:01:37 lr 0.000008	 wd 0.0500	time 0.0331 (0.0308)	loss 3.8956 (3.7154)	grad_norm 12.9175 (14.5768)	loss_scale 131072.0000 (89230.2676)	mem 286MB
[2022-11-04 20:21:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3140/6300]	eta 0:01:37 lr 0.000008	 wd 0.0500	time 0.0329 (0.0308)	loss 3.6888 (3.7154)	grad_norm 13.5524 (14.5694)	loss_scale 131072.0000 (89363.4791)	mem 286MB
[2022-11-04 20:21:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3150/6300]	eta 0:01:36 lr 0.000008	 wd 0.0500	time 0.0293 (0.0308)	loss 3.6448 (3.7154)	grad_norm 13.2960 (14.5651)	loss_scale 131072.0000 (89495.8451)	mem 286MB
[2022-11-04 20:21:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3160/6300]	eta 0:01:36 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 3.7781 (3.7154)	grad_norm 12.4167 (14.5594)	loss_scale 131072.0000 (89627.3736)	mem 286MB
[2022-11-04 20:21:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3170/6300]	eta 0:01:36 lr 0.000008	 wd 0.0500	time 0.0301 (0.0308)	loss 3.3889 (3.7149)	grad_norm 11.5136 (14.5528)	loss_scale 131072.0000 (89758.0725)	mem 286MB
[2022-11-04 20:21:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3180/6300]	eta 0:01:35 lr 0.000008	 wd 0.0500	time 0.0290 (0.0308)	loss 3.2746 (3.7146)	grad_norm 14.5280 (14.5458)	loss_scale 131072.0000 (89887.9497)	mem 286MB
[2022-11-04 20:21:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3190/6300]	eta 0:01:35 lr 0.000008	 wd 0.0500	time 0.0287 (0.0308)	loss 3.5408 (3.7144)	grad_norm 11.0628 (14.5393)	loss_scale 131072.0000 (90017.0128)	mem 286MB
[2022-11-04 20:21:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3200/6300]	eta 0:01:35 lr 0.000008	 wd 0.0500	time 0.0293 (0.0307)	loss 3.5052 (3.7138)	grad_norm 12.4124 (14.5337)	loss_scale 131072.0000 (90145.2696)	mem 286MB
[2022-11-04 20:21:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3210/6300]	eta 0:01:34 lr 0.000008	 wd 0.0500	time 0.0283 (0.0307)	loss 3.5680 (3.7134)	grad_norm 10.9241 (14.5258)	loss_scale 131072.0000 (90272.7275)	mem 286MB
[2022-11-04 20:21:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3220/6300]	eta 0:01:34 lr 0.000008	 wd 0.0500	time 0.0293 (0.0307)	loss 3.9964 (3.7130)	grad_norm 14.4524 (14.5193)	loss_scale 131072.0000 (90399.3940)	mem 286MB
[2022-11-04 20:22:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3230/6300]	eta 0:01:34 lr 0.000008	 wd 0.0500	time 0.0288 (0.0307)	loss 3.8960 (3.7140)	grad_norm 15.1754 (14.5243)	loss_scale 131072.0000 (90525.2764)	mem 286MB
[2022-11-04 20:22:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3240/6300]	eta 0:01:34 lr 0.000008	 wd 0.0500	time 0.0330 (0.0307)	loss 4.2729 (3.7154)	grad_norm 17.0535 (14.5285)	loss_scale 131072.0000 (90650.3820)	mem 286MB
[2022-11-04 20:22:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3250/6300]	eta 0:01:33 lr 0.000008	 wd 0.0500	time 0.0296 (0.0307)	loss 3.8680 (3.7164)	grad_norm 16.6730 (14.5334)	loss_scale 131072.0000 (90774.7179)	mem 286MB
[2022-11-04 20:22:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3260/6300]	eta 0:01:33 lr 0.000008	 wd 0.0500	time 0.0283 (0.0307)	loss 3.8806 (3.7171)	grad_norm 13.2218 (14.5357)	loss_scale 131072.0000 (90898.2913)	mem 286MB
[2022-11-04 20:22:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3270/6300]	eta 0:01:33 lr 0.000008	 wd 0.0500	time 0.0296 (0.0307)	loss 3.8589 (3.7177)	grad_norm 13.5524 (14.5377)	loss_scale 131072.0000 (91021.1091)	mem 286MB
[2022-11-04 20:22:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3280/6300]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.0288 (0.0307)	loss 3.6305 (3.7180)	grad_norm 14.4031 (14.5381)	loss_scale 131072.0000 (91143.1783)	mem 286MB
[2022-11-04 20:22:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3290/6300]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.0298 (0.0307)	loss 3.7789 (3.7184)	grad_norm 14.1147 (14.5422)	loss_scale 131072.0000 (91264.5056)	mem 286MB
[2022-11-04 20:22:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3300/6300]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.0285 (0.0307)	loss 3.8266 (3.7185)	grad_norm 16.7659 (14.5464)	loss_scale 131072.0000 (91385.0978)	mem 286MB
[2022-11-04 20:22:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3310/6300]	eta 0:01:31 lr 0.000008	 wd 0.0500	time 0.0292 (0.0307)	loss 3.7486 (3.7188)	grad_norm 16.9016 (14.5467)	loss_scale 131072.0000 (91504.9616)	mem 286MB
[2022-11-04 20:22:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3320/6300]	eta 0:01:31 lr 0.000008	 wd 0.0500	time 0.0298 (0.0307)	loss 3.4153 (3.7187)	grad_norm 15.7084 (14.5481)	loss_scale 131072.0000 (91624.1036)	mem 286MB
[2022-11-04 20:22:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3330/6300]	eta 0:01:31 lr 0.000008	 wd 0.0500	time 0.0290 (0.0307)	loss 3.5419 (3.7187)	grad_norm 15.5768 (14.5491)	loss_scale 131072.0000 (91742.5302)	mem 286MB
[2022-11-04 20:22:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3340/6300]	eta 0:01:30 lr 0.000008	 wd 0.0500	time 0.0291 (0.0307)	loss 3.6209 (3.7183)	grad_norm 13.2023 (14.5512)	loss_scale 131072.0000 (91860.2478)	mem 286MB
[2022-11-04 20:22:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3350/6300]	eta 0:01:30 lr 0.000008	 wd 0.0500	time 0.0289 (0.0307)	loss 3.5954 (3.7179)	grad_norm 17.9748 (14.5551)	loss_scale 131072.0000 (91977.2629)	mem 286MB
[2022-11-04 20:22:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3360/6300]	eta 0:01:30 lr 0.000008	 wd 0.0500	time 0.0294 (0.0307)	loss 4.2430 (3.7174)	grad_norm 15.4898 (14.5573)	loss_scale 131072.0000 (92093.5817)	mem 286MB
[2022-11-04 20:22:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3370/6300]	eta 0:01:29 lr 0.000008	 wd 0.0500	time 0.0288 (0.0307)	loss 4.0826 (3.7186)	grad_norm 15.9357 (14.5642)	loss_scale 131072.0000 (92209.2103)	mem 286MB
[2022-11-04 20:22:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3380/6300]	eta 0:01:29 lr 0.000008	 wd 0.0500	time 0.0302 (0.0307)	loss 4.0993 (3.7196)	grad_norm 16.1036 (14.5722)	loss_scale 131072.0000 (92324.1550)	mem 286MB
[2022-11-04 20:22:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3390/6300]	eta 0:01:29 lr 0.000008	 wd 0.0500	time 0.0281 (0.0307)	loss 4.1201 (3.7208)	grad_norm 18.3930 (14.5797)	loss_scale 131072.0000 (92438.4217)	mem 286MB
[2022-11-04 20:22:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3400/6300]	eta 0:01:28 lr 0.000008	 wd 0.0500	time 0.0290 (0.0307)	loss 3.9900 (3.7218)	grad_norm 15.2830 (14.5849)	loss_scale 131072.0000 (92552.0165)	mem 286MB
[2022-11-04 20:22:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3410/6300]	eta 0:01:28 lr 0.000008	 wd 0.0500	time 0.0300 (0.0307)	loss 3.8043 (3.7225)	grad_norm 15.6990 (14.5896)	loss_scale 131072.0000 (92664.9452)	mem 286MB
[2022-11-04 20:22:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3420/6300]	eta 0:01:28 lr 0.000008	 wd 0.0500	time 0.0298 (0.0307)	loss 3.6947 (3.7229)	grad_norm 16.7196 (14.5914)	loss_scale 131072.0000 (92777.2137)	mem 286MB
[2022-11-04 20:22:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3430/6300]	eta 0:01:28 lr 0.000008	 wd 0.0500	time 0.0288 (0.0307)	loss 3.8496 (3.7230)	grad_norm 16.4822 (14.5977)	loss_scale 131072.0000 (92888.8277)	mem 286MB
[2022-11-04 20:22:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3440/6300]	eta 0:01:27 lr 0.000008	 wd 0.0500	time 0.0329 (0.0307)	loss 3.9009 (3.7234)	grad_norm 16.1924 (14.6040)	loss_scale 131072.0000 (92999.7931)	mem 286MB
[2022-11-04 20:22:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3450/6300]	eta 0:01:27 lr 0.000008	 wd 0.0500	time 0.0291 (0.0307)	loss 3.7767 (3.7235)	grad_norm 13.8616 (14.6089)	loss_scale 131072.0000 (93110.1153)	mem 286MB
[2022-11-04 20:22:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3460/6300]	eta 0:01:27 lr 0.000008	 wd 0.0500	time 0.0323 (0.0307)	loss 3.6570 (3.7239)	grad_norm 16.3997 (14.6154)	loss_scale 131072.0000 (93219.8001)	mem 286MB
[2022-11-04 20:22:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3470/6300]	eta 0:01:26 lr 0.000008	 wd 0.0500	time 0.0343 (0.0307)	loss 3.3379 (3.7238)	grad_norm 17.7073 (14.6210)	loss_scale 131072.0000 (93328.8528)	mem 286MB
[2022-11-04 20:22:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3480/6300]	eta 0:01:26 lr 0.000008	 wd 0.0500	time 0.0343 (0.0307)	loss 3.8303 (3.7234)	grad_norm 15.5964 (14.6263)	loss_scale 131072.0000 (93437.2789)	mem 286MB
[2022-11-04 20:22:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3490/6300]	eta 0:01:26 lr 0.000008	 wd 0.0500	time 0.0298 (0.0307)	loss 3.9543 (3.7233)	grad_norm 18.1751 (14.6312)	loss_scale 131072.0000 (93545.0839)	mem 286MB
[2022-11-04 20:22:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3500/6300]	eta 0:01:25 lr 0.000008	 wd 0.0500	time 0.0317 (0.0307)	loss 3.9909 (3.7232)	grad_norm 13.8826 (14.6349)	loss_scale 131072.0000 (93652.2731)	mem 286MB
[2022-11-04 20:22:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3510/6300]	eta 0:01:25 lr 0.000008	 wd 0.0500	time 0.0294 (0.0307)	loss 3.7939 (3.7240)	grad_norm 15.3986 (14.6363)	loss_scale 131072.0000 (93758.8516)	mem 286MB
[2022-11-04 20:22:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3520/6300]	eta 0:01:25 lr 0.000008	 wd 0.0500	time 0.0286 (0.0307)	loss 4.2791 (3.7247)	grad_norm 14.7238 (14.6358)	loss_scale 131072.0000 (93864.8248)	mem 286MB
[2022-11-04 20:22:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3530/6300]	eta 0:01:25 lr 0.000008	 wd 0.0500	time 0.0300 (0.0307)	loss 3.9172 (3.7253)	grad_norm 15.0248 (14.6353)	loss_scale 131072.0000 (93970.1977)	mem 286MB
[2022-11-04 20:22:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3540/6300]	eta 0:01:24 lr 0.000008	 wd 0.0500	time 0.0296 (0.0307)	loss 3.8072 (3.7259)	grad_norm 13.1669 (14.6346)	loss_scale 131072.0000 (94074.9754)	mem 286MB
[2022-11-04 20:22:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3550/6300]	eta 0:01:24 lr 0.000008	 wd 0.0500	time 0.0291 (0.0307)	loss 3.7851 (3.7263)	grad_norm 13.3478 (14.6336)	loss_scale 131072.0000 (94179.1631)	mem 286MB
[2022-11-04 20:22:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3560/6300]	eta 0:01:24 lr 0.000008	 wd 0.0500	time 0.0299 (0.0307)	loss 3.9145 (3.7267)	grad_norm 15.4131 (14.6313)	loss_scale 131072.0000 (94282.7655)	mem 286MB
[2022-11-04 20:22:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3570/6300]	eta 0:01:23 lr 0.000008	 wd 0.0500	time 0.0316 (0.0307)	loss 3.7652 (3.7268)	grad_norm 15.9698 (14.6301)	loss_scale 131072.0000 (94385.7877)	mem 286MB
[2022-11-04 20:22:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3580/6300]	eta 0:01:23 lr 0.000008	 wd 0.0500	time 0.0297 (0.0307)	loss 3.7684 (3.7270)	grad_norm 12.4304 (14.6291)	loss_scale 131072.0000 (94488.2346)	mem 286MB
[2022-11-04 20:22:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3590/6300]	eta 0:01:23 lr 0.000008	 wd 0.0500	time 0.0289 (0.0307)	loss 3.6172 (3.7271)	grad_norm 12.8737 (14.6267)	loss_scale 131072.0000 (94590.1108)	mem 286MB
[2022-11-04 20:22:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3600/6300]	eta 0:01:22 lr 0.000008	 wd 0.0500	time 0.0281 (0.0307)	loss 3.7338 (3.7270)	grad_norm 14.0214 (14.6253)	loss_scale 131072.0000 (94691.4213)	mem 286MB
[2022-11-04 20:22:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3610/6300]	eta 0:01:22 lr 0.000008	 wd 0.0500	time 0.0296 (0.0307)	loss 3.7598 (3.7268)	grad_norm 14.0670 (14.6245)	loss_scale 131072.0000 (94792.1706)	mem 286MB
[2022-11-04 20:22:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3620/6300]	eta 0:01:22 lr 0.000008	 wd 0.0500	time 0.0292 (0.0307)	loss 3.6605 (3.7268)	grad_norm 14.0981 (14.6224)	loss_scale 131072.0000 (94892.3634)	mem 286MB
[2022-11-04 20:22:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3630/6300]	eta 0:01:21 lr 0.000008	 wd 0.0500	time 0.0299 (0.0307)	loss 3.5583 (3.7264)	grad_norm 14.1472 (14.6204)	loss_scale 131072.0000 (94992.0044)	mem 286MB
[2022-11-04 20:22:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3640/6300]	eta 0:01:21 lr 0.000008	 wd 0.0500	time 0.0295 (0.0307)	loss 3.8107 (3.7262)	grad_norm 14.8285 (14.6187)	loss_scale 131072.0000 (95091.0980)	mem 286MB
[2022-11-04 20:22:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3650/6300]	eta 0:01:21 lr 0.000008	 wd 0.0500	time 0.0286 (0.0307)	loss 4.0648 (3.7269)	grad_norm 16.5701 (14.6195)	loss_scale 131072.0000 (95189.6489)	mem 286MB
[2022-11-04 20:22:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3660/6300]	eta 0:01:20 lr 0.000008	 wd 0.0500	time 0.0300 (0.0307)	loss 3.7651 (3.7273)	grad_norm 13.6208 (14.6199)	loss_scale 131072.0000 (95287.6613)	mem 286MB
[2022-11-04 20:22:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3670/6300]	eta 0:01:20 lr 0.000008	 wd 0.0500	time 0.0291 (0.0306)	loss 3.9561 (3.7279)	grad_norm 16.3557 (14.6190)	loss_scale 131072.0000 (95385.1397)	mem 286MB
[2022-11-04 20:22:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3680/6300]	eta 0:01:20 lr 0.000008	 wd 0.0500	time 0.0301 (0.0306)	loss 4.2452 (3.7287)	grad_norm 14.2033 (14.6195)	loss_scale 131072.0000 (95482.0886)	mem 286MB
[2022-11-04 20:22:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3690/6300]	eta 0:01:19 lr 0.000008	 wd 0.0500	time 0.0324 (0.0306)	loss 3.7858 (3.7292)	grad_norm 12.5939 (14.6207)	loss_scale 131072.0000 (95578.5121)	mem 286MB
[2022-11-04 20:22:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3700/6300]	eta 0:01:19 lr 0.000008	 wd 0.0500	time 0.0317 (0.0307)	loss 3.8886 (3.7296)	grad_norm 13.1871 (14.6205)	loss_scale 131072.0000 (95674.4145)	mem 286MB
[2022-11-04 20:22:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3710/6300]	eta 0:01:19 lr 0.000008	 wd 0.0500	time 0.0323 (0.0307)	loss 4.0412 (3.7300)	grad_norm 18.4099 (14.6203)	loss_scale 131072.0000 (95769.8001)	mem 286MB
[2022-11-04 20:22:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3720/6300]	eta 0:01:19 lr 0.000008	 wd 0.0500	time 0.0316 (0.0307)	loss 3.7425 (3.7304)	grad_norm 15.9551 (14.6211)	loss_scale 131072.0000 (95864.6729)	mem 286MB
[2022-11-04 20:22:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3730/6300]	eta 0:01:18 lr 0.000008	 wd 0.0500	time 0.0328 (0.0307)	loss 3.8791 (3.7306)	grad_norm 16.9914 (14.6246)	loss_scale 131072.0000 (95959.0373)	mem 286MB
[2022-11-04 20:22:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3740/6300]	eta 0:01:18 lr 0.000008	 wd 0.0500	time 0.0325 (0.0307)	loss 3.7323 (3.7308)	grad_norm 13.9509 (14.6264)	loss_scale 131072.0000 (96052.8971)	mem 286MB
[2022-11-04 20:22:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3750/6300]	eta 0:01:18 lr 0.000008	 wd 0.0500	time 0.0322 (0.0307)	loss 3.7558 (3.7308)	grad_norm 14.0616 (14.6261)	loss_scale 131072.0000 (96146.2565)	mem 286MB
[2022-11-04 20:22:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3760/6300]	eta 0:01:17 lr 0.000008	 wd 0.0500	time 0.0335 (0.0307)	loss 3.7561 (3.7307)	grad_norm 14.0350 (14.6268)	loss_scale 131072.0000 (96239.1194)	mem 286MB
[2022-11-04 20:22:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3770/6300]	eta 0:01:17 lr 0.000008	 wd 0.0500	time 0.0330 (0.0307)	loss 3.6168 (3.7305)	grad_norm 13.5128 (14.6264)	loss_scale 131072.0000 (96331.4898)	mem 286MB
[2022-11-04 20:22:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3780/6300]	eta 0:01:17 lr 0.000008	 wd 0.0500	time 0.0343 (0.0307)	loss 4.2791 (3.7303)	grad_norm 14.8601 (14.6268)	loss_scale 131072.0000 (96423.3716)	mem 286MB
[2022-11-04 20:22:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3790/6300]	eta 0:01:17 lr 0.000008	 wd 0.0500	time 0.0320 (0.0307)	loss 4.2711 (3.7319)	grad_norm 16.7920 (14.6294)	loss_scale 131072.0000 (96514.7687)	mem 286MB
[2022-11-04 20:22:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3800/6300]	eta 0:01:16 lr 0.000008	 wd 0.0500	time 0.0327 (0.0307)	loss 4.5857 (3.7335)	grad_norm 18.8670 (14.6310)	loss_scale 131072.0000 (96605.6848)	mem 286MB
[2022-11-04 20:22:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3810/6300]	eta 0:01:16 lr 0.000008	 wd 0.0500	time 0.0327 (0.0307)	loss 4.2914 (3.7351)	grad_norm 15.5473 (14.6335)	loss_scale 131072.0000 (96696.1239)	mem 286MB
[2022-11-04 20:22:18 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3820/6300]	eta 0:01:16 lr 0.000008	 wd 0.0500	time 0.0333 (0.0307)	loss 4.2091 (3.7363)	grad_norm 14.5239 (14.6350)	loss_scale 131072.0000 (96786.0895)	mem 286MB
[2022-11-04 20:22:18 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3830/6300]	eta 0:01:15 lr 0.000008	 wd 0.0500	time 0.0322 (0.0307)	loss 4.1528 (3.7377)	grad_norm 15.2134 (14.6372)	loss_scale 131072.0000 (96875.5855)	mem 286MB
[2022-11-04 20:22:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3840/6300]	eta 0:01:15 lr 0.000008	 wd 0.0500	time 0.0323 (0.0307)	loss 4.1939 (3.7387)	grad_norm 13.7744 (14.6374)	loss_scale 131072.0000 (96964.6155)	mem 286MB
[2022-11-04 20:22:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3850/6300]	eta 0:01:15 lr 0.000008	 wd 0.0500	time 0.0325 (0.0307)	loss 4.1484 (3.7396)	grad_norm 12.8189 (14.6382)	loss_scale 131072.0000 (97053.1831)	mem 286MB
[2022-11-04 20:22:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3860/6300]	eta 0:01:15 lr 0.000008	 wd 0.0500	time 0.0324 (0.0308)	loss 4.0295 (3.7407)	grad_norm 16.6575 (14.6389)	loss_scale 131072.0000 (97141.2919)	mem 286MB
[2022-11-04 20:22:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3870/6300]	eta 0:01:14 lr 0.000008	 wd 0.0500	time 0.0343 (0.0308)	loss 4.0760 (3.7416)	grad_norm 16.4820 (14.6412)	loss_scale 131072.0000 (97228.9455)	mem 286MB
[2022-11-04 20:22:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3880/6300]	eta 0:01:14 lr 0.000008	 wd 0.0500	time 0.0293 (0.0308)	loss 3.9449 (3.7425)	grad_norm 14.2850 (14.6422)	loss_scale 131072.0000 (97316.1474)	mem 286MB
[2022-11-04 20:22:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3890/6300]	eta 0:01:14 lr 0.000008	 wd 0.0500	time 0.0281 (0.0308)	loss 3.8258 (3.7431)	grad_norm 12.9131 (14.6422)	loss_scale 131072.0000 (97402.9011)	mem 286MB
[2022-11-04 20:22:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3900/6300]	eta 0:01:13 lr 0.000008	 wd 0.0500	time 0.0293 (0.0308)	loss 3.9488 (3.7439)	grad_norm 13.3807 (14.6439)	loss_scale 131072.0000 (97489.2099)	mem 286MB
[2022-11-04 20:22:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3910/6300]	eta 0:01:13 lr 0.000008	 wd 0.0500	time 0.0289 (0.0308)	loss 3.9131 (3.7444)	grad_norm 16.8373 (14.6445)	loss_scale 131072.0000 (97575.0775)	mem 286MB
[2022-11-04 20:22:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3920/6300]	eta 0:01:13 lr 0.000008	 wd 0.0500	time 0.0299 (0.0308)	loss 4.1458 (3.7448)	grad_norm 14.6428 (14.6457)	loss_scale 131072.0000 (97660.5070)	mem 286MB
[2022-11-04 20:22:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3930/6300]	eta 0:01:12 lr 0.000008	 wd 0.0500	time 0.0287 (0.0307)	loss 4.0287 (3.7457)	grad_norm 16.6889 (14.6475)	loss_scale 131072.0000 (97745.5019)	mem 286MB
[2022-11-04 20:22:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3940/6300]	eta 0:01:12 lr 0.000008	 wd 0.0500	time 0.0296 (0.0307)	loss 4.0289 (3.7464)	grad_norm 14.8103 (14.6480)	loss_scale 131072.0000 (97830.0655)	mem 286MB
[2022-11-04 20:22:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3950/6300]	eta 0:01:12 lr 0.000008	 wd 0.0500	time 0.0284 (0.0307)	loss 4.0793 (3.7470)	grad_norm 15.6698 (14.6485)	loss_scale 131072.0000 (97914.2010)	mem 286MB
[2022-11-04 20:22:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3960/6300]	eta 0:01:11 lr 0.000008	 wd 0.0500	time 0.0300 (0.0307)	loss 4.0131 (3.7474)	grad_norm 13.7780 (14.6482)	loss_scale 131072.0000 (97997.9116)	mem 286MB
[2022-11-04 20:22:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3970/6300]	eta 0:01:11 lr 0.000008	 wd 0.0500	time 0.0326 (0.0307)	loss 3.7884 (3.7478)	grad_norm 15.3779 (14.6495)	loss_scale 131072.0000 (98081.2007)	mem 286MB
[2022-11-04 20:22:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3980/6300]	eta 0:01:11 lr 0.000008	 wd 0.0500	time 0.0316 (0.0307)	loss 3.9445 (3.7482)	grad_norm 15.2730 (14.6498)	loss_scale 131072.0000 (98164.0713)	mem 286MB
[2022-11-04 20:22:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][3990/6300]	eta 0:01:11 lr 0.000008	 wd 0.0500	time 0.0300 (0.0307)	loss 3.9393 (3.7484)	grad_norm 14.1647 (14.6505)	loss_scale 131072.0000 (98246.5267)	mem 286MB
[2022-11-04 20:22:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4000/6300]	eta 0:01:10 lr 0.000008	 wd 0.0500	time 0.0291 (0.0307)	loss 3.8345 (3.7485)	grad_norm 15.7017 (14.6517)	loss_scale 262144.0000 (98394.0895)	mem 286MB
[2022-11-04 20:22:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4010/6300]	eta 0:01:10 lr 0.000008	 wd 0.0500	time 0.0296 (0.0307)	loss 4.1760 (3.7487)	grad_norm 16.8769 (14.6528)	loss_scale 262144.0000 (98802.3416)	mem 286MB
[2022-11-04 20:22:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4020/6300]	eta 0:01:10 lr 0.000008	 wd 0.0500	time 0.0318 (0.0307)	loss 3.8541 (3.7487)	grad_norm 13.2641 (14.6541)	loss_scale 262144.0000 (99208.5630)	mem 286MB
[2022-11-04 20:22:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4030/6300]	eta 0:01:09 lr 0.000008	 wd 0.0500	time 0.0292 (0.0307)	loss 3.5329 (3.7487)	grad_norm 15.2777 (14.6569)	loss_scale 262144.0000 (99612.7690)	mem 286MB
[2022-11-04 20:22:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4040/6300]	eta 0:01:09 lr 0.000008	 wd 0.0500	time 0.0303 (0.0307)	loss 3.7026 (3.7486)	grad_norm 13.6340 (14.6567)	loss_scale 262144.0000 (100014.9745)	mem 286MB
[2022-11-04 20:22:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4050/6300]	eta 0:01:09 lr 0.000008	 wd 0.0500	time 0.0290 (0.0307)	loss 3.5748 (3.7482)	grad_norm 15.7549 (14.6567)	loss_scale 262144.0000 (100415.1943)	mem 286MB
[2022-11-04 20:22:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4060/6300]	eta 0:01:08 lr 0.000008	 wd 0.0500	time 0.0291 (0.0307)	loss 3.9325 (3.7479)	grad_norm 15.0648 (14.6575)	loss_scale 262144.0000 (100813.4430)	mem 286MB
[2022-11-04 20:22:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4070/6300]	eta 0:01:08 lr 0.000008	 wd 0.0500	time 0.0292 (0.0307)	loss 3.6581 (3.7485)	grad_norm 13.3229 (14.6566)	loss_scale 262144.0000 (101209.7352)	mem 286MB
[2022-11-04 20:22:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4080/6300]	eta 0:01:08 lr 0.000008	 wd 0.0500	time 0.0319 (0.0307)	loss 4.0701 (3.7493)	grad_norm 15.8086 (14.6566)	loss_scale 262144.0000 (101604.0853)	mem 286MB
[2022-11-04 20:22:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4090/6300]	eta 0:01:07 lr 0.000008	 wd 0.0500	time 0.0306 (0.0307)	loss 3.8915 (3.7501)	grad_norm 14.4678 (14.6573)	loss_scale 262144.0000 (101996.5075)	mem 286MB
[2022-11-04 20:22:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4100/6300]	eta 0:01:07 lr 0.000008	 wd 0.0500	time 0.0307 (0.0307)	loss 3.8875 (3.7506)	grad_norm 14.9097 (14.6570)	loss_scale 262144.0000 (102387.0158)	mem 286MB
[2022-11-04 20:22:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4110/6300]	eta 0:01:07 lr 0.000008	 wd 0.0500	time 0.0300 (0.0307)	loss 3.8997 (3.7511)	grad_norm 12.8139 (14.6558)	loss_scale 262144.0000 (102775.6244)	mem 286MB
[2022-11-04 20:22:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4120/6300]	eta 0:01:06 lr 0.000008	 wd 0.0500	time 0.0338 (0.0307)	loss 4.1466 (3.7517)	grad_norm 15.3322 (14.6543)	loss_scale 262144.0000 (103162.3470)	mem 286MB
[2022-11-04 20:22:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4130/6300]	eta 0:01:06 lr 0.000008	 wd 0.0500	time 0.0305 (0.0307)	loss 3.8331 (3.7520)	grad_norm 15.4004 (14.6555)	loss_scale 262144.0000 (103547.1973)	mem 286MB
[2022-11-04 20:22:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4140/6300]	eta 0:01:06 lr 0.000008	 wd 0.0500	time 0.0344 (0.0307)	loss 3.7619 (3.7522)	grad_norm 13.9891 (14.6551)	loss_scale 262144.0000 (103930.1888)	mem 286MB
[2022-11-04 20:22:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4150/6300]	eta 0:01:06 lr 0.000008	 wd 0.0500	time 0.0309 (0.0307)	loss 3.9224 (3.7523)	grad_norm 15.0104 (14.6549)	loss_scale 262144.0000 (104311.3351)	mem 286MB
[2022-11-04 20:22:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4160/6300]	eta 0:01:05 lr 0.000008	 wd 0.0500	time 0.0292 (0.0307)	loss 3.7386 (3.7525)	grad_norm 12.6945 (14.6552)	loss_scale 262144.0000 (104690.6494)	mem 286MB
[2022-11-04 20:22:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4170/6300]	eta 0:01:05 lr 0.000008	 wd 0.0500	time 0.0291 (0.0307)	loss 3.9001 (3.7525)	grad_norm 14.3019 (14.6542)	loss_scale 262144.0000 (105068.1448)	mem 286MB
[2022-11-04 20:22:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4180/6300]	eta 0:01:05 lr 0.000008	 wd 0.0500	time 0.0306 (0.0307)	loss 3.7418 (3.7524)	grad_norm 15.0450 (14.6539)	loss_scale 262144.0000 (105443.8345)	mem 286MB
[2022-11-04 20:22:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4190/6300]	eta 0:01:04 lr 0.000008	 wd 0.0500	time 0.0287 (0.0307)	loss 3.6430 (3.7525)	grad_norm 12.1540 (14.6523)	loss_scale 262144.0000 (105817.7313)	mem 286MB
[2022-11-04 20:22:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4200/6300]	eta 0:01:04 lr 0.000008	 wd 0.0500	time 0.0297 (0.0307)	loss 3.8091 (3.7522)	grad_norm 15.4244 (14.6510)	loss_scale 262144.0000 (106189.8481)	mem 286MB
[2022-11-04 20:22:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4210/6300]	eta 0:01:04 lr 0.000008	 wd 0.0500	time 0.0300 (0.0307)	loss 3.7637 (3.7525)	grad_norm 14.5024 (14.6522)	loss_scale 262144.0000 (106560.1976)	mem 286MB
[2022-11-04 20:22:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4220/6300]	eta 0:01:03 lr 0.000008	 wd 0.0500	time 0.0280 (0.0307)	loss 3.6470 (3.7526)	grad_norm 14.9277 (14.6542)	loss_scale 262144.0000 (106928.7922)	mem 286MB
[2022-11-04 20:22:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4230/6300]	eta 0:01:03 lr 0.000008	 wd 0.0500	time 0.0328 (0.0307)	loss 3.6890 (3.7526)	grad_norm 15.8998 (14.6561)	loss_scale 262144.0000 (107295.6445)	mem 286MB
[2022-11-04 20:22:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4240/6300]	eta 0:01:03 lr 0.000008	 wd 0.0500	time 0.0292 (0.0307)	loss 3.7113 (3.7528)	grad_norm 15.7387 (14.6562)	loss_scale 262144.0000 (107660.7668)	mem 286MB
[2022-11-04 20:22:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4250/6300]	eta 0:01:02 lr 0.000008	 wd 0.0500	time 0.0294 (0.0307)	loss 3.9243 (3.7528)	grad_norm 15.6478 (14.6567)	loss_scale 262144.0000 (108024.1713)	mem 286MB
[2022-11-04 20:22:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4260/6300]	eta 0:01:02 lr 0.000008	 wd 0.0500	time 0.0296 (0.0307)	loss 3.3187 (3.7526)	grad_norm 14.6227 (14.6576)	loss_scale 262144.0000 (108385.8700)	mem 286MB
[2022-11-04 20:22:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4270/6300]	eta 0:01:02 lr 0.000008	 wd 0.0500	time 0.0312 (0.0307)	loss 3.6006 (3.7524)	grad_norm 14.5041 (14.6579)	loss_scale 262144.0000 (108745.8750)	mem 286MB
[2022-11-04 20:22:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4280/6300]	eta 0:01:02 lr 0.000008	 wd 0.0500	time 0.0303 (0.0307)	loss 3.4609 (3.7521)	grad_norm 13.7227 (14.6593)	loss_scale 262144.0000 (109104.1981)	mem 286MB
[2022-11-04 20:22:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4290/6300]	eta 0:01:01 lr 0.000008	 wd 0.0500	time 0.0290 (0.0307)	loss 3.6974 (3.7516)	grad_norm 16.9311 (14.6606)	loss_scale 262144.0000 (109460.8511)	mem 286MB
[2022-11-04 20:22:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4300/6300]	eta 0:01:01 lr 0.000008	 wd 0.0500	time 0.0286 (0.0307)	loss 3.6227 (3.7511)	grad_norm 14.3118 (14.6609)	loss_scale 262144.0000 (109815.8456)	mem 286MB
[2022-11-04 20:22:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4310/6300]	eta 0:01:01 lr 0.000008	 wd 0.0500	time 0.0305 (0.0307)	loss 3.5565 (3.7506)	grad_norm 15.3298 (14.6609)	loss_scale 262144.0000 (110169.1932)	mem 286MB
[2022-11-04 20:22:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4320/6300]	eta 0:01:00 lr 0.000008	 wd 0.0500	time 0.0293 (0.0307)	loss 3.4995 (3.7499)	grad_norm 15.6494 (14.6616)	loss_scale 262144.0000 (110520.9053)	mem 286MB
[2022-11-04 20:22:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4330/6300]	eta 0:01:00 lr 0.000008	 wd 0.0500	time 0.0306 (0.0307)	loss 3.2663 (3.7491)	grad_norm 16.1901 (14.6635)	loss_scale 262144.0000 (110870.9933)	mem 286MB
[2022-11-04 20:22:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4340/6300]	eta 0:01:00 lr 0.000008	 wd 0.0500	time 0.0319 (0.0307)	loss 4.2812 (3.7487)	grad_norm 12.5025 (14.6639)	loss_scale 262144.0000 (111219.4683)	mem 286MB
[2022-11-04 20:22:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4350/6300]	eta 0:00:59 lr 0.000008	 wd 0.0500	time 0.0316 (0.0307)	loss 3.6303 (3.7488)	grad_norm 12.3211 (14.6613)	loss_scale 262144.0000 (111566.3415)	mem 286MB
[2022-11-04 20:22:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4360/6300]	eta 0:00:59 lr 0.000008	 wd 0.0500	time 0.0334 (0.0307)	loss 3.8779 (3.7492)	grad_norm 13.2057 (14.6587)	loss_scale 262144.0000 (111911.6239)	mem 286MB
[2022-11-04 20:22:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4370/6300]	eta 0:00:59 lr 0.000008	 wd 0.0500	time 0.0317 (0.0307)	loss 3.6373 (3.7496)	grad_norm 14.3896 (14.6570)	loss_scale 262144.0000 (112255.3265)	mem 286MB
[2022-11-04 20:22:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4380/6300]	eta 0:00:58 lr 0.000008	 wd 0.0500	time 0.0334 (0.0307)	loss 3.9536 (3.7497)	grad_norm 12.6549 (14.6559)	loss_scale 262144.0000 (112597.4599)	mem 286MB
[2022-11-04 20:22:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4390/6300]	eta 0:00:58 lr 0.000008	 wd 0.0500	time 0.0332 (0.0307)	loss 3.6900 (3.7497)	grad_norm 14.1399 (14.6546)	loss_scale 262144.0000 (112938.0351)	mem 286MB
[2022-11-04 20:22:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4400/6300]	eta 0:00:58 lr 0.000008	 wd 0.0500	time 0.0346 (0.0307)	loss 4.0128 (3.7495)	grad_norm 13.1283 (14.6534)	loss_scale 262144.0000 (113277.0625)	mem 286MB
[2022-11-04 20:22:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4410/6300]	eta 0:00:58 lr 0.000008	 wd 0.0500	time 0.0314 (0.0307)	loss 3.9882 (3.7495)	grad_norm 15.3529 (14.6516)	loss_scale 262144.0000 (113614.5527)	mem 286MB
[2022-11-04 20:22:36 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4420/6300]	eta 0:00:57 lr 0.000008	 wd 0.0500	time 0.0316 (0.0307)	loss 3.8583 (3.7494)	grad_norm 13.8181 (14.6503)	loss_scale 262144.0000 (113950.5162)	mem 286MB
[2022-11-04 20:22:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4430/6300]	eta 0:00:57 lr 0.000008	 wd 0.0500	time 0.0314 (0.0307)	loss 3.4733 (3.7493)	grad_norm 14.3174 (14.6480)	loss_scale 262144.0000 (114284.9632)	mem 286MB
[2022-11-04 20:22:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4440/6300]	eta 0:00:57 lr 0.000008	 wd 0.0500	time 0.0316 (0.0307)	loss 3.9964 (3.7491)	grad_norm 10.7479 (14.6450)	loss_scale 262144.0000 (114617.9041)	mem 286MB
[2022-11-04 20:22:37 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4450/6300]	eta 0:00:56 lr 0.000008	 wd 0.0500	time 0.0312 (0.0307)	loss 3.5340 (3.7488)	grad_norm 14.9304 (14.6426)	loss_scale 262144.0000 (114949.3489)	mem 286MB
[2022-11-04 20:22:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4460/6300]	eta 0:00:56 lr 0.000008	 wd 0.0500	time 0.0313 (0.0307)	loss 3.2908 (3.7484)	grad_norm 13.8183 (14.6409)	loss_scale 262144.0000 (115279.3078)	mem 286MB
[2022-11-04 20:22:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4470/6300]	eta 0:00:56 lr 0.000008	 wd 0.0500	time 0.0416 (0.0308)	loss 3.6761 (3.7481)	grad_norm 11.8989 (14.6391)	loss_scale 262144.0000 (115607.7907)	mem 286MB
[2022-11-04 20:22:38 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4480/6300]	eta 0:00:55 lr 0.000008	 wd 0.0500	time 0.0318 (0.0308)	loss 4.0901 (3.7477)	grad_norm 15.1788 (14.6374)	loss_scale 262144.0000 (115934.8074)	mem 286MB
[2022-11-04 20:22:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4490/6300]	eta 0:00:55 lr 0.000008	 wd 0.0500	time 0.0326 (0.0308)	loss 3.9015 (3.7483)	grad_norm 14.7321 (14.6364)	loss_scale 262144.0000 (116260.3678)	mem 286MB
[2022-11-04 20:22:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4500/6300]	eta 0:00:55 lr 0.000008	 wd 0.0500	time 0.0332 (0.0308)	loss 3.8832 (3.7489)	grad_norm 12.8800 (14.6366)	loss_scale 262144.0000 (116584.4817)	mem 286MB
[2022-11-04 20:22:39 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4510/6300]	eta 0:00:55 lr 0.000008	 wd 0.0500	time 0.0289 (0.0308)	loss 4.1537 (3.7494)	grad_norm 13.8642 (14.6333)	loss_scale 262144.0000 (116907.1585)	mem 286MB
[2022-11-04 20:22:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4520/6300]	eta 0:00:54 lr 0.000008	 wd 0.0500	time 0.0295 (0.0308)	loss 3.9652 (3.7497)	grad_norm 15.3718 (14.6318)	loss_scale 262144.0000 (117228.4079)	mem 286MB
[2022-11-04 20:22:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4530/6300]	eta 0:00:54 lr 0.000008	 wd 0.0500	time 0.0320 (0.0308)	loss 3.8417 (3.7502)	grad_norm 14.0464 (14.6311)	loss_scale 262144.0000 (117548.2392)	mem 286MB
[2022-11-04 20:22:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4540/6300]	eta 0:00:54 lr 0.000008	 wd 0.0500	time 0.0312 (0.0308)	loss 4.0177 (3.7504)	grad_norm 13.7623 (14.6287)	loss_scale 262144.0000 (117866.6620)	mem 286MB
[2022-11-04 20:22:40 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4550/6300]	eta 0:00:53 lr 0.000008	 wd 0.0500	time 0.0312 (0.0308)	loss 3.9867 (3.7506)	grad_norm 14.3166 (14.6264)	loss_scale 262144.0000 (118183.6853)	mem 286MB
[2022-11-04 20:22:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4560/6300]	eta 0:00:53 lr 0.000008	 wd 0.0500	time 0.0343 (0.0308)	loss 3.7107 (3.7506)	grad_norm 13.7513 (14.6233)	loss_scale 262144.0000 (118499.3186)	mem 286MB
[2022-11-04 20:22:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4570/6300]	eta 0:00:53 lr 0.000008	 wd 0.0500	time 0.0301 (0.0308)	loss 3.7260 (3.7507)	grad_norm 15.3620 (14.6226)	loss_scale 262144.0000 (118813.5708)	mem 286MB
[2022-11-04 20:22:41 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4580/6300]	eta 0:00:52 lr 0.000008	 wd 0.0500	time 0.0311 (0.0308)	loss 3.9399 (3.7508)	grad_norm 14.0770 (14.6219)	loss_scale 262144.0000 (119126.4510)	mem 286MB
[2022-11-04 20:22:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4590/6300]	eta 0:00:52 lr 0.000008	 wd 0.0500	time 0.0315 (0.0308)	loss 3.8430 (3.7507)	grad_norm 14.7550 (14.6200)	loss_scale 262144.0000 (119437.9682)	mem 286MB
[2022-11-04 20:22:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4600/6300]	eta 0:00:52 lr 0.000008	 wd 0.0500	time 0.0332 (0.0308)	loss 3.7091 (3.7507)	grad_norm 11.6234 (14.6180)	loss_scale 262144.0000 (119748.1313)	mem 286MB
[2022-11-04 20:22:42 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4610/6300]	eta 0:00:52 lr 0.000008	 wd 0.0500	time 0.0315 (0.0308)	loss 3.6198 (3.7505)	grad_norm 12.0880 (14.6156)	loss_scale 262144.0000 (120056.9490)	mem 286MB
[2022-11-04 20:22:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4620/6300]	eta 0:00:51 lr 0.000008	 wd 0.0500	time 0.0343 (0.0308)	loss 4.1701 (3.7503)	grad_norm 15.3728 (14.6150)	loss_scale 262144.0000 (120364.4302)	mem 286MB
[2022-11-04 20:22:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4630/6300]	eta 0:00:51 lr 0.000008	 wd 0.0500	time 0.0323 (0.0308)	loss 4.0784 (3.7516)	grad_norm 14.7905 (14.6184)	loss_scale 262144.0000 (120670.5835)	mem 286MB
[2022-11-04 20:22:43 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4640/6300]	eta 0:00:51 lr 0.000008	 wd 0.0500	time 0.0292 (0.0308)	loss 4.5466 (3.7529)	grad_norm 14.4953 (14.6201)	loss_scale 262144.0000 (120975.4174)	mem 286MB
[2022-11-04 20:22:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4650/6300]	eta 0:00:50 lr 0.000008	 wd 0.0500	time 0.0331 (0.0308)	loss 4.2969 (3.7541)	grad_norm 14.2884 (14.6218)	loss_scale 262144.0000 (121278.9404)	mem 286MB
[2022-11-04 20:22:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4660/6300]	eta 0:00:50 lr 0.000008	 wd 0.0500	time 0.0326 (0.0308)	loss 4.3196 (3.7550)	grad_norm 14.8914 (14.6224)	loss_scale 262144.0000 (121581.1611)	mem 286MB
[2022-11-04 20:22:44 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4670/6300]	eta 0:00:50 lr 0.000008	 wd 0.0500	time 0.0318 (0.0308)	loss 4.1113 (3.7559)	grad_norm 15.5699 (14.6240)	loss_scale 262144.0000 (121882.0878)	mem 286MB
[2022-11-04 20:22:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4680/6300]	eta 0:00:49 lr 0.000008	 wd 0.0500	time 0.0334 (0.0308)	loss 4.3346 (3.7568)	grad_norm 15.7758 (14.6261)	loss_scale 262144.0000 (122181.7287)	mem 286MB
[2022-11-04 20:22:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4690/6300]	eta 0:00:49 lr 0.000008	 wd 0.0500	time 0.0343 (0.0308)	loss 4.0865 (3.7576)	grad_norm 14.3605 (14.6274)	loss_scale 262144.0000 (122480.0921)	mem 286MB
[2022-11-04 20:22:45 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4700/6300]	eta 0:00:49 lr 0.000008	 wd 0.0500	time 0.0308 (0.0308)	loss 4.3512 (3.7584)	grad_norm 17.1883 (14.6288)	loss_scale 262144.0000 (122777.1861)	mem 286MB
[2022-11-04 20:22:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4710/6300]	eta 0:00:49 lr 0.000008	 wd 0.0500	time 0.0348 (0.0308)	loss 3.9936 (3.7591)	grad_norm 17.4371 (14.6323)	loss_scale 262144.0000 (123073.0189)	mem 286MB
[2022-11-04 20:22:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4720/6300]	eta 0:00:48 lr 0.000008	 wd 0.0500	time 0.0326 (0.0308)	loss 4.1049 (3.7597)	grad_norm 15.4639 (14.6346)	loss_scale 262144.0000 (123367.5984)	mem 286MB
[2022-11-04 20:22:46 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4730/6300]	eta 0:00:48 lr 0.000008	 wd 0.0500	time 0.0380 (0.0308)	loss 3.9037 (3.7601)	grad_norm 14.5901 (14.6343)	loss_scale 262144.0000 (123660.9326)	mem 286MB
[2022-11-04 20:22:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4740/6300]	eta 0:00:48 lr 0.000008	 wd 0.0500	time 0.0321 (0.0308)	loss 3.7768 (3.7605)	grad_norm 16.7827 (14.6360)	loss_scale 262144.0000 (123953.0293)	mem 286MB
[2022-11-04 20:22:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4750/6300]	eta 0:00:47 lr 0.000008	 wd 0.0500	time 0.0321 (0.0308)	loss 3.6098 (3.7609)	grad_norm 14.5422 (14.6374)	loss_scale 262144.0000 (124243.8964)	mem 286MB
[2022-11-04 20:22:47 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4760/6300]	eta 0:00:47 lr 0.000008	 wd 0.0500	time 0.0319 (0.0309)	loss 4.1592 (3.7611)	grad_norm 12.9034 (14.6379)	loss_scale 262144.0000 (124533.5417)	mem 286MB
[2022-11-04 20:22:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4770/6300]	eta 0:00:47 lr 0.000008	 wd 0.0500	time 0.0324 (0.0309)	loss 3.6531 (3.7616)	grad_norm 11.9151 (14.6356)	loss_scale 262144.0000 (124821.9728)	mem 286MB
[2022-11-04 20:22:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4780/6300]	eta 0:00:46 lr 0.000008	 wd 0.0500	time 0.0312 (0.0309)	loss 3.9900 (3.7621)	grad_norm 13.5652 (14.6334)	loss_scale 262144.0000 (125109.1972)	mem 286MB
[2022-11-04 20:22:48 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4790/6300]	eta 0:00:46 lr 0.000008	 wd 0.0500	time 0.0327 (0.0309)	loss 3.9955 (3.7624)	grad_norm 13.0007 (14.6310)	loss_scale 262144.0000 (125395.2227)	mem 286MB
[2022-11-04 20:22:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4800/6300]	eta 0:00:46 lr 0.000008	 wd 0.0500	time 0.0316 (0.0309)	loss 3.7358 (3.7627)	grad_norm 12.3258 (14.6291)	loss_scale 262144.0000 (125680.0567)	mem 286MB
[2022-11-04 20:22:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4810/6300]	eta 0:00:45 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 3.7056 (3.7630)	grad_norm 11.5358 (14.6268)	loss_scale 262144.0000 (125963.7065)	mem 286MB
[2022-11-04 20:22:49 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4820/6300]	eta 0:00:45 lr 0.000008	 wd 0.0500	time 0.0344 (0.0309)	loss 3.8288 (3.7631)	grad_norm 13.2051 (14.6228)	loss_scale 262144.0000 (126246.1796)	mem 286MB
[2022-11-04 20:22:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4830/6300]	eta 0:00:45 lr 0.000008	 wd 0.0500	time 0.0310 (0.0309)	loss 3.5694 (3.7632)	grad_norm 13.5276 (14.6209)	loss_scale 262144.0000 (126527.4833)	mem 286MB
[2022-11-04 20:22:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4840/6300]	eta 0:00:45 lr 0.000008	 wd 0.0500	time 0.0300 (0.0309)	loss 3.7283 (3.7633)	grad_norm 15.4643 (14.6194)	loss_scale 262144.0000 (126807.6249)	mem 286MB
[2022-11-04 20:22:50 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4850/6300]	eta 0:00:44 lr 0.000008	 wd 0.0500	time 0.0301 (0.0309)	loss 3.8434 (3.7633)	grad_norm 15.3348 (14.6162)	loss_scale 262144.0000 (127086.6114)	mem 286MB
[2022-11-04 20:22:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4860/6300]	eta 0:00:44 lr 0.000008	 wd 0.0500	time 0.0307 (0.0309)	loss 3.7062 (3.7634)	grad_norm 14.6739 (14.6138)	loss_scale 262144.0000 (127364.4501)	mem 286MB
[2022-11-04 20:22:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4870/6300]	eta 0:00:44 lr 0.000008	 wd 0.0500	time 0.0307 (0.0309)	loss 3.8734 (3.7635)	grad_norm 12.5781 (14.6101)	loss_scale 262144.0000 (127641.1480)	mem 286MB
[2022-11-04 20:22:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4880/6300]	eta 0:00:43 lr 0.000008	 wd 0.0500	time 0.0303 (0.0309)	loss 3.7273 (3.7634)	grad_norm 13.1020 (14.6071)	loss_scale 262144.0000 (127916.7121)	mem 286MB
[2022-11-04 20:22:51 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4890/6300]	eta 0:00:43 lr 0.000008	 wd 0.0500	time 0.0319 (0.0309)	loss 3.5703 (3.7632)	grad_norm 14.0097 (14.6049)	loss_scale 262144.0000 (128191.1495)	mem 286MB
[2022-11-04 20:22:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4900/6300]	eta 0:00:43 lr 0.000008	 wd 0.0500	time 0.0309 (0.0309)	loss 3.8539 (3.7631)	grad_norm 14.6126 (14.6015)	loss_scale 262144.0000 (128464.4668)	mem 286MB
[2022-11-04 20:22:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4910/6300]	eta 0:00:42 lr 0.000008	 wd 0.0500	time 0.0317 (0.0309)	loss 3.6350 (3.7631)	grad_norm 14.2728 (14.6003)	loss_scale 262144.0000 (128736.6711)	mem 286MB
[2022-11-04 20:22:52 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4920/6300]	eta 0:00:42 lr 0.000008	 wd 0.0500	time 0.0327 (0.0309)	loss 3.8143 (3.7631)	grad_norm 13.4689 (14.5995)	loss_scale 262144.0000 (129007.7692)	mem 286MB
[2022-11-04 20:22:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4930/6300]	eta 0:00:42 lr 0.000008	 wd 0.0500	time 0.0293 (0.0309)	loss 3.8295 (3.7631)	grad_norm 13.6269 (14.5972)	loss_scale 262144.0000 (129277.7676)	mem 286MB
[2022-11-04 20:22:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4940/6300]	eta 0:00:41 lr 0.000008	 wd 0.0500	time 0.0330 (0.0309)	loss 3.7488 (3.7631)	grad_norm 13.7539 (14.5956)	loss_scale 262144.0000 (129546.6731)	mem 286MB
[2022-11-04 20:22:53 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4950/6300]	eta 0:00:41 lr 0.000008	 wd 0.0500	time 0.0325 (0.0309)	loss 3.6530 (3.7629)	grad_norm 12.6010 (14.5941)	loss_scale 262144.0000 (129814.4924)	mem 286MB
[2022-11-04 20:22:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4960/6300]	eta 0:00:41 lr 0.000008	 wd 0.0500	time 0.0318 (0.0309)	loss 3.4657 (3.7626)	grad_norm 14.2867 (14.5923)	loss_scale 262144.0000 (130081.2320)	mem 286MB
[2022-11-04 20:22:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4970/6300]	eta 0:00:41 lr 0.000008	 wd 0.0500	time 0.0303 (0.0309)	loss 3.5776 (3.7621)	grad_norm 15.2990 (14.5905)	loss_scale 262144.0000 (130346.8984)	mem 286MB
[2022-11-04 20:22:54 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4980/6300]	eta 0:00:40 lr 0.000008	 wd 0.0500	time 0.0309 (0.0309)	loss 3.5602 (3.7617)	grad_norm 15.9774 (14.5899)	loss_scale 262144.0000 (130611.4981)	mem 286MB
[2022-11-04 20:22:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][4990/6300]	eta 0:00:40 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 3.4284 (3.7613)	grad_norm 13.7391 (14.5890)	loss_scale 262144.0000 (130875.0375)	mem 286MB
[2022-11-04 20:22:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5000/6300]	eta 0:00:40 lr 0.000008	 wd 0.0500	time 0.0301 (0.0309)	loss 3.5883 (3.7609)	grad_norm 11.4586 (14.5880)	loss_scale 262144.0000 (131137.5229)	mem 286MB
[2022-11-04 20:22:55 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5010/6300]	eta 0:00:39 lr 0.000008	 wd 0.0500	time 0.0331 (0.0309)	loss 3.5064 (3.7602)	grad_norm 14.2516 (14.5867)	loss_scale 262144.0000 (131398.9607)	mem 286MB
[2022-11-04 20:22:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5020/6300]	eta 0:00:39 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 3.1688 (3.7595)	grad_norm 16.5120 (14.5864)	loss_scale 262144.0000 (131659.3571)	mem 286MB
[2022-11-04 20:22:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5030/6300]	eta 0:00:39 lr 0.000008	 wd 0.0500	time 0.0347 (0.0309)	loss 3.0550 (3.7588)	grad_norm 15.6756 (14.5855)	loss_scale 262144.0000 (131918.7183)	mem 286MB
[2022-11-04 20:22:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5040/6300]	eta 0:00:38 lr 0.000008	 wd 0.0500	time 0.0324 (0.0309)	loss 3.8675 (3.7581)	grad_norm 13.8719 (14.5839)	loss_scale 262144.0000 (132177.0506)	mem 286MB
[2022-11-04 20:22:56 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5050/6300]	eta 0:00:38 lr 0.000008	 wd 0.0500	time 0.0313 (0.0309)	loss 3.6834 (3.7584)	grad_norm 13.0947 (14.5818)	loss_scale 262144.0000 (132434.3599)	mem 286MB
[2022-11-04 20:22:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5060/6300]	eta 0:00:38 lr 0.000008	 wd 0.0500	time 0.0302 (0.0309)	loss 3.9980 (3.7584)	grad_norm 15.0876 (14.5814)	loss_scale 262144.0000 (132690.6524)	mem 286MB
[2022-11-04 20:22:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5070/6300]	eta 0:00:38 lr 0.000008	 wd 0.0500	time 0.0298 (0.0309)	loss 3.7429 (3.7585)	grad_norm 11.3409 (14.5780)	loss_scale 262144.0000 (132945.9341)	mem 286MB
[2022-11-04 20:22:57 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5080/6300]	eta 0:00:37 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.6880 (3.7587)	grad_norm 10.4730 (14.5754)	loss_scale 262144.0000 (133200.2110)	mem 286MB
[2022-11-04 20:22:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5090/6300]	eta 0:00:37 lr 0.000008	 wd 0.0500	time 0.0301 (0.0309)	loss 3.8736 (3.7588)	grad_norm 13.6680 (14.5736)	loss_scale 262144.0000 (133453.4889)	mem 286MB
[2022-11-04 20:22:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5100/6300]	eta 0:00:37 lr 0.000008	 wd 0.0500	time 0.0305 (0.0309)	loss 3.4715 (3.7588)	grad_norm 14.2725 (14.5716)	loss_scale 262144.0000 (133705.7738)	mem 286MB
[2022-11-04 20:22:58 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5110/6300]	eta 0:00:36 lr 0.000008	 wd 0.0500	time 0.0292 (0.0309)	loss 3.8002 (3.7590)	grad_norm 14.4339 (14.5683)	loss_scale 262144.0000 (133957.0714)	mem 286MB
[2022-11-04 20:22:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5120/6300]	eta 0:00:36 lr 0.000008	 wd 0.0500	time 0.0297 (0.0309)	loss 3.7107 (3.7588)	grad_norm 12.8037 (14.5660)	loss_scale 262144.0000 (134207.3876)	mem 286MB
[2022-11-04 20:22:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5130/6300]	eta 0:00:36 lr 0.000008	 wd 0.0500	time 0.0293 (0.0309)	loss 3.3785 (3.7586)	grad_norm 14.6004 (14.5613)	loss_scale 262144.0000 (134456.7281)	mem 286MB
[2022-11-04 20:22:59 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5140/6300]	eta 0:00:35 lr 0.000008	 wd 0.0500	time 0.0292 (0.0309)	loss 3.4701 (3.7584)	grad_norm 12.0194 (14.5584)	loss_scale 262144.0000 (134705.0986)	mem 286MB
[2022-11-04 20:23:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5150/6300]	eta 0:00:35 lr 0.000008	 wd 0.0500	time 0.0291 (0.0309)	loss 3.6776 (3.7581)	grad_norm 12.6106 (14.5539)	loss_scale 262144.0000 (134952.5048)	mem 286MB
[2022-11-04 20:23:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5160/6300]	eta 0:00:35 lr 0.000008	 wd 0.0500	time 0.0314 (0.0309)	loss 3.4693 (3.7578)	grad_norm 15.7748 (14.5521)	loss_scale 262144.0000 (135198.9521)	mem 286MB
[2022-11-04 20:23:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5170/6300]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.0343 (0.0309)	loss 3.5011 (3.7575)	grad_norm 15.0507 (14.5502)	loss_scale 262144.0000 (135444.4463)	mem 286MB
[2022-11-04 20:23:00 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5180/6300]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.8831 (3.7571)	grad_norm 14.4437 (14.5475)	loss_scale 262144.0000 (135688.9929)	mem 286MB
[2022-11-04 20:23:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5190/6300]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.0299 (0.0309)	loss 4.0200 (3.7574)	grad_norm 14.3715 (14.5477)	loss_scale 262144.0000 (135932.5972)	mem 286MB
[2022-11-04 20:23:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5200/6300]	eta 0:00:33 lr 0.000008	 wd 0.0500	time 0.0320 (0.0309)	loss 3.9790 (3.7577)	grad_norm 15.6276 (14.5472)	loss_scale 262144.0000 (136175.2648)	mem 286MB
[2022-11-04 20:23:01 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5210/6300]	eta 0:00:33 lr 0.000008	 wd 0.0500	time 0.0342 (0.0309)	loss 3.7771 (3.7579)	grad_norm 13.3753 (14.5452)	loss_scale 262144.0000 (136417.0010)	mem 286MB
[2022-11-04 20:23:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5220/6300]	eta 0:00:33 lr 0.000008	 wd 0.0500	time 0.0316 (0.0309)	loss 3.6450 (3.7581)	grad_norm 13.2092 (14.5452)	loss_scale 262144.0000 (136657.8111)	mem 286MB
[2022-11-04 20:23:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5230/6300]	eta 0:00:33 lr 0.000008	 wd 0.0500	time 0.0344 (0.0309)	loss 3.8367 (3.7581)	grad_norm 16.2083 (14.5454)	loss_scale 262144.0000 (136897.7006)	mem 286MB
[2022-11-04 20:23:02 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5240/6300]	eta 0:00:32 lr 0.000008	 wd 0.0500	time 0.0338 (0.0309)	loss 3.8414 (3.7583)	grad_norm 14.9755 (14.5449)	loss_scale 262144.0000 (137136.6747)	mem 286MB
[2022-11-04 20:23:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5250/6300]	eta 0:00:32 lr 0.000008	 wd 0.0500	time 0.0318 (0.0309)	loss 3.6268 (3.7583)	grad_norm 13.8389 (14.5433)	loss_scale 262144.0000 (137374.7385)	mem 286MB
[2022-11-04 20:23:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5260/6300]	eta 0:00:32 lr 0.000008	 wd 0.0500	time 0.0326 (0.0309)	loss 3.8909 (3.7583)	grad_norm 14.1412 (14.5430)	loss_scale 262144.0000 (137611.8974)	mem 286MB
[2022-11-04 20:23:03 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5270/6300]	eta 0:00:31 lr 0.000008	 wd 0.0500	time 0.0333 (0.0309)	loss 3.6933 (3.7581)	grad_norm 13.6996 (14.5422)	loss_scale 262144.0000 (137848.1563)	mem 286MB
[2022-11-04 20:23:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5280/6300]	eta 0:00:31 lr 0.000008	 wd 0.0500	time 0.0324 (0.0309)	loss 3.4598 (3.7578)	grad_norm 14.4795 (14.5420)	loss_scale 262144.0000 (138083.5205)	mem 286MB
[2022-11-04 20:23:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5290/6300]	eta 0:00:31 lr 0.000008	 wd 0.0500	time 0.0353 (0.0309)	loss 3.4887 (3.7575)	grad_norm 13.5360 (14.5425)	loss_scale 262144.0000 (138317.9951)	mem 286MB
[2022-11-04 20:23:04 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5300/6300]	eta 0:00:30 lr 0.000008	 wd 0.0500	time 0.0353 (0.0309)	loss 3.5222 (3.7572)	grad_norm 14.3224 (14.5427)	loss_scale 262144.0000 (138551.5850)	mem 286MB
[2022-11-04 20:23:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5310/6300]	eta 0:00:30 lr 0.000008	 wd 0.0500	time 0.0356 (0.0309)	loss 3.8422 (3.7568)	grad_norm 15.9921 (14.5444)	loss_scale 262144.0000 (138784.2952)	mem 286MB
[2022-11-04 20:23:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5320/6300]	eta 0:00:30 lr 0.000008	 wd 0.0500	time 0.0300 (0.0309)	loss 4.2604 (3.7564)	grad_norm 12.7805 (14.5437)	loss_scale 262144.0000 (139016.1308)	mem 286MB
[2022-11-04 20:23:05 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5330/6300]	eta 0:00:30 lr 0.000008	 wd 0.0500	time 0.0326 (0.0309)	loss 4.3755 (3.7576)	grad_norm 13.8272 (14.5434)	loss_scale 262144.0000 (139247.0966)	mem 286MB
[2022-11-04 20:23:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5340/6300]	eta 0:00:29 lr 0.000008	 wd 0.0500	time 0.0305 (0.0309)	loss 4.1201 (3.7586)	grad_norm 12.4725 (14.5427)	loss_scale 262144.0000 (139477.1975)	mem 286MB
[2022-11-04 20:23:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5350/6300]	eta 0:00:29 lr 0.000008	 wd 0.0500	time 0.0318 (0.0309)	loss 4.5972 (3.7597)	grad_norm 14.9690 (14.5416)	loss_scale 262144.0000 (139706.4384)	mem 286MB
[2022-11-04 20:23:06 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5360/6300]	eta 0:00:29 lr 0.000008	 wd 0.0500	time 0.0324 (0.0309)	loss 4.3897 (3.7608)	grad_norm 13.6990 (14.5420)	loss_scale 262144.0000 (139934.8241)	mem 286MB
[2022-11-04 20:23:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5370/6300]	eta 0:00:28 lr 0.000008	 wd 0.0500	time 0.0320 (0.0309)	loss 4.2220 (3.7618)	grad_norm 15.3291 (14.5417)	loss_scale 262144.0000 (140162.3593)	mem 286MB
[2022-11-04 20:23:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5380/6300]	eta 0:00:28 lr 0.000008	 wd 0.0500	time 0.0328 (0.0309)	loss 4.0208 (3.7626)	grad_norm 11.7467 (14.5412)	loss_scale 262144.0000 (140389.0489)	mem 286MB
[2022-11-04 20:23:07 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5390/6300]	eta 0:00:28 lr 0.000008	 wd 0.0500	time 0.0322 (0.0310)	loss 4.0424 (3.7634)	grad_norm 12.1527 (14.5406)	loss_scale 262144.0000 (140614.8974)	mem 286MB
[2022-11-04 20:23:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5400/6300]	eta 0:00:27 lr 0.000008	 wd 0.0500	time 0.0355 (0.0310)	loss 4.5027 (3.7642)	grad_norm 13.9601 (14.5398)	loss_scale 262144.0000 (140839.9096)	mem 286MB
[2022-11-04 20:23:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5410/6300]	eta 0:00:27 lr 0.000008	 wd 0.0500	time 0.0304 (0.0310)	loss 4.1747 (3.7649)	grad_norm 14.8885 (14.5387)	loss_scale 262144.0000 (141064.0902)	mem 286MB
[2022-11-04 20:23:08 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5420/6300]	eta 0:00:27 lr 0.000008	 wd 0.0500	time 0.0309 (0.0310)	loss 3.9302 (3.7654)	grad_norm 14.5800 (14.5367)	loss_scale 262144.0000 (141287.4436)	mem 286MB
[2022-11-04 20:23:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5430/6300]	eta 0:00:26 lr 0.000008	 wd 0.0500	time 0.0326 (0.0310)	loss 4.0493 (3.7659)	grad_norm 16.7552 (14.5356)	loss_scale 262144.0000 (141509.9746)	mem 286MB
[2022-11-04 20:23:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5440/6300]	eta 0:00:26 lr 0.000008	 wd 0.0500	time 0.0307 (0.0310)	loss 4.1386 (3.7662)	grad_norm 16.3929 (14.5336)	loss_scale 262144.0000 (141731.6876)	mem 286MB
[2022-11-04 20:23:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5450/6300]	eta 0:00:26 lr 0.000008	 wd 0.0500	time 0.0296 (0.0310)	loss 4.0771 (3.7666)	grad_norm 13.9820 (14.5328)	loss_scale 262144.0000 (141952.5870)	mem 286MB
[2022-11-04 20:23:09 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5460/6300]	eta 0:00:25 lr 0.000008	 wd 0.0500	time 0.0296 (0.0310)	loss 4.3185 (3.7671)	grad_norm 14.4317 (14.5316)	loss_scale 262144.0000 (142172.6775)	mem 286MB
[2022-11-04 20:23:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5470/6300]	eta 0:00:25 lr 0.000008	 wd 0.0500	time 0.0302 (0.0309)	loss 4.2253 (3.7678)	grad_norm 13.5365 (14.5330)	loss_scale 262144.0000 (142391.9634)	mem 286MB
[2022-11-04 20:23:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5480/6300]	eta 0:00:25 lr 0.000008	 wd 0.0500	time 0.0375 (0.0310)	loss 4.1741 (3.7685)	grad_norm 16.8372 (14.5353)	loss_scale 262144.0000 (142610.4492)	mem 286MB
[2022-11-04 20:23:10 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5490/6300]	eta 0:00:25 lr 0.000008	 wd 0.0500	time 0.0298 (0.0309)	loss 4.1764 (3.7692)	grad_norm 15.8166 (14.5362)	loss_scale 262144.0000 (142828.1391)	mem 286MB
[2022-11-04 20:23:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5500/6300]	eta 0:00:24 lr 0.000008	 wd 0.0500	time 0.0332 (0.0309)	loss 4.0798 (3.7699)	grad_norm 14.8913 (14.5370)	loss_scale 262144.0000 (143045.0376)	mem 286MB
[2022-11-04 20:23:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5510/6300]	eta 0:00:24 lr 0.000008	 wd 0.0500	time 0.0311 (0.0309)	loss 4.1585 (3.7705)	grad_norm 15.6298 (14.5375)	loss_scale 262144.0000 (143261.1490)	mem 286MB
[2022-11-04 20:23:11 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5520/6300]	eta 0:00:24 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 4.1187 (3.7710)	grad_norm 13.2266 (14.5384)	loss_scale 262144.0000 (143476.4774)	mem 286MB
[2022-11-04 20:23:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5530/6300]	eta 0:00:23 lr 0.000008	 wd 0.0500	time 0.0289 (0.0309)	loss 3.8659 (3.7714)	grad_norm 13.6464 (14.5397)	loss_scale 262144.0000 (143691.0273)	mem 286MB
[2022-11-04 20:23:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5540/6300]	eta 0:00:23 lr 0.000008	 wd 0.0500	time 0.0375 (0.0309)	loss 3.9696 (3.7717)	grad_norm 14.5241 (14.5391)	loss_scale 262144.0000 (143904.8027)	mem 286MB
[2022-11-04 20:23:12 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5550/6300]	eta 0:00:23 lr 0.000008	 wd 0.0500	time 0.0291 (0.0309)	loss 4.0509 (3.7720)	grad_norm 16.6817 (14.5409)	loss_scale 262144.0000 (144117.8080)	mem 286MB
[2022-11-04 20:23:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5560/6300]	eta 0:00:22 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.9506 (3.7723)	grad_norm 14.5120 (14.5420)	loss_scale 262144.0000 (144330.0471)	mem 286MB
[2022-11-04 20:23:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5570/6300]	eta 0:00:22 lr 0.000008	 wd 0.0500	time 0.0298 (0.0309)	loss 3.7822 (3.7724)	grad_norm 17.5507 (14.5434)	loss_scale 262144.0000 (144541.5243)	mem 286MB
[2022-11-04 20:23:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5580/6300]	eta 0:00:22 lr 0.000008	 wd 0.0500	time 0.0312 (0.0309)	loss 3.6885 (3.7725)	grad_norm 14.2425 (14.5432)	loss_scale 262144.0000 (144752.2437)	mem 286MB
[2022-11-04 20:23:13 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5590/6300]	eta 0:00:21 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 3.7060 (3.7724)	grad_norm 14.3964 (14.5440)	loss_scale 262144.0000 (144962.2093)	mem 286MB
[2022-11-04 20:23:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5600/6300]	eta 0:00:21 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 4.2186 (3.7725)	grad_norm 16.2587 (14.5452)	loss_scale 262144.0000 (145171.4251)	mem 286MB
[2022-11-04 20:23:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5610/6300]	eta 0:00:21 lr 0.000008	 wd 0.0500	time 0.0306 (0.0309)	loss 4.1024 (3.7733)	grad_norm 13.5083 (14.5477)	loss_scale 262144.0000 (145379.8952)	mem 286MB
[2022-11-04 20:23:14 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5620/6300]	eta 0:00:21 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 4.7652 (3.7741)	grad_norm 18.0765 (14.5500)	loss_scale 262144.0000 (145587.6236)	mem 286MB
[2022-11-04 20:23:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5630/6300]	eta 0:00:20 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 4.3333 (3.7747)	grad_norm 16.1671 (14.5517)	loss_scale 262144.0000 (145794.6141)	mem 286MB
[2022-11-04 20:23:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5640/6300]	eta 0:00:20 lr 0.000008	 wd 0.0500	time 0.0288 (0.0309)	loss 4.1317 (3.7754)	grad_norm 15.3129 (14.5537)	loss_scale 262144.0000 (146000.8708)	mem 286MB
[2022-11-04 20:23:15 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5650/6300]	eta 0:00:20 lr 0.000008	 wd 0.0500	time 0.0292 (0.0309)	loss 3.9216 (3.7758)	grad_norm 13.4912 (14.5543)	loss_scale 262144.0000 (146206.3975)	mem 286MB
[2022-11-04 20:23:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5660/6300]	eta 0:00:19 lr 0.000008	 wd 0.0500	time 0.0292 (0.0309)	loss 4.0008 (3.7764)	grad_norm 16.0281 (14.5556)	loss_scale 262144.0000 (146411.1980)	mem 286MB
[2022-11-04 20:23:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5670/6300]	eta 0:00:19 lr 0.000008	 wd 0.0500	time 0.0284 (0.0309)	loss 4.0113 (3.7767)	grad_norm 15.3559 (14.5568)	loss_scale 262144.0000 (146615.2763)	mem 286MB
[2022-11-04 20:23:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5680/6300]	eta 0:00:19 lr 0.000008	 wd 0.0500	time 0.0301 (0.0309)	loss 3.6406 (3.7770)	grad_norm 15.1490 (14.5585)	loss_scale 262144.0000 (146818.6362)	mem 286MB
[2022-11-04 20:23:16 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5690/6300]	eta 0:00:18 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 4.0338 (3.7773)	grad_norm 16.2092 (14.5601)	loss_scale 262144.0000 (147021.2813)	mem 286MB
[2022-11-04 20:23:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5700/6300]	eta 0:00:18 lr 0.000008	 wd 0.0500	time 0.0307 (0.0309)	loss 4.0914 (3.7774)	grad_norm 18.6964 (14.5616)	loss_scale 262144.0000 (147223.2156)	mem 286MB
[2022-11-04 20:23:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5710/6300]	eta 0:00:18 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.7450 (3.7775)	grad_norm 12.9852 (14.5621)	loss_scale 262144.0000 (147424.4427)	mem 286MB
[2022-11-04 20:23:17 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5720/6300]	eta 0:00:17 lr 0.000008	 wd 0.0500	time 0.0295 (0.0309)	loss 4.0311 (3.7776)	grad_norm 17.4999 (14.5643)	loss_scale 262144.0000 (147624.9663)	mem 286MB
[2022-11-04 20:23:18 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5730/6300]	eta 0:00:17 lr 0.000008	 wd 0.0500	time 0.0295 (0.0309)	loss 3.8184 (3.7776)	grad_norm 13.4913 (14.5656)	loss_scale 262144.0000 (147824.7901)	mem 286MB
[2022-11-04 20:23:18 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5740/6300]	eta 0:00:17 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 4.1036 (3.7776)	grad_norm 12.3377 (14.5672)	loss_scale 262144.0000 (148023.9178)	mem 286MB
[2022-11-04 20:23:18 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5750/6300]	eta 0:00:17 lr 0.000008	 wd 0.0500	time 0.0308 (0.0309)	loss 4.1624 (3.7782)	grad_norm 14.5520 (14.5677)	loss_scale 262144.0000 (148222.3530)	mem 286MB
[2022-11-04 20:23:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5760/6300]	eta 0:00:16 lr 0.000008	 wd 0.0500	time 0.0317 (0.0309)	loss 4.0644 (3.7787)	grad_norm 15.2219 (14.5674)	loss_scale 262144.0000 (148420.0993)	mem 286MB
[2022-11-04 20:23:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5770/6300]	eta 0:00:16 lr 0.000008	 wd 0.0500	time 0.0339 (0.0309)	loss 3.9477 (3.7790)	grad_norm 15.5633 (14.5688)	loss_scale 262144.0000 (148617.1603)	mem 286MB
[2022-11-04 20:23:19 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5780/6300]	eta 0:00:16 lr 0.000008	 wd 0.0500	time 0.0298 (0.0309)	loss 4.0476 (3.7794)	grad_norm 12.9756 (14.5686)	loss_scale 262144.0000 (148813.5395)	mem 286MB
[2022-11-04 20:23:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5790/6300]	eta 0:00:15 lr 0.000008	 wd 0.0500	time 0.0317 (0.0309)	loss 3.8812 (3.7797)	grad_norm 14.2853 (14.5688)	loss_scale 262144.0000 (149009.2405)	mem 286MB
[2022-11-04 20:23:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5800/6300]	eta 0:00:15 lr 0.000008	 wd 0.0500	time 0.0351 (0.0309)	loss 3.9040 (3.7799)	grad_norm 13.5886 (14.5684)	loss_scale 262144.0000 (149204.2669)	mem 286MB
[2022-11-04 20:23:20 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5810/6300]	eta 0:00:15 lr 0.000008	 wd 0.0500	time 0.0338 (0.0309)	loss 3.5808 (3.7800)	grad_norm 15.6394 (14.5685)	loss_scale 262144.0000 (149398.6219)	mem 286MB
[2022-11-04 20:23:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5820/6300]	eta 0:00:14 lr 0.000008	 wd 0.0500	time 0.0313 (0.0309)	loss 3.6968 (3.7801)	grad_norm 15.2398 (14.5697)	loss_scale 262144.0000 (149592.3092)	mem 286MB
[2022-11-04 20:23:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5830/6300]	eta 0:00:14 lr 0.000008	 wd 0.0500	time 0.0320 (0.0309)	loss 3.7682 (3.7802)	grad_norm 15.8235 (14.5701)	loss_scale 262144.0000 (149785.3322)	mem 286MB
[2022-11-04 20:23:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5840/6300]	eta 0:00:14 lr 0.000008	 wd 0.0500	time 0.0301 (0.0309)	loss 3.7041 (3.7800)	grad_norm 16.4964 (14.5706)	loss_scale 262144.0000 (149977.6942)	mem 286MB
[2022-11-04 20:23:21 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5850/6300]	eta 0:00:13 lr 0.000008	 wd 0.0500	time 0.0322 (0.0309)	loss 3.8110 (3.7800)	grad_norm 13.2375 (14.5690)	loss_scale 262144.0000 (150169.3987)	mem 286MB
[2022-11-04 20:23:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5860/6300]	eta 0:00:13 lr 0.000008	 wd 0.0500	time 0.0328 (0.0309)	loss 3.5900 (3.7798)	grad_norm 15.7943 (14.5690)	loss_scale 262144.0000 (150360.4491)	mem 286MB
[2022-11-04 20:23:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5870/6300]	eta 0:00:13 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 3.6231 (3.7796)	grad_norm 13.7397 (14.5696)	loss_scale 262144.0000 (150550.8486)	mem 286MB
[2022-11-04 20:23:22 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5880/6300]	eta 0:00:12 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 4.0065 (3.7794)	grad_norm 13.4885 (14.5676)	loss_scale 262144.0000 (150740.6006)	mem 286MB
[2022-11-04 20:23:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5890/6300]	eta 0:00:12 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.9600 (3.7800)	grad_norm 13.8784 (14.5675)	loss_scale 262144.0000 (150929.7084)	mem 286MB
[2022-11-04 20:23:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5900/6300]	eta 0:00:12 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 4.1254 (3.7805)	grad_norm 12.0267 (14.5662)	loss_scale 262144.0000 (151118.1752)	mem 286MB
[2022-11-04 20:23:23 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5910/6300]	eta 0:00:12 lr 0.000008	 wd 0.0500	time 0.0298 (0.0309)	loss 4.1059 (3.7811)	grad_norm 15.0165 (14.5657)	loss_scale 262144.0000 (151306.0044)	mem 286MB
[2022-11-04 20:23:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5920/6300]	eta 0:00:11 lr 0.000008	 wd 0.0500	time 0.0302 (0.0309)	loss 4.0716 (3.7814)	grad_norm 15.7721 (14.5654)	loss_scale 262144.0000 (151493.1991)	mem 286MB
[2022-11-04 20:23:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5930/6300]	eta 0:00:11 lr 0.000008	 wd 0.0500	time 0.0304 (0.0309)	loss 4.1611 (3.7818)	grad_norm 17.4108 (14.5644)	loss_scale 262144.0000 (151679.7626)	mem 286MB
[2022-11-04 20:23:24 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5940/6300]	eta 0:00:11 lr 0.000008	 wd 0.0500	time 0.0294 (0.0309)	loss 3.8888 (3.7822)	grad_norm 14.2993 (14.5631)	loss_scale 262144.0000 (151865.6980)	mem 286MB
[2022-11-04 20:23:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5950/6300]	eta 0:00:10 lr 0.000008	 wd 0.0500	time 0.0315 (0.0309)	loss 4.0195 (3.7826)	grad_norm 14.8848 (14.5629)	loss_scale 262144.0000 (152051.0086)	mem 286MB
[2022-11-04 20:23:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5960/6300]	eta 0:00:10 lr 0.000008	 wd 0.0500	time 0.0314 (0.0309)	loss 3.8203 (3.7828)	grad_norm 13.7894 (14.5616)	loss_scale 262144.0000 (152235.6974)	mem 286MB
[2022-11-04 20:23:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5970/6300]	eta 0:00:10 lr 0.000008	 wd 0.0500	time 0.0288 (0.0309)	loss 3.9155 (3.7830)	grad_norm 13.6106 (14.5610)	loss_scale 262144.0000 (152419.7675)	mem 286MB
[2022-11-04 20:23:25 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5980/6300]	eta 0:00:09 lr 0.000008	 wd 0.0500	time 0.0299 (0.0309)	loss 3.7942 (3.7829)	grad_norm 13.7844 (14.5586)	loss_scale 262144.0000 (152603.2222)	mem 286MB
[2022-11-04 20:23:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][5990/6300]	eta 0:00:09 lr 0.000008	 wd 0.0500	time 0.0324 (0.0309)	loss 3.6686 (3.7829)	grad_norm 15.4879 (14.5596)	loss_scale 262144.0000 (152786.0644)	mem 286MB
[2022-11-04 20:23:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6000/6300]	eta 0:00:09 lr 0.000008	 wd 0.0500	time 0.0303 (0.0309)	loss 4.0144 (3.7830)	grad_norm 12.3763 (14.5576)	loss_scale 524288.0000 (153055.6641)	mem 286MB
[2022-11-04 20:23:26 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6010/6300]	eta 0:00:08 lr 0.000008	 wd 0.0500	time 0.0299 (0.0309)	loss 3.9068 (3.7830)	grad_norm 12.4991 (14.5548)	loss_scale 524288.0000 (153673.2524)	mem 286MB
[2022-11-04 20:23:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6020/6300]	eta 0:00:08 lr 0.000008	 wd 0.0500	time 0.0287 (0.0309)	loss 3.9298 (3.7830)	grad_norm 15.6961 (14.5527)	loss_scale 524288.0000 (154288.7892)	mem 286MB
[2022-11-04 20:23:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6030/6300]	eta 0:00:08 lr 0.000008	 wd 0.0500	time 0.0313 (0.0309)	loss 3.9922 (3.7836)	grad_norm 14.2776 (14.5527)	loss_scale 524288.0000 (154902.2849)	mem 286MB
[2022-11-04 20:23:27 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6040/6300]	eta 0:00:08 lr 0.000008	 wd 0.0500	time 0.0297 (0.0309)	loss 4.2538 (3.7842)	grad_norm 14.9804 (14.5521)	loss_scale 524288.0000 (155513.7494)	mem 286MB
[2022-11-04 20:23:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6050/6300]	eta 0:00:07 lr 0.000008	 wd 0.0500	time 0.0293 (0.0309)	loss 4.0754 (3.7848)	grad_norm 15.6483 (14.5525)	loss_scale 524288.0000 (156123.1929)	mem 286MB
[2022-11-04 20:23:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6060/6300]	eta 0:00:07 lr 0.000008	 wd 0.0500	time 0.0293 (0.0309)	loss 3.8155 (3.7852)	grad_norm 14.5070 (14.5522)	loss_scale 524288.0000 (156730.6253)	mem 286MB
[2022-11-04 20:23:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6070/6300]	eta 0:00:07 lr 0.000008	 wd 0.0500	time 0.0288 (0.0309)	loss 4.1059 (3.7856)	grad_norm 14.0169 (14.5527)	loss_scale 524288.0000 (157336.0567)	mem 286MB
[2022-11-04 20:23:28 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6080/6300]	eta 0:00:06 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 3.8289 (3.7859)	grad_norm 14.2312 (14.5516)	loss_scale 524288.0000 (157939.4968)	mem 286MB
[2022-11-04 20:23:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6090/6300]	eta 0:00:06 lr 0.000008	 wd 0.0500	time 0.0309 (0.0309)	loss 4.1180 (3.7862)	grad_norm 15.6114 (14.5518)	loss_scale 524288.0000 (158540.9555)	mem 286MB
[2022-11-04 20:23:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6100/6300]	eta 0:00:06 lr 0.000008	 wd 0.0500	time 0.0298 (0.0309)	loss 3.9937 (3.7865)	grad_norm 15.5008 (14.5532)	loss_scale 524288.0000 (159140.4426)	mem 286MB
[2022-11-04 20:23:29 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6110/6300]	eta 0:00:05 lr 0.000008	 wd 0.0500	time 0.0296 (0.0309)	loss 3.8444 (3.7867)	grad_norm 15.0816 (14.5526)	loss_scale 524288.0000 (159737.9676)	mem 286MB
[2022-11-04 20:23:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6120/6300]	eta 0:00:05 lr 0.000008	 wd 0.0500	time 0.0299 (0.0309)	loss 3.5655 (3.7867)	grad_norm 13.0861 (14.5527)	loss_scale 524288.0000 (160333.5403)	mem 286MB
[2022-11-04 20:23:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6130/6300]	eta 0:00:05 lr 0.000008	 wd 0.0500	time 0.0297 (0.0309)	loss 3.6330 (3.7867)	grad_norm 12.3999 (14.5524)	loss_scale 524288.0000 (160927.1701)	mem 286MB
[2022-11-04 20:23:30 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6140/6300]	eta 0:00:04 lr 0.000008	 wd 0.0500	time 0.0292 (0.0309)	loss 3.8381 (3.7866)	grad_norm 17.1874 (14.5521)	loss_scale 524288.0000 (161518.8666)	mem 286MB
[2022-11-04 20:23:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6150/6300]	eta 0:00:04 lr 0.000008	 wd 0.0500	time 0.0291 (0.0309)	loss 3.7384 (3.7865)	grad_norm 15.7096 (14.5527)	loss_scale 524288.0000 (162108.6392)	mem 286MB
[2022-11-04 20:23:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6160/6300]	eta 0:00:04 lr 0.000008	 wd 0.0500	time 0.0333 (0.0309)	loss 4.0601 (3.7865)	grad_norm 13.5570 (14.5528)	loss_scale 524288.0000 (162696.4973)	mem 286MB
[2022-11-04 20:23:31 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6170/6300]	eta 0:00:04 lr 0.000008	 wd 0.0500	time 0.0303 (0.0309)	loss 4.1159 (3.7869)	grad_norm 14.6978 (14.5510)	loss_scale 524288.0000 (163282.4502)	mem 286MB
[2022-11-04 20:23:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6180/6300]	eta 0:00:03 lr 0.000008	 wd 0.0500	time 0.0290 (0.0309)	loss 3.9316 (3.7872)	grad_norm 13.7177 (14.5489)	loss_scale 524288.0000 (163866.5070)	mem 286MB
[2022-11-04 20:23:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6190/6300]	eta 0:00:03 lr 0.000008	 wd 0.0500	time 0.0291 (0.0309)	loss 3.9850 (3.7874)	grad_norm 13.0134 (14.5458)	loss_scale 524288.0000 (164448.6771)	mem 286MB
[2022-11-04 20:23:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6200/6300]	eta 0:00:03 lr 0.000008	 wd 0.0500	time 0.0302 (0.0309)	loss 4.1298 (3.7878)	grad_norm 13.1462 (14.5433)	loss_scale 524288.0000 (165028.9695)	mem 286MB
[2022-11-04 20:23:32 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6210/6300]	eta 0:00:02 lr 0.000008	 wd 0.0500	time 0.0291 (0.0309)	loss 4.0612 (3.7880)	grad_norm 12.3223 (14.5408)	loss_scale 524288.0000 (165607.3933)	mem 286MB
[2022-11-04 20:23:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6220/6300]	eta 0:00:02 lr 0.000008	 wd 0.0500	time 0.0295 (0.0309)	loss 3.9690 (3.7882)	grad_norm 12.6897 (14.5377)	loss_scale 524288.0000 (166183.9576)	mem 286MB
[2022-11-04 20:23:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6230/6300]	eta 0:00:02 lr 0.000008	 wd 0.0500	time 0.0295 (0.0309)	loss 3.8711 (3.7883)	grad_norm 13.5627 (14.5354)	loss_scale 524288.0000 (166758.6712)	mem 286MB
[2022-11-04 20:23:33 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6240/6300]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.0304 (0.0309)	loss 4.0464 (3.7884)	grad_norm 12.9446 (14.5332)	loss_scale 524288.0000 (167331.5430)	mem 286MB
[2022-11-04 20:23:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6250/6300]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.0293 (0.0309)	loss 3.7908 (3.7884)	grad_norm 12.9816 (14.5308)	loss_scale 524288.0000 (167902.5820)	mem 286MB
[2022-11-04 20:23:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6260/6300]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.0319 (0.0309)	loss 3.5233 (3.7884)	grad_norm 13.8692 (14.5281)	loss_scale 524288.0000 (168471.7968)	mem 286MB
[2022-11-04 20:23:34 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6270/6300]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.0300 (0.0309)	loss 3.5667 (3.7882)	grad_norm 12.3677 (14.5258)	loss_scale 524288.0000 (169039.1963)	mem 286MB
[2022-11-04 20:23:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6280/6300]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.0305 (0.0309)	loss 3.7490 (3.7882)	grad_norm 12.1834 (14.5223)	loss_scale 524288.0000 (169604.7890)	mem 286MB
[2022-11-04 20:23:35 swin_tiny_patch4_window7_224_resisc45] (main.py 252): INFO Train: [0/25][6290/6300]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.0304 (0.0309)	loss 3.7931 (3.7881)	grad_norm 13.1762 (14.5196)	loss_scale 524288.0000 (170168.5837)	mem 286MB
[2022-11-04 20:23:35 swin_tiny_patch4_window7_224_resisc45] (main.py 260): INFO EPOCH 0 training takes 0:03:14
[2022-11-04 20:23:35 swin_tiny_patch4_window7_224_resisc45] (utils.py 149): INFO output/swin_tiny_patch4_window7_224_resisc45/default/ckpt_epoch_0.pth saving......
